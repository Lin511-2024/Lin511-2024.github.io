[
  {
    "objectID": "notes/meetings/01-1_fsm_addenda.html",
    "href": "notes/meetings/01-1_fsm_addenda.html",
    "title": "FSA in-class notes",
    "section": "",
    "text": "ok(a+y)?\n\n\n\n\n\nstateDiagram\n    direction LR\n    [*] --&gt; 1\n    1 --&gt; 2: o\n    2 --&gt; 3: k\n\n    3 --&gt; group1\n\n    state group1{\n        [*] --&gt; 4\n        4 --&gt; 4: a\n        4 --&gt; [*]: y\n    }\n    group1 --&gt; [*]: ε\n\n    3 --&gt; [*]: ε",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "FSA in-class notes"
    ]
  },
  {
    "objectID": "notes/meetings/01-1_fsm_addenda.html#finite-state-automata-and-regex",
    "href": "notes/meetings/01-1_fsm_addenda.html#finite-state-automata-and-regex",
    "title": "FSA in-class notes",
    "section": "",
    "text": "ok(a+y)?\n\n\n\n\n\nstateDiagram\n    direction LR\n    [*] --&gt; 1\n    1 --&gt; 2: o\n    2 --&gt; 3: k\n\n    3 --&gt; group1\n\n    state group1{\n        [*] --&gt; 4\n        4 --&gt; 4: a\n        4 --&gt; [*]: y\n    }\n    group1 --&gt; [*]: ε\n\n    3 --&gt; [*]: ε",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "FSA in-class notes"
    ]
  },
  {
    "objectID": "notes/meetings/01-1_fsm_addenda.html#the-regex-for-batman",
    "href": "notes/meetings/01-1_fsm_addenda.html#the-regex-for-batman",
    "title": "FSA in-class notes",
    "section": "The RegEx for Batman",
    "text": "The RegEx for Batman\n\nregex\n\nNa (na){15} Batman!( Batman! Batman!)?",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "FSA in-class notes"
    ]
  },
  {
    "objectID": "notes/meetings/05_ngrams2.html",
    "href": "notes/meetings/05_ngrams2.html",
    "title": "Sampling from a probability distribution",
    "section": "",
    "text": "From an n-gram model, we can generate new sequences by sampling from the probability distribution over tokens.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Sampling from a probability distribution"
    ]
  },
  {
    "objectID": "notes/meetings/05_ngrams2.html#from-counts-to-proportions",
    "href": "notes/meetings/05_ngrams2.html#from-counts-to-proportions",
    "title": "Sampling from a probability distribution",
    "section": "From counts to proportions",
    "text": "From counts to proportions\nLet’s start by saying we’re working with a bigram model (counts of 2 token sequences). If we start with an input token “the”, how do we randomly generate the next token?\nLet’s restrict our view to just the top 5 words that followed “the” in Moby Dick:\n\n\n\n\n\n\n\n\nFrequency of words that come after ‘the’\n\n\n\n\nWe can re-express these frequencies as proportions by dividing each frequency by the sum of all frequencies.\n\\[\nP(w_i|w_{i-1}) = \\frac{C(w_{i-1}w_i)}{\\sum(w_{i-1}w)}\n\\]\n\n\n\n\n\nProportion of words that come after ‘the’\n\n\n\n\nThese two plots should look the same, just with different labels on the y-axis.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Sampling from a probability distribution"
    ]
  },
  {
    "objectID": "notes/meetings/05_ngrams2.html#from-proportions-to-a-probability-distribution",
    "href": "notes/meetings/05_ngrams2.html#from-proportions-to-a-probability-distribution",
    "title": "Sampling from a probability distribution",
    "section": "From proportions to a “probability distribution”",
    "text": "From proportions to a “probability distribution”\nNow, if we stack each bar on top of each other and lie it flat, we get the “probability distribution” over tokens. The rectangle for each word represents how much probability it “takes up”.\n\n\n\n\n\nWords as a probability distribution",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Sampling from a probability distribution"
    ]
  },
  {
    "objectID": "notes/meetings/05_ngrams2.html#sampling-from-the-probability-distribution",
    "href": "notes/meetings/05_ngrams2.html#sampling-from-the-probability-distribution",
    "title": "Sampling from a probability distribution",
    "section": "Sampling from the probability distribution",
    "text": "Sampling from the probability distribution\nTo sample from this probability distribution, we can randomly throw darts at the figure above. Whichever rectangle the dart lands inside, we’ll say is the word we “sampled”.\nJust be super clear that the 20 ❌es in the next plot are totally at random, I’ll include the code that I used to generate them.\n\n\nr\n\ntibble(\n  samp = runif(20, min = 0, max = 1)\n) -&gt;\n  rand_samples\n\n\n\n\n\n\n\n\n\n\n\n\nRandom samples from a probability distribution\n\n\n\n\nIf we count up the total number of ❌es in each square above, the order of highest to lowest frequency samples probably won’t perfectly line up with the tokens that had the highest to lowest probabilities.\n\n\n\n\n\n\n\n\nfol\nn\n\n\n\n\nwhale\n8\n\n\nsea\n5\n\n\nsame\n4\n\n\nship\n2\n\n\nPequod\n1\n\n\n\n\n\n\n\n\nA little more realistic\nIf we try to get just a little bit more realistic, and look at the top 100 tokens that follow “the”, we’ll see that a single randomly sampled token is very often not going to be the highest probability token.\n\n\n\n\n\n\n\n\nRandomly sampling from the top 100\n\n\n\n\n\n\n\n\n\n\n\n\nfol\nn\nprop\n\n\n\n\nmost\n110\n0.02",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Sampling from a probability distribution"
    ]
  },
  {
    "objectID": "notes/meetings/05_ngrams2.html#different-probability-distributions",
    "href": "notes/meetings/05_ngrams2.html#different-probability-distributions",
    "title": "Sampling from a probability distribution",
    "section": "Different Probability distributions",
    "text": "Different Probability distributions\nWords occur at different frequencies in different contexts, so they’ll have different probability distributions in each context. Even the same “dart throws will return different sampled words if the probability distributions are different.\n\n\n\n\n\n\n\n\n\n\n\n\n\"The _\"\n\"the _\"\n\n\nn\nproportion\nn\nproportion\n\n\n\n\nwhale\n12\n0.26\n325\n0.31\n\n\nship\n13\n0.28\n230\n0.22\n\n\nsea\n2\n0.04\n207\n0.19\n\n\nsame\n4\n0.09\n155\n0.15\n\n\nPequod\n15\n0.33\n145\n0.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"The _\"\n\"the _\"\n\n\n\n\nwhale\n7\n8\n\n\nship\n3\n2\n\n\nsea\n1\n5\n\n\nsame\n2\n4\n\n\nPequod\n7\n1",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Sampling from a probability distribution"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#lesson-1-data-is-messy",
    "href": "notes/meetings/03_tokenization.html#lesson-1-data-is-messy",
    "title": "Tokenization",
    "section": "Lesson 1: Data is Messy",
    "text": "Lesson 1: Data is Messy\n\nHeaders, etc.\nIf you go take a look at the plain-text version of Moby Dick on Project Gutenberg, you’ll see that it starts out with the following block of text:\n\n\ntxt\n\nThe Project Gutenberg eBook of Moby Dick; Or, The Whale\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n\nTitle: Moby Dick; Or, The Whale\n\n\nAuthor: Herman Melville\n\nRelease date: July 1, 2001 [eBook #2701]\n                Most recently updated: August 18, 2021\n\nLanguage: English\n\n\n\n*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n\nAnd if you scroll to the very bottom of the file, you’ll also find a very long block of text describing the Project Gutenberg License. If we wanted to analyze the text of Moby Dick, we’d have to make sure to remove these headers and footers appropriately.\n\n\n\n\n\n\nHeader removal\n\n\n\n\n\nI’ve been using gutenbergpy to get text from project Gutenberg, which has a special function gutenbergpy.textget.strip_headers() to handily remove these headers.\n\n\n\nSimilarly, if you wanted to analyze the text of the course notes, and you downloaded the html page, on each page you’d be faced with a long header looking like this:\n\n\nhtml\n\n!DOCTYPE html&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\n\n&lt;meta charset=\"utf-8\"&gt;\n&lt;meta name=\"generator\" content=\"quarto-1.4.549\"&gt;\n\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\n\n&lt;meta name=\"author\" content=\"Josef Fruehwald\"&gt;\n&lt;meta name=\"dcterms.date\" content=\"2024-01-16\"&gt;\n\n&lt;title&gt;Lin511-2024 - Regular Languages and Finite State Machines&lt;/title&gt;\n&lt;style&gt;\ncode{white-space: pre-wrap;}\nspan.smallcaps{font-variant: small-caps;}\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\ndiv.column{flex: auto; overflow-x: auto;}\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\nul.task-list{list-style: none;}\nul.task-list li input[type=\"checkbox\"] {\n  width: 0.8em;\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \n  vertical-align: middle;\n}\n/* CSS for syntax highlighting */\npre &gt; code.sourceCode { white-space: pre; position: relative; }\npre &gt; code.sourceCode &gt; span { line-height: 1.25; }\npre &gt; code.sourceCode &gt; span:empty { height: 1.2em; }\n.sourceCode { overflow: visible; }\ncode.sourceCode &gt; span { color: inherit; text-decoration: inherit; }\ndiv.sourceCode { margin: 1em 0; }\npre.sourceCode { margin: 0; }\n@media screen {\ndiv.sourceCode { overflow: auto; }\n}\n@media print {\npre &gt; code.sourceCode { white-space: pre-wrap; }\npre &gt; code.sourceCode &gt; span { text-indent: -5em; padding-left: 5em; }\n}\npre.numberSource code\n  { counter-reset: source-line 0; }\npre.numberSource code &gt; span\n  { position: relative; left: -4em; counter-increment: source-line; }\npre.numberSource code &gt; span &gt; a:first-child::before\n  { content: counter(source-line);\n    position: relative; left: -1em; text-align: right; vertical-align: baseline;\n    border: none; display: inline-block;\n    -webkit-touch-callout: none; -webkit-user-select: none;\n    -khtml-user-select: none; -moz-user-select: none;\n    -ms-user-select: none; user-select: none;\n    padding: 0 4px; width: 4em;\n  }\npre.numberSource { margin-left: 3em;  padding-left: 4px; }\ndiv.sourceCode\n  {   }\n@media screen {\npre &gt; code.sourceCode &gt; span &gt; a:first-child::before { text-decoration: underline; }\n}\n/* CSS for citations */\ndiv.csl-bib-body { }\ndiv.csl-entry {\n  clear: both;\n  margin-bottom: 0em;\n}\n.hanging-indent div.csl-entry {\n  margin-left:2em;\n  text-indent:-2em;\n}\ndiv.csl-left-margin {\n  min-width:2em;\n  float:left;\n}\ndiv.csl-right-inline {\n  margin-left:2em;\n  padding-left:1em;\n}\ndiv.csl-indent {\n  margin-left: 2em;\n}&lt;/style&gt;\n...\n\nThat’s not even the end of it.\n\n\nMarkup\nOnce we get to the content of the text, there’s still “markup” to deal with. Here’s a sentence from Moby Dick.\n\n\ntxt\n\nBut _being paid_,—what will compare with it?\n\nThe underscores _ are there to indicate italics in the original text. Here’s how it looks when rendered:\n\n\n\nRendered text\n\n\nIf we just split this text up into words based on spaces, those underscores (and other punctiuation) are going to stuck around.\n\n\n\npython\n\nsentence = \"But _being paid_,—what will compare with it?\"\nwrap_print(sentence.split(\" \"))\n\n'But'   '_being'    'paid_,—what'   'will'  'compare'   \n'with'  'it?'   \n\n\n\nI don’t think “paid_,-what” is a word.\nThe same issue goes for trying to analyze text from the course notes. Here a paragraph from the finite state automata notes.\n\n\nhtml\n\n&lt;p&gt;\n  But since this is &lt;em&gt;Computational&lt;/em&gt; Linguistics, \n  we should probably learn about what is \n  “regular” about “regular” expressions, \n  because it’s related to formal language \n  theory!\n&lt;/p&gt;\n\nAgain, if we want to analyze the text, we’d need to extract it from this markup.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#lesson-2-language-is-complex.",
    "href": "notes/meetings/03_tokenization.html#lesson-2-language-is-complex.",
    "title": "Tokenization",
    "section": "Lesson 2: Language is complex.",
    "text": "Lesson 2: Language is complex.\nWe already started touching on how we may need to “case-fold” text before we analyze it.\n\n\n\npython\n\nimport re\nfrom collections import Counter\n\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_count = Counter(words)\n\nfor key in cat_count:\n  if re.match(\"[Cc]at\", key):\n    print(f\"{key}\\t{cat_count[key]}\")\n\nCats    2\ncats.   1\ncat 1\n\n\n\nConverting the whole phrase to lowercase will help, but there’s still the issue of punctuation.\nTokenization is the non-trivial task of splitting text up into meaningful units.\n\nTokenization\nSetting aside semantic issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and tokenizing it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\nPlaces to leave in punctuation\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\nYou don’t want to eliminate punctuation from inside Ph.D, or m.p.h.. You also don’t want to eliminate it from some proper names, like ampersands in Procter & Gamble, Texas A&M, A&W, m&m's.\nYou’ll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n1,000.55\n1.000,55\n1 000,55\n\nCurrency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n$0.99\n99¢\n€0,99\n\nDates: There are so many different permutations on how dates can be formatted that I shouldn’t list them all here, but here are some.1\n\n4 digit year, 2 digit month, 2 digit day\n\n2022-09-12\n2022/09/12\n\n4 digit year, 1 or 2 digit month, 2 digit day\n\n2022-9-12\n2022/9/12\n\n2 digit day, 2 digit month, 4 digit year\n\n12-09-2022\n12/09/2022\n\n2 digit day, 1 or 2 digit month, 4 digit year\n\n12-9-2022\n12/9/2022\n\n2 digit day, 2 digit month, 2 digit year\n\n12-09-22\n12/09/22\n\n2 digit month, 2 digit day, 4 digit year\n\n09-12-2022\n09/12/2022\n\n1 digit month, 2 digit day, 2 digit year\n\n9-12-22\n9/12/22\n\n\nEmoticons,2 where the token is entirely punctuation :), &gt;.&lt;.\n\n\n\nPlaces to split up words\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like hard-won.\n\nhard-won ➔ hard, won or hard, -, won.\n\nAnother example involves clitics, like n't or 're in English.\n\nisn't ➔ is, n't\ncan't ➔ ca, n't\nwhat're ➔ what, 're\n\n\n\nPlaces to glue words together\nYou might want to also glue together tokens from whitespace tokenization.\n\nNew, York, City ➔ New York City\nSuper, Smash, Brothers ➔ Super Smash Brothers\n\n\n\nChallenges with speech and text\n\n: $1,500\n\n: “one thousand five hundred dollars”\n: “fifteen hundred dollars”\n: “one and a half thousand dollars”\n: “one point five thousand dollars”",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#tokenizers--part-1-",
    "href": "notes/meetings/03_tokenization.html#tokenizers--part-1-",
    "title": "Tokenization",
    "section": "Tokenizers -part 1-",
    "text": "Tokenizers -part 1-\nThe Natural Language Toolkit library (Bird, Klein, and Loper 2009) has a few tokenizers available.\n\n\n\npython\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n## The first time you run it, \n## you need to download some data\nnltk.download('punkt')\n\n\n\npython\n\ntokens_01 = word_tokenize(sentence)\nwrap_print(tokens_01)\n\n'But'   '_being'    'paid_' ',' '—what' \n'will'  'compare'   'with'  'it'    '?' \n\n\n\nThe spacy package also has tokenizers available,\n\n\nbash\n\npip install spacy\npip install $(spacy info en_core_web_sm --url)\n\n\npython\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(sentence)\nwrap_print(doc)\n\n'But'   '_' 'being' 'paid_,—what'   'will'  \n'compare'   'with'  'it'    '?'",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#data-sparsity",
    "href": "notes/meetings/03_tokenization.html#data-sparsity",
    "title": "Tokenization",
    "section": "Data Sparsity",
    "text": "Data Sparsity\nWith any tokenization of text, you’re going to wind up with a lot of tokens that appear just once or twice.\n\n\n\n\n\n\nHere’s a plot of the top 10 most frequent tokens in Moby Dick.\n\n\n\n\n\nTop 10 tokens in Moby Dick\n\n\n\n\nThe trend of tokens getting less and less frequent continues\n\n\n\n\n\nAll tokens in Moby Dick\n\n\n\n\nThere is also a general phenomenon that the larger your corpus of tokens gets, the larger the vocabulary will get.\n\n\n\n\n\nAs the number of tokens increases, the size of the vocabulary increases\n\n\n\n\nThis means that if you have an “open vocabulary” tokenization method, where you can always create a new token based on some rules, you’ll never be able to analyze enough text such that you’ll never encounter a new token you’ve never seen before.\n\n\n\n\n\nThe probability of a new type as token size increases",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#closed-vocabulary-tokenization-byte-pair-encoding",
    "href": "notes/meetings/03_tokenization.html#closed-vocabulary-tokenization-byte-pair-encoding",
    "title": "Tokenization",
    "section": "Closed Vocabulary Tokenization (Byte Pair Encoding)",
    "text": "Closed Vocabulary Tokenization (Byte Pair Encoding)\nA different approach to tokenization is to have a pre-specified closed vocabulary that you use to pull tokens out of text.\nLet’s start out with a fake training of a byte pair encoder with the simple vocabulary “cats can’t canter”. We kick things off treating every character as a token, plus a specialized start-of-word symbol, which I’m representing with _.\n\n\n\n\n\n\nTokens\n\n\n_ c a t s\n_ c a n ' t\n_ c a n t e r\n\n\n\n\nTypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", '_'}\n\n\n\nThis is, in principle, the smallest and simplest tokenization we could do for any input text. While the total number of words is infinite, the total number of characters or symbols we use to create those words is finite.\nThe next step is to count up all of the pairs (or bigrams) of tokens in the training data. In this case, both (_, c) and (c, a) appear equally commonly, so I make a decision and say (_, c) is the one we’ll process first. We’ll paste them together, call them a new type, and replace all (_, c) sequences with _c.\n\n\n\n\n\n\ntokens\n\n\n_c a t s\n_c a n ' t\n_c a n t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ',\n`_c`}\n\n\n\nRepeating the process, the most frequently occurring bigram is now (_c, a), so we’ll add _ca as a new type, and replace all (_c, a) sequences with _ca.\n\n\n\n\n\n\ntokens\n\n\n_ca t s \n_ca n ' t _\n_ca n t e r _\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca'}\n\n\n\nFinally, the last most frequent sequence is (_ca, n), so we’ll add _can to the vocabulary, and collapse (_ca, n) sequences.\n\n\n\n\n\n\ntokens\n\n\n_ca t s\n_can ' t\n_can t e r\n\n\n\n\ntypes\n\n\n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca', '_can'}\n\n\n\nWe’ll stop at that point, but we could either continue for a fixed number of iterations, or until our type, or vocabulary size reaches a fixed number.\n\nThe use of Byte Pair Encoding\nThis kind of tokenization approach is necessary when you want to be able to tokenizer anything, and also have a pre-specified vocabulary size. We can see how OpenAI’s byte pair encoder handles the first few sentences of Moby Dick\n\n\n\npython\n\nimport tiktoken\nenc = tiktoken.encoding_for_model(\"gpt-4\")\n\n\n\npython\n\nmb_string = \"\"\"\nCall me Ishmael. Some years ago—never mind how long precisely—having\nlittle or no money in my purse, and nothing particular to interest me\non shore, I thought I would sail about a little and see the watery part\nof the world. It is a way I have of driving off the spleen and\nregulating the circulation.\n\"\"\"\n\ntokens = [\n  enc.decode([token]) \n    for token in enc.encode(mb_string.replace(\"\\n\", \" \"))\n]\n\nwrap_print(tokens)\n\n' Call' ' me'   ' Ish'  'ma'    'el'    \n'.' ' Some' ' years'    ' ago'  '—' \n'never' ' mind' ' how'  ' long' ' precisely'    \n'—' 'having'    ' little'   ' or'   ' no'   \n' money'    ' in'   ' my'   ' purse'    ',' \n' and'  ' nothing'  ' particular'   ' to'   ' interest' \n' me'   ' on'   ' shore'    ',' ' I'    \n' thought'  ' I'    ' would'    ' sail' ' about'    \n' a'    ' little'   ' and'  ' see'  ' the'  \n' wat'  'ery'   ' part' ' of'   ' the'  \n' world'    '.' ' It'   ' is'   ' a'    \n' way'  ' I'    ' have' ' of'   ' driving'  \n' off'  ' the'  ' sple' 'en'    ' and'  \n' regulating'   ' the'  ' circulation'  '.' ' ' \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank vs Frequency per tokenizer\n\n\n\n\n\n\n\n\n\nCorpus size vs Vocabulary size per tokenizer",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/meetings/03_tokenization.html#footnotes",
    "href": "notes/meetings/03_tokenization.html#footnotes",
    "title": "Tokenization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m being tedious here on purpose, because you have to keep in mind that if you wrote a function to handle just one of these possible date formats, it would not immediately translate over to the others! There are also entire libraries in multiple programming languages for parsing and reformatting date times. Python: datetime, R: lubridate.↩︎\nThis example isn’t from Jurafsky & Martin.↩︎",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Tokenization"
    ]
  },
  {
    "objectID": "notes/concepts/04_git_errors.html",
    "href": "notes/concepts/04_git_errors.html",
    "title": "Git Errors",
    "section": "",
    "text": "When you clicked on “Sync”, did you get an error window that looked like this?\nIf you clock on “Show Command Output”, a file will pop open that looks like this.\nImportant things to note:",
    "crumbs": [
      "Notes",
      "Concepts",
      "Git Errors"
    ]
  },
  {
    "objectID": "notes/concepts/04_git_errors.html#how-to-fix-it.",
    "href": "notes/concepts/04_git_errors.html#how-to-fix-it.",
    "title": "Git Errors",
    "section": "How to fix it.",
    "text": "How to fix it.\n\nHit the key combination  to pop open the VSCode Command Palette. It should look like this:\n\n\n\n\n\n\n\nType in terminal editor to find the command “Terminal: Create New Terminal in Editor Area”. It should probably be the top result. If it’s highlighted, just hit enter to open a new terminal.\n\n\n\n\n\n\n\nCopy-paste the following code into the terminal window that opened, then hit enter.\n\n\nbash\n\ngit config pull.rebase false\n\nThat’s it. You should be able to Sync successfully.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Git Errors"
    ]
  },
  {
    "objectID": "notes/concepts/04_git_errors.html#whats-going-on.",
    "href": "notes/concepts/04_git_errors.html#whats-going-on.",
    "title": "Git Errors",
    "section": "What’s going on.",
    "text": "What’s going on.\nThe “problem” is that the repository history in your codespace and the version on github don’t completely match up. The most likely scenario is\n\nYou accepted the assignment link, created your code space, made some changes and committed your changes.\nAfter you created your code space, I made some changes to the assignment (e.g fixed a misspelling in the README, or fixed an error in one of the tests), which then got merged into your GitHub repository.\n\nAll git is asking for is clarity in how to resolve the different histories between the version in your codespace and the version up on GitHub.\nI’m going to try to make sure that the command git config pull.rebase false gets run automatically when you create the Codespace for any future assignments.\n\nDetails\n\n\n\n\n\n\nWarning\n\n\n\nThis is only if you’re still curious!\n\n\nThis warning pops up when you have a situation like this, where “main” is the version on github, and “local” is the version on your computer, or in your codespace.\n\n\n\n\n\ngitGraph\n  commit id: \"assignment created\"\n  branch local\n  commit id: \"local copy\"\n  commit id: \"edits\"\n  checkout main\n  commit id: \"text fixes\"\n\n\n\n\n\n\nBy setting pull.rebase false, this is what happens:\n\n\n\n\n\ngitGraph\n  commit id: \"assignment created\"\n  branch local\n  commit id: \"local copy\"\n  commit id: \"edits\"\n  checkout main\n  commit id: \"text fixes\"\n  checkout local\n  merge main\n\n\n\n\n\n\nThe other option, a “rebase”, basically re-writes the history of your local version to look like this:\n\n\n\n\n\ngitGraph\n  commit id: \"assignment created\"\n  branch local\n  commit id: \"text fixes\" type: HIGHLIGHT\n  commit id: \"local copy\"\n  commit id: \"edits\"",
    "crumbs": [
      "Notes",
      "Concepts",
      "Git Errors"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html",
    "href": "notes/concepts/03_navigating-directories.html",
    "title": "Navigating Directories",
    "section": "",
    "text": "Some of what we’ll be doing in class will involve interacting with your computer through a “Command Line Interface.” Command line interacts are purely text based way to interact with your computer. It’s your same exact computer, just a different way of using it without using icons, or clicking. \n\n\nIn macOS, we can access a command line interface using the program Terminal. You can find Terminal in your Applications inside the folder called Utilities, or just by typing in “Terminal” into Spotlight search.\n\n\n\nFor now, on Windows, I’d recommend using PowerShell. You can launch PowerShell by searching for it in the Start menu.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html#command-line-interface",
    "href": "notes/concepts/03_navigating-directories.html#command-line-interface",
    "title": "Navigating Directories",
    "section": "",
    "text": "Some of what we’ll be doing in class will involve interacting with your computer through a “Command Line Interface.” Command line interacts are purely text based way to interact with your computer. It’s your same exact computer, just a different way of using it without using icons, or clicking. \n\n\nIn macOS, we can access a command line interface using the program Terminal. You can find Terminal in your Applications inside the folder called Utilities, or just by typing in “Terminal” into Spotlight search.\n\n\n\nFor now, on Windows, I’d recommend using PowerShell. You can launch PowerShell by searching for it in the Start menu.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html#getting-your-bearings",
    "href": "notes/concepts/03_navigating-directories.html#getting-your-bearings",
    "title": "Navigating Directories",
    "section": "Getting your Bearings",
    "text": "Getting your Bearings\nWhen you first launch your command line interface, it will look something like this:\n\n\n\nCommand Line Interface\n\n\nThis is obviously scary looking, and it doesn’t give you much hint as to what to do next. The important to thing to keep in mind is that while you are using the command line interface you are inside of a folder. There are two commands I recommend using over and over again to figure out where you are, and where you can go.\n\n“The Prompt”\nWe call the the space waiting for us to type in something the “prompt”. It usually has a symbol before it. Depending on your system, it might be any of the following: $, %, or &gt;. If you type something in here, your computer will interpret it as a command, and try to run it. \n\n\npwd\nIf you type in pwd into the terminal and hit enter, it will print out the absolute path to where you currently are. When you first launch a terminal, it usually starts you off in your home directory, which is usually going to be named after your username.\n\nbash\n\npwd\n/Users/Display\nUse pwd early, and use it often. It stands for present working directory. \n\n\nls\nIf you type in ls into the terminal and hit enter, it will print out a list of all of the directories and files inside the folder you are currently in. Here’s how it prints out on my own computer when I first launch Terminal:\n\nbash\n\nls\nDesktop      Library    Pictures\nDocuments    Movies     Public\nDownloads    Music      Sites  \nEach of the items listed here is another directory inside of my current directory. I can look inside and get a list of all the files inside of these directories by typing in ls and the name of the directory I want to see inside of. For example, if I wanted to get a list of all of the files on my desktop, this is what I would type:\n\nbash\n\nls Desktop\nSampletext.txt\nI only have one file on my desktop right now, and it’s a document called Sampletext.txt. \n\n\nKnow your surroundings\nReally skilled and accomplished programmers use the commands pwd and ls constantly, and so should you! It’s the only way to keep track of your surroundings on the command line. \nIf you ever are looking at a terminal, and don’t know what to do next, just type in pwd and ls. \nEvery time you do a new thing in the terminal, type in type in pwd and ls afterwards.\nJust type in pwd and ls all the time. :::",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html#getting-around",
    "href": "notes/concepts/03_navigating-directories.html#getting-around",
    "title": "Navigating Directories",
    "section": "Getting around",
    "text": "Getting around\nYou’re not always going to want to hang around in your home directory the whole time. You may want to get to a specific directory where some code or a script is to run it. To do this, we use the cd command, which stands for change directory.\n\ncd : Moving down 1 level\nTo change the folder you’re in, you just need to type in cd and then the name of the folder. For example to move from my home directory into my Documents folder, it would look like this:\n\nbash\n\nls\nDesktop     Library   Pictures\nDocuments   Movies    Public\nDownloads   Music     Sites\n\nbash\n\ncd Documents\nls\nFranklinInstituteSamples.pptx   Zoom\nFranklinInstituteSounds         html_practice\n\nbash\n\npwd\n/Users/Display/Documents\n\nFirst, I double checked which directories were in the folder I was currently in with ls.\nSecond, I moved into the Documents directory with cd Documents.\nThird, I checked what files and directories were in my new location with ls.\nFourth, I double checked my current location with pwd, which gave my the absolute path to where I was.\n\n\n\ncd : Moving up 1 level\nIf after moving into my documents folder, I wanted to move back to my home directory, that would involve moving up one level, because my Documents folder is inside of my home directory. We can do this using cd again, but instead of the name of a directory, we follow it with ... The two dots, .., is a short hand for “one level up from where I am.” Here’s how that works out on my system.\n\nbash\n\npwd\n/Users/Display/Documents\n\nbash\n\ncd ..\npwd\n/Users/Display\n\nFirst, I double checked where I was with pwd.\nSecond, I moved up one level with cd ..\nThird, I double checked where I would up with pwd again, confirming that I was now in my home directory again.\n\n\n\ncd : Moving down multiple levels\nYou don’t have to move down just one level at a time. For example, right now we’re in my home directory, but inside of my documents there’s a directory called html_practice. That’s two levels down. To get into html_practice, I just need to enter cd followed by the relative path to it. Remember, the relative path is the sequence of directory names that lie between where we are and where we want to go. Here’s how that looks on my system.\n\nbash\n\npwd\n/Users/Display\n\nbash\n\ncd Documents/html_practice\npwd\n/Users/Display/Documents/html_practice\n\nbash\n\nls\nsite.html  style.css\n\nFirst, I double checked my location with pwd\nSecond, I moved down two levels through Documents into html_practice with cd.\nThird, I double checked where I had moved to with pwd.\nLastly, I checked what files were in my new directory with ls.\n\n\n\nMoving up and over\nNow, we’re inside html_practice which is inside Documents. But what if we wanted to move into another directory that is also inside of Documents. For example, the directory Zoom is also inside of Documents, which is where Zoom saves all of the chat logs from our class meetings.\n\n\n\nWhere we want to move\n\n\nWe could do this in a two step process, using cd .. to move up one level, then cd Zoom to move into the zoom directory. But we can also combine the two into one step.\n\nbash\n\npwd\n/Users/Display/Documents/html_practice\n\nbash\n\ncd ../Zoom\npwd\n/Users/Display/Documents/Zoom\n\nFirst, I double checked where I was with pwd.\nThen, I moved up into Documents and down into Zoom in one go with cd ../Zoom\nFinally, I double checked where I wound up with pwd.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html#very-useful-tips",
    "href": "notes/concepts/03_navigating-directories.html#very-useful-tips",
    "title": "Navigating Directories",
    "section": "Very useful tips!",
    "text": "Very useful tips!\nThere are a few tips, tricks, and shortcuts that can make using the command line a little bit easier.\n\nTab autocomplete\nWhen using a command line interface, “tab autocomplete” is a crucial tool to help you get around. For example, if you are in your home directory and you want to move into your documents directory, all you really need to type is cd Doc and the terminal should autocomplete the rest of the file name! \nIf you have multiple directories that start with Doc, it will list all of them, and then you’ll only need to type in a few more characters before it will be able to autocomplete. \nTab autocomplete works for ls as well, which can let you explore through directories without needing to move around.\n\n\nGet back home!\nTechnically, your home directory is a directory with a long absolute path. But because it’s a directory you’ll probably want to keep getting back to often, there’s a shortcut built in. If you run cd ~, it will zip you back to your home directory from wherever you are.\n\n\nAbsolute paths\nIf, for some reason, you don’t know what the relative path is to a directory you want to get to from where you are (or maybe it’s very complicated), you can always run cd followed by the absolute path to where you want to go.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/03_navigating-directories.html#this-is-all-going-to-matter-a-lot",
    "href": "notes/concepts/03_navigating-directories.html#this-is-all-going-to-matter-a-lot",
    "title": "Navigating Directories",
    "section": "This is all going to matter a lot",
    "text": "This is all going to matter a lot\nUsing the command line and navigating your computer is going to come up a lot more in this course. You should try as much as you can to get a handle on how this all works now.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Navigating Directories"
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#single-quotes-vs-backticks",
    "href": "notes/concepts/01_keyboarding.html#single-quotes-vs-backticks",
    "title": "“Where is the … key?”",
    "section": "Single Quotes vs Backticks",
    "text": "Single Quotes vs Backticks\nIt’s important to distinguish between single quotes: ' and backticks `. They look similar, but aren’t interchangeable.\nThe backtick key is highlighted in the images of the keyboards below, in the top left side.\n\nPCMac",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#slash-vs-backslash-.",
    "href": "notes/concepts/01_keyboarding.html#slash-vs-backslash-.",
    "title": "“Where is the … key?”",
    "section": "Slash (/) vs backslash (\\).",
    "text": "Slash (/) vs backslash (\\).\nThese different symbols will do different things, and aren’t interchangeable!\n\nSlash (/) location\n\nPCMac\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackslash (\\) location\n\nPCMac",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#the-tilde-key.",
    "href": "notes/concepts/01_keyboarding.html#the-tilde-key.",
    "title": "“Where is the … key?”",
    "section": "The tilde ~ key.",
    "text": "The tilde ~ key.\nTo type the tilde1 key you have to hold Shift+`Shift+`.\n\nPCMac",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#the-caret",
    "href": "notes/concepts/01_keyboarding.html#the-caret",
    "title": "“Where is the … key?”",
    "section": "The caret ^",
    "text": "The caret ^\nYou can get the caret symbol with Shift+6Shift+6\n\nPCMac",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#the-pipe",
    "href": "notes/concepts/01_keyboarding.html#the-pipe",
    "title": "“Where is the … key?”",
    "section": "The Pipe |",
    "text": "The Pipe |\nYou can get the pipe, or vertical bar, with Shift+\\Shift+\\.\n\nPCMac",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#side-note-smart-quotes",
    "href": "notes/concepts/01_keyboarding.html#side-note-smart-quotes",
    "title": "“Where is the … key?”",
    "section": "Side Note: “Smart Quotes”",
    "text": "Side Note: “Smart Quotes”\nIf, at some point, you copy-paste text from the internet into your code or a text file, you might wind up with “smart quotes” that make things work weird!\n\n\n\nRegular Quotes\nSmart Quotes\n\n\n\n\n\"regular\"\n“smart”\n\n\n'regular'\n‘smart’\n\n\n\nBe careful not to have smart quotes cropping up in your code or data!",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/concepts/01_keyboarding.html#footnotes",
    "href": "notes/concepts/01_keyboarding.html#footnotes",
    "title": "“Where is the … key?”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPronunciations include [tɪl.də] and [tɪl.di]↩︎",
    "crumbs": [
      "Notes",
      "Concepts",
      "\"Where is the ... key?\""
    ]
  },
  {
    "objectID": "notes/programming/00_setup.html",
    "href": "notes/programming/00_setup.html",
    "title": "Setup Notes",
    "section": "",
    "text": "We’ll need to get our systems setup to work properly with the tools we’ll be using this semester.\n\n\n\n\n\n\nIf you ware using a windows computer, I’ll ask you to install Windows Subsystem for Linux like so:\n\nFollow the tutorial here for enabling and installing WSL\nInstall VS Code\nWork through the first section of this tutorial, stopping after you connect to WSL\n\n\n\n\n\n\n\n\nInstall VS Code\n\n\n\n\n\n\n\n\n\nWe’ll be using a program called pyenv to manage the versions of python we use. To install pyenv, open the VSCode Command Palette (with ) which will open the command line interface. Then,\n\ncopy this line of code code from this page ()\n\n\nbash\n\ncurl https://pyenv.run | bash\n\nPaste it into the terminal (()\nHit enter.\n\nOnce that has successfully finished:\n\n\n\n\n\n\n\nRun this\n\n\nbash\n\nopen ~/.zshrc\n\n\nIf you get an error.\n\n\nRun this\n\n\nbash\n\ntouch ~/.zshrc\nopen ~/.zshrc\n\n\n\n\n\n\n\n\nRun this\n\n\nbash\n\nopen ~/.bashrc\n\n\nIf you get an error\n\n\nRun ths\n\n\nbash\n\ntouch ~/.bashrc\nopen ~/.bashrc\n\n\n\n\n\n\nPaste the following into the window that opens.\n\n\nsh\n\nexport PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n\nSave it\n\n\n\n\nClose and reopen the terminal window\nCopy-Paste the following lines of code\n\n\nbash\n\npyenv install 3.11\npyenv global 3.11\npyenv virtualenv 3.11 Lin511\npyenv activate Lin511\nThis will have installed a recent version of python onto your computer, and created a “virtual environment” for the class.\n\n\n\n\n\n\n\nMake sure you change the email and name to the email and username you use for Github!\n\nIn the terminal, set your git email address:\n\n\nbash\n\ngit config --global user.email \"you@example.com\"\n\nSet your username.\n\n\nbash\n\ngit config --global user.name \"Your Name\"\nThen, follow these tutorials from github:\n\nGenerating a new SSH Key\nAdding your ssh key to github",
    "crumbs": [
      "Notes",
      "Programming",
      "Setup Notes"
    ]
  },
  {
    "objectID": "notes/programming/00_setup.html#setup",
    "href": "notes/programming/00_setup.html#setup",
    "title": "Setup Notes",
    "section": "",
    "text": "We’ll need to get our systems setup to work properly with the tools we’ll be using this semester.\n\n\n\n\n\n\nIf you ware using a windows computer, I’ll ask you to install Windows Subsystem for Linux like so:\n\nFollow the tutorial here for enabling and installing WSL\nInstall VS Code\nWork through the first section of this tutorial, stopping after you connect to WSL\n\n\n\n\n\n\n\n\nInstall VS Code\n\n\n\n\n\n\n\n\n\nWe’ll be using a program called pyenv to manage the versions of python we use. To install pyenv, open the VSCode Command Palette (with ) which will open the command line interface. Then,\n\ncopy this line of code code from this page ()\n\n\nbash\n\ncurl https://pyenv.run | bash\n\nPaste it into the terminal (()\nHit enter.\n\nOnce that has successfully finished:\n\n\n\n\n\n\n\nRun this\n\n\nbash\n\nopen ~/.zshrc\n\n\nIf you get an error.\n\n\nRun this\n\n\nbash\n\ntouch ~/.zshrc\nopen ~/.zshrc\n\n\n\n\n\n\n\n\nRun this\n\n\nbash\n\nopen ~/.bashrc\n\n\nIf you get an error\n\n\nRun ths\n\n\nbash\n\ntouch ~/.bashrc\nopen ~/.bashrc\n\n\n\n\n\n\nPaste the following into the window that opens.\n\n\nsh\n\nexport PYENV_ROOT=\"$HOME/.pyenv\"\n[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n\nSave it\n\n\n\n\nClose and reopen the terminal window\nCopy-Paste the following lines of code\n\n\nbash\n\npyenv install 3.11\npyenv global 3.11\npyenv virtualenv 3.11 Lin511\npyenv activate Lin511\nThis will have installed a recent version of python onto your computer, and created a “virtual environment” for the class.\n\n\n\n\n\n\n\nMake sure you change the email and name to the email and username you use for Github!\n\nIn the terminal, set your git email address:\n\n\nbash\n\ngit config --global user.email \"you@example.com\"\n\nSet your username.\n\n\nbash\n\ngit config --global user.name \"Your Name\"\nThen, follow these tutorials from github:\n\nGenerating a new SSH Key\nAdding your ssh key to github",
    "crumbs": [
      "Notes",
      "Programming",
      "Setup Notes"
    ]
  },
  {
    "objectID": "notes/programming/04_tokenizers.html",
    "href": "notes/programming/04_tokenizers.html",
    "title": "Working with Tokenizers",
    "section": "",
    "text": "Let’s get a little bit practical with\nFor this lesson, we’re going to use gutenbergpy and nltk, but if you try to import them right now, like they were in the course notes, you’re going to get an error.",
    "crumbs": [
      "Notes",
      "Programming",
      "Working with Tokenizers"
    ]
  },
  {
    "objectID": "notes/programming/04_tokenizers.html#installing-gutenbergpy",
    "href": "notes/programming/04_tokenizers.html#installing-gutenbergpy",
    "title": "Working with Tokenizers",
    "section": "Installing gutenbergpy",
    "text": "Installing gutenbergpy\nWe’ll need to install these packages. We’ll start with gutenbergpy.\n\n\npython\n\n! pip install gutenbergpy\n\nNow, we can import the functions to get Project Gutenberg books. The url for Moby Dick on Project Gutenberg is https://www.gutenberg.org/ebooks/2701. That last part of the url is the ID of the book, which we can pass to get_text_by_id() to download the book.\n\n\npython\n\nfrom gutenbergpy.textget import get_text_by_id, strip_headers\n\nbook_id = 2701\n\nraw_book = get_text_by_id(book_id)\n\nraw_book contains the book with all of its legal headers and footers. we can remove the headers and footers with strip_headers()\n\n\npython\n\nbook_byte = strip_headers(raw_book)\n\nOne last hitch here has to do with “character encoding”. We need to “decode” it.\n\n\npython\n\nbook_clean = book_byte.decode(\"utf-8\")\n\nLet’s wrap that up into one function we can re-run on new IDs\n\n\npython\n\ndef get_clean_book(book_id):\n    \"\"\"Get the cleaned book\n\n    Args:\n        book_id (str|int): The book id\n\n    Returns:\n        (str): The full book\n    \"\"\"\n    raw_book = get_text_by_id(book_id)\n    book_byte = strip_headers(raw_book)\n    book_clean = book_byte.decode(\"utf-8\")\n\n    return book_clean\n\nGo ahead and point get_clean_book() at another book id.",
    "crumbs": [
      "Notes",
      "Programming",
      "Working with Tokenizers"
    ]
  },
  {
    "objectID": "notes/programming/04_tokenizers.html#nltk-tokenization",
    "href": "notes/programming/04_tokenizers.html#nltk-tokenization",
    "title": "Working with Tokenizers",
    "section": "NLTK tokenization",
    "text": "NLTK tokenization\nLet’s tokenize one of our books with nltk.tokenize.word_tokenize().\n\nSteps\n\nInstall nltk.\nTry tokenizing your book.\n\nIt might not go right at first. You can double check what to do here in the course notes.",
    "crumbs": [
      "Notes",
      "Programming",
      "Working with Tokenizers"
    ]
  },
  {
    "objectID": "notes/programming/04_tokenizers.html#lets-try-spacy",
    "href": "notes/programming/04_tokenizers.html#lets-try-spacy",
    "title": "Working with Tokenizers",
    "section": "Lets try spacy",
    "text": "Lets try spacy\nTo work with spacy, we need to:\n\nInstall spacy\nInstall one of the spacy models.\n\n\nThe steps\n\nGo to the spacy website\nCan you find the code to successfully install it and its language model?\n\n\n\npython\n\n## Installation\n\nLet’s tokenize a book.\n\n\npython\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\n\npython\n\nimport re\nfirst_para = re.findall(\n    r\"Call me Ishmael.*?\\n\\n\", \n    book_clean, \n    re.DOTALL)[0]\n\n\n\npython\n\npara_doc = nlp(first_para)\n\nThe output of nlp is actually a complex object enriched with a lot of information that we can access a few different ways.\n\n\npython\n\npara_doc\n\nCall me Ishmael. Some years ago—never mind how long precisely—having\nlittle or no money in my purse, and nothing particular to interest me\non shore, I thought I would sail about a little and see the watery part\nof the world. It is a way I have of driving off the spleen and\nregulating the circulation. Whenever I find myself growing grim about\nthe mouth; whenever it is a damp, drizzly November in my soul; whenever\nI find myself involuntarily pausing before coffin warehouses, and\nbringing up the rear of every funeral I meet; and especially whenever\nmy hypos get such an upper hand of me, that it requires a strong moral\nprinciple to prevent me from deliberately stepping into the street, and\nmethodically knocking people’s hats off—then, I account it high time to\nget to sea as soon as I can. This is my substitute for pistol and ball.\nWith a philosophical flourish Cato throws himself upon his sword; I\nquietly take to the ship. There is nothing surprising in this. If they\nbut knew it, almost all men in their degree, some time or other,\ncherish very nearly the same feelings towards the ocean with me.\n\n\nTo get any particular token out, you can do ordinary indexing.\n\n\npython\n\npara_doc[2]\n\nIshmael\n\n\nTo get the actual text of a token, we need to get its .text attribute.\n\n\npython\n\npara_doc[2].text\n\n'Ishmael'\n\n\nThere’s lots of great stuff we can get out, like each sentence.\n\n\npython\n\nlist(para_doc.sents)[0]\n\nCall me Ishmael.\n\n\nOr the parts of speech of each token.\n\n\npython\n\nfirst_sent = list(para_doc.sents)[0]\n[x.pos_ for x in first_sent]\n\n['VERB', 'PRON', 'PROPN', 'PUNCT']\n\n\n\n\npython\n\n[x.morph for x in first_sent]\n\n[VerbForm=Inf,\n Case=Acc|Number=Sing|Person=1|PronType=Prs,\n Number=Sing,\n PunctType=Peri]",
    "crumbs": [
      "Notes",
      "Programming",
      "Working with Tokenizers"
    ]
  },
  {
    "objectID": "notes/programming/04_tokenizers.html#byte-pair-encoding",
    "href": "notes/programming/04_tokenizers.html#byte-pair-encoding",
    "title": "Working with Tokenizers",
    "section": "Byte Pair Encoding",
    "text": "Byte Pair Encoding\nWe can install and use the byte pair encoder from Open AI like so:\n\n\npython\n\nimport tiktoken\nenc = tiktoken.encoding_for_model(\"gpt-4\")\n\n\n\npython\n\nfirst_encoded = enc.encode(first_para)\nfirst_encoded[0:10]\n\n[7368, 757, 57704, 1764, 301, 13, 4427, 1667, 4227, 2345]\n\n\nThis looks like a bunch of numbers, because this is actually saying “The first word is the 7368th token in the vocabulary list.” To get the actual text of this token, we need to “decode” it.\n\n\npython\n\nenc.decode([7368])\n\n'Call'\n\n\nYou can just grab random tokens from the vocabulary like this\n\n\npython\n\nenc.decode([2024])\n\n' ter'\n\n\n\nTraining your own byte pair encoding\nWe can train or own byte pair encoder with the sentencepiece library.\n\n\npython\n\nimport sentencepiece as spm\nfrom pathlib import Path\n\n\n\npython\n\noutput = Path(\"book_clean.txt\")\noutput.write_text(book_clean)\n\n1218929\n\n\n\n\npython\n\nspm.SentencePieceTrainer.train(\n    input = output,\n    model_prefix = \"mine\",\n    vocab_size = 1000,\n    model_type = \"bpe\"\n)\n\n\n\npython\n\nmy_spm = spm.SentencePieceProcessor(model_file='mine.model')\n\n\n\npython\n\nmy_para = my_spm.encode_as_pieces(first_para)\nmy_para[0:20]\n\n['▁C',\n 'all',\n '▁me',\n '▁I',\n 'sh',\n 'm',\n 'a',\n 'el',\n '.',\n '▁S',\n 'ome',\n '▁years',\n '▁ag',\n 'o',\n '—',\n 'n',\n 'ever',\n '▁mind',\n '▁how',\n '▁long']\n\n\n\n\npython\n\nmy_spm.encode_as_pieces(\"Who is Josef Fruehwald\")\n\n['▁Wh', 'o', '▁is', '▁J', 'ose', 'f', '▁F', 'r', 'ue', 'h', 'w', 'a', 'ld']",
    "crumbs": [
      "Notes",
      "Programming",
      "Working with Tokenizers"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html",
    "href": "notes/programming/02_regex.html",
    "title": "Regular Expressions, Quickly",
    "section": "",
    "text": "Look at Github\n\n\n\nThe real content is in your regex-in-class-&lt;username&gt; repository on github. This is meant to be more of a reference sheet.",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#setting-up-for-using-regular-expressions-in-python",
    "href": "notes/programming/02_regex.html#setting-up-for-using-regular-expressions-in-python",
    "title": "Regular Expressions, Quickly",
    "section": "Setting up for using regular expressions in python",
    "text": "Setting up for using regular expressions in python\n\nWe’ll need to import the re module\nUnlike simple strings, we’ll need to write our regular expressions with a preceding r\n\n\nimport re\n\nr\"regex\"\n\n'regex'",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#important-re-functions",
    "href": "notes/programming/02_regex.html#important-re-functions",
    "title": "Regular Expressions, Quickly",
    "section": "Important re functions",
    "text": "Important re functions\nTwo ways to use re to search strings are\n\nre.search()\n\nReturn structured information about where the regex matches.\n\nre.findall()\n\nReturn all actual matching substrings\n\n\n\nsentence1 = \"The speaker is speaking.\"\n\n\nre.search()\n\nre.search(r\"speak\", sentence1)\n\n&lt;re.Match object; span=(4, 9), match='speak'&gt;\n\n\n\nsentence1[4:9]\n\n'speak'\n\n\n\n\nre.findall()\n\nre.findall(r\"speak\", sentence1)\n\n['speak', 'speak']",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#simple-character-searches",
    "href": "notes/programming/02_regex.html#simple-character-searches",
    "title": "Regular Expressions, Quickly",
    "section": "Simple character searches",
    "text": "Simple character searches\nLike the examples above, literally the characters you want to use will match.\n\nspeak_regex = r\"speak\"\nget_regex_url(speak_regex)\n\nspeak",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#options",
    "href": "notes/programming/02_regex.html#options",
    "title": "Regular Expressions, Quickly",
    "section": "Options",
    "text": "Options\nIf you want some characters to be chosen from a set of options, place them in [].\n\nvowels_regex = r\"[aeiou]\"\nget_regex_url(vowels_regex)\n\n[aeiou]\n\n\n\nre.findall(r\"[aeiou]\", sentence1)\n\n['e', 'e', 'a', 'e', 'i', 'e', 'a', 'i']\n\n\n\nthe_regex = r\"[Tt]he\"\nget_regex_url(the_regex)\n\n[Tt]he\n\n\n\nre.findall(the_regex, sentence1)\n\n['The']\n\n\n\nRanges\nRanges of characters or numbers can be given inside [] like so\n\nget_regex_url(r\"[a-z]\")\nget_regex_url(r\"[A-Z]\")\nget_regex_url(r\"[0-9]\")\nget_regex_url(r\"[A-Za-z]\")\n\n[a-z]\n\n\n[A-Z]\n\n\n[0-9]\n\n\n[A-Za-z]\n\n\n\n\n“Metacharacters”\n\n\\w == [A-Za-z0-9_]\n\nword characters\n\n\\W == [^A-Za-z0-9_]\n\nnon-word characters\n\n\\d == [0-9]\n\ndigits\n\n\\D == [^0-9]\n\nnon-digits\n\n\\s == [ \\t\\n]\n\nAny whitespace character\n\n\\S == [^ \\t\\n]\n\nnon-whitespace\n\n\n\n\nAny Character\nTo match any character (letter, number, punctuation, space, etc.) use . or “dot”\n\nre.findall(\n    # return every word character and \n    # the following character\n    r\"\\w.\",\n    sentence1\n)\n\n['Th', 'e ', 'sp', 'ea', 'ke', 'r ', 'is', 'sp', 'ea', 'ki', 'ng']\n\n\n\n\nEscaping special symbols\nIf you wanted to find the actual period in sentence1, you’d have to “escape” the . with a preceding `.\n\n# compare\nget_regex_url(r\".\")\nget_regex_url(r\"\\.\")\n\n.\n\n\n.\n\n\n\nre.findall(\n    \"\\.\",\n    sentence1\n)\n\n['.']",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#modifiers",
    "href": "notes/programming/02_regex.html#modifiers",
    "title": "Regular Expressions, Quickly",
    "section": "Modifiers",
    "text": "Modifiers\nModifiers come after the definition of a single character, and define how many times that character can appear.\n\na? = zero or one a\na+ = one or more a\na* = zero or more a\n\n\nget_regex_url(r\"bana?na\")\nget_regex_url(r\"bana+na\")\nget_regex_url(r\"bana*na\")\n\nbana?na\n\n\nbana+na\n\n\nbana*na",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#grouping",
    "href": "notes/programming/02_regex.html#grouping",
    "title": "Regular Expressions, Quickly",
    "section": "Grouping",
    "text": "Grouping\nYou can define groupings within regular expressions. The effect of these groupings depends what kind of regex function you’re using. For re.findall(), it’ll find the whole string, but return just the text from the grouping.\n\nsentence2 = \"The big bear and the small bear ran away.\"\n\n\nget_regex_url(r\"[Tt]he (\\w+) bear\")\n\n[Tt]he (+) bear\n\n\n\nre.findall(\n    r\"[Tt]he (\\w+) bear\",\n    sentence2\n)\n\n['big', 'small']",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/programming/02_regex.html#boundaries",
    "href": "notes/programming/02_regex.html#boundaries",
    "title": "Regular Expressions, Quickly",
    "section": "Boundaries",
    "text": "Boundaries\n\n^the == Finds “the” at the start of a string.\nthe$ == Finds ” the” at the end of a string.\n\\bthe\\b == Finds “the” in between word boundaries.\n\n\nget_regex_url(r\"^the \")\nget_regex_url(r\" the$\")\nget_regex_url(r\"\\bthe\\b\")\n\n^the\n\n\nthe$\n\n\n[(https://regexper.com/#%5Cbthe%5Cb)\n\n\n\nsentence3 = \"I saw the other bear.\"\nre.findall(\n    r\"the\",\n    sentence3\n)\n\n['the', 'the']\n\n\nThe second “the” there comes from inside “other”\n\nre.findall(\n    r\"\\bthe\\b\",\n    sentence3\n)\n\n['the']\n\n\n\nsentence3\n\n'I saw the other bear.'",
    "crumbs": [
      "Notes",
      "Programming",
      "Regular Expressions, Quickly"
    ]
  },
  {
    "objectID": "notes/in_class/00_python.html",
    "href": "notes/in_class/00_python.html",
    "title": "Starting Python",
    "section": "",
    "text": "This is how python works",
    "crumbs": [
      "Notes",
      "In-class",
      "Starting Python"
    ]
  },
  {
    "objectID": "notes/in_class/00_python.html#arithmetic-math",
    "href": "notes/in_class/00_python.html#arithmetic-math",
    "title": "Starting Python",
    "section": "Arithmetic & Math",
    "text": "Arithmetic & Math\n\n8+5\n\n13\n\n\n\n2084 -          1985\n\n99",
    "crumbs": [
      "Notes",
      "In-class",
      "Starting Python"
    ]
  },
  {
    "objectID": "notes/in_class/00_python.html#variables-and-assignment",
    "href": "notes/in_class/00_python.html#variables-and-assignment",
    "title": "Starting Python",
    "section": "“Variables” and “Assignment”",
    "text": "“Variables” and “Assignment”\n\nfruehwald_age = 2084 - 1985\n\nTitleCase\nsnake_case\ncamelCase\n\nfruehwald_age * 12\n\n1188\n\n\n\nVariables vs Strings\n\n\"fruehwald_age\"\n\n'fruehwald_age'\n\n\n\n'fruehwald_age'\n\n'fruehwald_age'\n\n\n\njohn_favorite_color = \"purple\"\n\n\nJohn_favorite_color\n\nNameError: name 'John_favorite_color' is not defined",
    "crumbs": [
      "Notes",
      "In-class",
      "Starting Python"
    ]
  },
  {
    "objectID": "notes/in_class/00_python.html#importing",
    "href": "notes/in_class/00_python.html#importing",
    "title": "Starting Python",
    "section": "Importing",
    "text": "Importing\n\nimport re\n\n\nre.search()\n\n\n# import pathlib\n# pathlib.Path()\n\nfrom pathlib import Path\n\n\nPath()\n\nPosixPath('.')",
    "crumbs": [
      "Notes",
      "In-class",
      "Starting Python"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for Lin511, Computational Linguistics, taught at the University of Kentucky Spring 2024\n\nAttribution\n\nThe robot emoji in the course logo is modified from Open Emoji (link), licenced CC-BY-SA 4.0. Edited version here(svg).\n\n\n\n\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {About},\n  url = {https://lin511-2024.github.io/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “About.” https://lin511-2024.github.io/about.html."
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "",
    "text": "Where:\nWhitehall, Rm 205\n\n\nWhen:\nTues & Thurs, 09:30 - 10:45\n\n\nPrereq:\nLin221\n\n\nCredits:\n3\n\n\n\n\n\n\n\n\n\n\nDr. Josef Fruehwald\n\n\nemail:\njosef.fruehwald@uky.edu\n\n\noffice hours:\nMons, 12:00pm - 02:00pm\n\n\noffice hours location:\nBreckinridge, Rm 10",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#key-info",
    "href": "syllabus/syllabus.html#key-info",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "",
    "text": "Where:\nWhitehall, Rm 205\n\n\nWhen:\nTues & Thurs, 09:30 - 10:45\n\n\nPrereq:\nLin221\n\n\nCredits:\n3\n\n\n\n\n\n\n\n\n\n\nDr. Josef Fruehwald\n\n\nemail:\njosef.fruehwald@uky.edu\n\n\noffice hours:\nMons, 12:00pm - 02:00pm\n\n\noffice hours location:\nBreckinridge, Rm 10",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-at-a-glance",
    "href": "syllabus/syllabus.html#course-at-a-glance",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "2 Course at a Glance",
    "text": "2 Course at a Glance\n\nWhat you’ll learn:\n\nComputational approaches to linguistic analysis; Computational tools (python, regular expressions, Git, GitHub).\n\nWhat you’ll do:\n\nIn class exercises; Programming assignments.\n\nWhat you’ll need:\n\nA computer with a physical keyboard; A GitHub account.\n\nThe final-est deadline\n\nApril 30, 2024\n\nAttendance Policy\n\nAttendance is crucial for successful completion of the course, but there are no grade penalties.\n\nLate Work Policy\n\n2 day penalty free grace period on all assignments, 5% flat penalty afterwards. See Late Submissions and Re-submissions",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "3 Course Description",
    "text": "3 Course Description\nThere are two important components to this course\n\nThis is an introduction to computational linguistics, with an emphasis on linguistics. We’ll be learning about approaches to computation as it relates to linguistic theory (e.g. phonological rules, syntactic parsing, etc) as well as computation involved in processing linguistic data (e.g. Large Language Models, Speech-to-Text etc.).\nThis will be an introduction to some practical aspects of general purpose computation, including basics of file system organization, version control, Integrated Developent Environments, command-line interfaces, and program writing (specifically in Python).\n\n\nHave you ever said one these things?\n\n“My computer hates me.”\n“I’m not a tech person.”\n\nAs part of our course meetings, I’ll be labelling these and other similar statements as “negative self-talk”. Instead, I’ll encourage you to try different statements, like\n\n“I’m not familiar with these concepts yet.”\n“Up to now, I’ve found these methods opaque.”\n\nWe can acknowledge your current struggle or confusion with computation or technology, while also acknowledging that their use is a skill, not a talent, and that skill can be built upon and improved with experience and practice.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#learning-outcomes",
    "href": "syllabus/syllabus.html#learning-outcomes",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "4 Learning Outcomes",
    "text": "4 Learning Outcomes\nAfter attending class meetings and completion of the coursework, students should be able to\n\nDescribe the computational methods used to model and process linguistic structures.\nUse Regular Expressions to search and match strings.\nWrite a python program to linguistically parse language data.\nCritically evaluate claims made about modern natural language processing applications.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-materials",
    "href": "syllabus/syllabus.html#course-materials",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "5 Course Materials",
    "text": "5 Course Materials\nWe will be using a mixture of textbooks and online resources for the course. These are currently available for free online. The labels SLP, NLTK and TP will be used to refer to each book in the reading schedule.\n\nSLP:\n\nDaniel Jurafsky and James H Martin. 202\\d. Speech and Language Processing: An Introduction to Natural Language Processing Computational Linguistics, and Speech Recognition Third Edition.\n\nNLTK:\n\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit\n\nTP:\n\nAllen B. Downey. 2015. Think Python, 2nd edition",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-technology",
    "href": "syllabus/syllabus.html#course-technology",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "6 Course Technology",
    "text": "6 Course Technology\nAs course with a practical programming component, we’re going to be using a number of technical programs. There will be specific class time set aside for setting these up.\n\nCommand-Line Interfaces\nCommand-line Interfaces (CLI) are text-only ways to interact with your computer, including accessing its files and running programs. We will all be accessing the CLI through Visual Studio Code. For Windows users, this will also involve installing Windows Subsystem for Linux.\n\n\nPython Scripts & Jupyter Notebooks\nPython is the programming language that we’ll be using to do most of our work and analysis in this course. We’ll also be using extensions to the Python programming language in the form of freely available Python libraries, such as nltk and numpy.\nWe’ll be writing both python scripts as well as using Jupyter notebooks to interact with python.\n\n\nVisual Studio Code\nThe fact we will be “writing” python scripts implies we will be writing them in something. The program we will be using for composing our programs will be Visual Studio Code. VS Code is a general purpose Integrated Development Environment (IDE).\nAs “formal languages”, programming languages are very sensitive to any kind of typo or formatting error. The purpose IDEs is to provide you support to avoid these typos & errors in the first place, and to warn you when they exist.\n\n\nGit/Github\nGit is a “Version Control System” that lets you keep track of changes on software projects. Github is a service that allows online hosting of Git projects. You will need to create a free a Github account for the course.\nThere will be a number of course assignments that you will submit via commits to GitHub.\n\n\nCanvas\nCanvas will be used to make course announcements, and to distribute assignment links.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#communications",
    "href": "syllabus/syllabus.html#communications",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "7 Communications",
    "text": "7 Communications\nI will respond to emails in a timely manner during normal working hours, but it may take longer if you email me after 5pm on weekdays, or any time during the weekend.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-schedule",
    "href": "syllabus/syllabus.html#course-schedule",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "8 Course Schedule",
    "text": "8 Course Schedule\nThe topics and readings listed here are the tentative schedule for the course. We may find, in the room, that some topics will take longer than initially scheduled.\n\nWeek 1\n\n\n\nDates:\n\nJan 8-12\n\n\n\n\nTopics:\n\nSetup, What is Computational Linguistics?\n\n\n\n\n\nReadings\n\nFollow posted tutorials for setting up our course technology.\nTP Chapter 1\n\n\n\n\n\nWeek 2-3\n\n\n\nDates:\n\nJan 15-26\n\n\n\n\nTopics:\n\nFiles, Text, Strings, Regular Expressions, Finite State Automata\n\n\n\n\nNo Office Hours for MLK Jr Day.\n\nReadings:\n\nCourse Notes: What is Python\nSLP Chapter 2, section 2.1 (pdf)\nTP, Chapter 2, Variables, expressions and statements\nTP Chapter 3, Functionss\n\n\n\n\n\nWeek 4-5\n\n\n\nDates:\n\nJan 29-Feb 09\n\n\n\n\nTopics:\n\nText Normalization, Tokenization, Python Variables, Python Functions\n\n\n\n\n\nReadings:\n\nSLP Chapter 2, sections 2.2, 2.3, 2.4\nTP, Chapter 10, Lists\nTP, Chapter 7, Iteration\nTP, Chapter 5, Conditionals\nNLTK Book, Chapter 3\n\n\n\n\n\nWeek 6-7\n\n\n\nDates:\n\nFeb 12 - 23\n\n\n\n\nTopics:\n\nn-grams, Corpora, Document Classification, Python Loops and Conditionals\n\n\n\n\n\nReadings\n\nSLP Chapters 3, 4, 5\n\n\n\n\n\nWeek 8-9\n\n\n\nDates:\n\nFeb 26 - Mar 08\n\n\n\n\nTopics:\n\nHidden Markov Models\n\n\n\n\n\nReadings\n\nSLP Appendix A: Hidden Markov Models\nTP Chapter 11, Dictionaries\nTP Chapter 15, Classes and Objects\n\n\n\nSpring Break\n\n\n\n\n\nWeek 10-11\n\n\n\nDates:\n\nMar 18-29\n\n\n\n\nTopics:\n\nParsing, Dependency trees\n\n\n\n\n\n\n\nWeek 12-13\n\n\n\nDates:\n\nApr 01-12\n\n\n\n\nTopics:\n\nDistributional semantics, word “embeddings”\n\n\n\n\n\n\n\nWeek 14-15\n\n\n\nDates:\n\nApr 15-19\n\n\n\n\nTopics:\n\nNeural Network Basics, Review",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-evaluation",
    "href": "syllabus/syllabus.html#course-evaluation",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "9 Course Evaluation",
    "text": "9 Course Evaluation\n\n\n\n\n\n\nGrade Components\n\n\n\nWeekly Exercises\n45%\n\n\nIn-Class Exercises\n25%\n\n\nFinal Project\n20%\n\n\nEngagement\n10%\n\n\n\n\n\nGrading Scale\n\n\n\nA\n&gt;= 90\n\n\nB\n80 to 89\n\n\nC\n70 to 79\n\n\nD\n60 to 69\n\n\nE\n&lt;= 59\n\n\n\n\n\n\n\nAssignment Submission\nSome assignments will be created in GitHub Classroom, and their invite code will be posted to canvas. Some of these assignments will have “autograding” tests enabled. These “autogrades” are intended to be feedback to help you fine tune your code, and are not meant to be the final grade you will get for the assignment. Only grades as they are appear on Canvas are your official grade.\n\n\nFinal Project\nA final capstone project for the course. This could be a report, an extension of an earlier exercise, or some other agreed upon format.\n\n\nEngagement\nInspired by Kirby Conrod’s approach to Participation Grades\n\nThis portion of the grade is a way for me to give you credit for informal/unstructured collaborative work that you do. Participation and collaboration are strong predictors of success and learning retention, so please make an effort to find a way that works well for you to participate and engage with your colleagues.\n\nA well known process for solving programming problems is “Rubber Duck Debugging.” It works by describing how each step of a program is supposed to work to another person or, as the name suggests, a rubber duck. Often the solution to the problem or the typo causing the bug jumps out at you during the process. Having a study buddy or study group could be really helpful if only for this purpose.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#late-submissions-and-re-submissions",
    "href": "syllabus/syllabus.html#late-submissions-and-re-submissions",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "10 Late Submissions and Re-submissions",
    "text": "10 Late Submissions and Re-submissions\nEvery graded piece of work will have a due date. After a 2 day grace period, there will be a single, flat 5% deduction from late work, whenever it is submitted between the due date and the The Final-est Deadline\n\nMidterm Grades\nI will submit midterm grades on March 08, 2024, at the end of the midterm grading window. Any unsubmitted assignments that were due before March 08 will be given a grade of 0, BUT you can still submit those assignments after March 08 for their inclusion in the final grade.\n\n\nThe Final-est Deadline\nThe final-est deadline by which to submit any material to be graded is April 30, 2024. I have to set this hard deadline in order to have enough time to conclude final grading in time for the university’s final grade submission deadline.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#group-work-and-code-sources",
    "href": "syllabus/syllabus.html#group-work-and-code-sources",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "11 Group Work and Code Sources",
    "text": "11 Group Work and Code Sources\nIt is acceptable to collaborate and confer with other students in the course. Any collaboration should be indicated in the assignment submission. You may also refer to code sources from elsewhere on the internet, as long as you also document the source, and explain what the code does. You might not receive credit for code which has been copied wholesale from another online source or from another student without credit or documentation.\n\nLarge Language Model (a.k.a. AI) Generated Code\nThere are a number of services that will generate code based on natural language queries. Some words of warning:\n\nFluent BS\nLarge Language Models have been found to generate code that looks superficially correct, but often does not actually run properly, or do what the human asker wanted. Being able to successfully identify where or why code does not work correctly is not always straight forward. This issue led the Q&A site StackOverflow to ban submissions generated by LLMs, stating\n\n[…] because GPT is good enough to convince users of the site that the answer holds merit, signals the community typically use to determine the legitimacy of their peers’ contributions frequently fail to detect severe issues with GPT-generated answers.\n\n\n\nExplain what the code does\nAs stated above, you should provide credit to any external sources you turned to for code help, and explain what the resulting code does.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#attendance-and-engagement",
    "href": "syllabus/syllabus.html#attendance-and-engagement",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "12 Attendance and Engagement",
    "text": "12 Attendance and Engagement\nYou are expected to attend all scheduled course meetings. It would be helpful, but not necessary, if you let me know in advance if you are going to miss any lectures.\nIf you feel sick in any way, including but not limited to the well-known symptoms of COVID-19 (loss of taste or smell, a new and persistent cough, high fever, etc), do not come to class. There are other mechanisms for demonstrating engagement than attending lectures.\nI will also expect all of us in the course to treat each other with respect and civility in all aspects of the course, including\n\nIn the audio of a Zoom meeting\nIn the text chat of a Zoom meeting\nOn any course discussion boards or other forums.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#academic-conduct",
    "href": "syllabus/syllabus.html#academic-conduct",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "13 Academic Conduct",
    "text": "13 Academic Conduct\nUK Senate rules on academic offences\nAppropriating someone else’s work and portraying it as your own is cheating. Collaborating with someone and portraying that work as solely your own is cheating. Obtaining answers to homework assignments or exams from previous semesters is cheating. Using an internet search engine to look up a question and reporting that answer as your own is cheating. Falsifying data or experimental results is cheating. If you are unsure about whether a specific action is cheating, you may check with me.\nThe minimum penalty for a first offense is a zero on the assignment on which the offense occurred. If the offense is considered severe or if the student has other academic offenses on their record, more serious penalties, up to suspension from the University may be imposed.\nWhen students submit work purporting to be their own, but which in any way borrows ideas, organization, wording or anything else from another source without appropriate acknowledgement of the fact, the students are guilty of plagiarism. Plagiarism includes reproducing someone else’s work, whether it be a published article, chapter of a book, a paper from a friend or some file, or something similar to this. Plagiarism also includes the practice of employing or allowing another person to alter or revise the work which a student submits as their own, whoever that other person may be.\nStudents may discuss assignments among themselves or with an instructor or tutor, but when the actual work is done, it must be done by the student, and the student alone. When a student’s assignment involves research in outside sources of information, the student must carefully acknowledge exactly what, where and how they employed them. If the words of someone else are used, the student must put quotation marks around the passage in question and add an appropriate indication of its origin. Making simple changes while leaving the organization, content and phraseology intact is plagiaristic. However, nothing in these Rules shall apply to those ideas which are so generally and freely circulated as to be a part of the public domain (University Senate Rules Section 6.3.1).",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#university-academic-policy-statements",
    "href": "syllabus/syllabus.html#university-academic-policy-statements",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "14 University Academic Policy Statements",
    "text": "14 University Academic Policy Statements\nLink to University Senate Academic Policy Statements\n\nExcused Absences and Acceptable Excuses\n\nExcused Absences: Senate Rules 5.2.5.2.1 defines the following as acceptable reasons for excused absences: (a) significant illness, (b) death of a family member, (c) trips for members of student organizations sponsored by an educational unit, trips for University classes, and trips for participation in intercollegiate athletic events, (d) major religious holidays, (e) interviews for graduate/professional school or full-time employment post-graduation, and (f) other circumstances found to fit “reasonable cause for nonattendance” by the instructor of record. Students should notify the professor of absences prior to class when possible.\nIf a course syllabus requires specific interactions (e.g., with the instructor or other students), in situations where a student’s total EXCUSED absences exceed 1/5 (or 20%) of the required interactions for the course, the student shall have the right to request and receive a “W,” or the Instructor of Record may award an “I” for the course if the student declines a “W.” (Senate Rules 5.2.5.2.3.1)\n\n\n\nReligious Observances\n\nReligious Observances: Students anticipating an absence for a major religious holiday are responsible for notifying the instructor in writing of anticipated absences due to their observance of such holidays. Senate Rules 5.2.5.2.1(4) requires faculty to include any notification requirements within the syllabus. If no requirement is specified, two weeks prior to the absence is reasonable and should not be given any later. Information regarding major religious holidays may be obtained through the Ombud’s websiteor calling 859-257-3737.\n\n\n\nVerification of Absences\n\nVerification of Absences:Students may be asked to verify their absences in order for them to be considered excused. Senate Rule 5.2.5.2.1 states that faculty have the right to request appropriate verification when students claim an excused absence due to: significant illness; death in the household, trips for classes, trips sponsored by an educational unit and trips for participation related to intercollegiate athletic events; and interviews for full-time job opportunities after graduation and interviews for graduate and professional school. (Appropriate notification of absences due to University-related trips is required prior to the absence when feasible and in no case more than one week after the absence.)\n\n\n\nMake-Up Work\n\nMake-Up Work: Students missing any graded work due to an excused absence are responsible: for informing the Instructor of Record about their excused absence within one week following the period of the excused absence (except where prior notification is required); and for making up the missed work. The instructor must give the student an opportunity to make up the work and/or the exams missed due to the excused absence, and shall do so, if feasible, during the semester in which the absence occurred. The instructor shall provide the student with an opportunity to make up the graded work and may not simply calculate the student’s grade on the basis of the other course requirements, unless the student agrees in writing. According to SR 5.2.5.2.2, if a student adds a class after the first day of classes and misses graded work, the instructor must provide the student with an opportunity to make up any graded work.\n\n\n\nExcused Absences for Military Duties\n\nExcused Absences for Military Duties: If a student is required to be absent for one-fifth or less of the required course interactions (e.g., class meetings) due to military duties, the following procedure (per SR 5.2.5.2.3.2) shall apply:\n\nOnce a student is aware of a call to duty, the student shall provide a copy of the military orders to the Director of the Veterans Resource Center. The student shall also provide the Director with a list of his/her courses and instructors.\nThe Director will verify the orders with the appropriate military authority, and on behalf of the military student, notify each Instructor of Record via Department Letterhead as to the known extent of the absence.\nThe Instructor of Record shall not penalize the student’s absence in any way and shall provide accommodations and timeframes so that the student can make up missed assignments, quizzes, and tests in a mutually agreed upon manner.\n\n\n\n\nUnexcused Absences\n\nUnexcused Absences: If an attendance/interaction policy is not stated in the course syllabus or the policy does not include a penalty to the student, the instructor cannot penalize a student for any unexcused absences. (SR 5.2.5.2.3.3)\n\n\n\nPrep Week and Reading Days\n\nPrep Week and Reading Days: Per Senate Rules 5.2.5.6, the last week of instruction of a regular semester is termed “Prep Week.” This phrase also refers to the last three days of instruction of the summer session and winter intersession. The Prep Week rule applies to ALL courses taught in the fall semester, spring semester, and summer session, including those taught by distance learning or in a format that has been compressed into less than one semester or session. This rule does not apply to courses in professional programs in colleges that have University Senate approval to have their own calendar.\nMake-up exams and quizzes are allowed during Prep Week. In cases of “Take Home” final examinations, students shall not be required to return the completed examination before the regularly scheduled examination period for that course. No written examinations, including final examinations, may be scheduled during the Prep Week. No quizzes may be given during Prep Week. No project/lab practicals/paper/presentation deadlines or oral/listening examinations may fall during the Prep Week unless it was scheduled in the syllabus AND the course has no final examination (or assignment that acts as a final examination) scheduled during finals week. (A course with a lab component may schedule the lab practical of the course during Prep Week if the lab portion does not also require a Final Examination during finals week.) Class participation and attendance grades are permitted during Prep Week. The Senate Rules permit continuing into Prep Week regularly assigned graded homework that was announced in the class syllabus.\nFor fall and spring semester, the Thursday and Friday of Prep Week are study days (i.e. “Reading Days”). There cannot be any required “interactions” on a Reading Day. “Interactions” include participation in an in-class or online discussion, attendance at a guest lecture, or uploading an assignment. See Senate Rules 9.1 for a more complete description of required interactions.\n\n\n\nAccommodations Due to Disability\n\nAccommodations Due to Disability: In accordance with federal law, if you have a documented disability that requires academic accommodations, please inform your instructor as soon as possible during scheduled office hours. In order to receive accommodations in a course, you must provide your instructor with a Letter of Accommodation from the Disability Resource Center (DRC). The DRC coordinates campus disability services available to students with disabilities. It is located on the corner of Rose Street and Huguelet Drive in the Multidisciplinary Science Building, Suite 407. You can reach them via phone at (859) 257-2754, via email (drc@uky.edu) or visit their website (uky.edu/DisabilityResourceCenter). DRC accommodations are not retroactive and should therefore be established with the DRC as early in the semester as is feasible.\n\n\n\nNon-Discrimination Statement and Title IX Information\n\nNon-discrimination and Title IX policy: In accordance with federal law, UK is committed to providing a safe learning, living, and working environment for all members of the University community. The University maintains a comprehensive program which protects all members from discrimination, harassment, and sexual misconduct. For complete information about UK’s prohibition on discrimination and harassment on aspects such as race, color, ethnic origin, national origin, creed, religion, political belief, sex, and sexual orientation, please see the electronic version of UK’s Administrative Regulation 6:1 (“Policy on Discrimination and Harassment”) (https://www.uky.edu/regs/ar6-1). In accordance with Title IX of the Education Amendments of 1972, the University prohibits discrimination and harassment on the basis of sex in academics, employment, and all of its programs and activities. Sexual misconduct is a form of sexual harassment in which one act is severe enough to create a hostile environment based on sex and is prohibited between members of the University community and shall not be tolerated. For more details, please see the electronic version of Administrative Regulations 6:2 (“Policy and Procedures for Addressing and Resolving Allegations of Sexual Harassment Under Title IX and Other Forms of Sexual Misconduct”) (https://www.uky.edu/regs/sites/www.uky.edu.regs/files/files/ar/ar_6.2-in...). Complaints regarding violations of University policies on discrimination, harassment, and sexual misconduct are handled by the Office of Institutional Equity and Equal Opportunity (Institutional Equity), which is located in 13 Main Building and can be reached by phone at (859) 257-8927. You can also visit Institutional Equity’s website (https://www.uky.edu/eeo).\nFaculty members are obligated to forward any report made by a student related to discrimination, harassment, and sexual misconduct to the Office of Institutional Equity. Students can confidentially report alleged incidences through the Violence Intervention and Prevention Center (https://www.uky.edu/vipcenter), Counseling Center (https://www.uky.edu/counselingcenter), or University Health Service (https://ukhealthcare.uky.edu/university-health-service/student-health).\nReports of discrimination, harassment, or sexual misconduct may be made via the Institutional Equity’s website (https://www.uky.edu/eeo); at that site, click on “Make a Report” on the left-hand side of the page.\n\n\n\nRegular and Substantive Interaction\n\nRegular and Substantive Interaction: All credit-bearing courses must support regular and substantive interaction (RSI) between the students and the instructor, regardless of the course’s delivery mode (e.g., in-person, hybrid, or online). Courses satisfy this requirement when course participants meet regularly as prescribed in SR 10.6, and the Instructor of Record substantively interacts with students in at least two of the following ways: provides direct instruction; assesses students’ learning; provides information or responds to students’ questions; and facilitates student discussions. Some exceptions allowed as per SACSCOC. For further information about the RSI requirement, see the Compliance Resources link on the Teaching, Learning and Academic Innovation Compliance page.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#diversity-equity-and-inclusion",
    "href": "syllabus/syllabus.html#diversity-equity-and-inclusion",
    "title": "Lin511-001 - Computational Linguistics",
    "section": "15 Diversity, Equity and Inclusion",
    "text": "15 Diversity, Equity and Inclusion\nUK Senate DEI Statement\nThe University of Kentucky is committed to our core values of diversity and inclusion, mutual respect and human dignity, and a sense of community (Governing Regulations XIV). We acknowledge and respect the seen and unseen diverse identities and experiences of all members of the university community (https://www.uky.edu/regs/gr14). These identities include but are not limited to those based on race, ethnicity, gender identity and expressions, ideas and perspectives, religious and cultural beliefs, sexual orientation, national origin, age, ability, and socioeconomic status. We are committed to equity and justice and providing a learning and engaging community in which every member is engaged, heard, and valued.\nWe strive to rectify and change behavior that is inconsistent with our principles and commitment to diversity, equity, and inclusion. If students encounter such behavior in a course, they are encouraged to speak with the instructor of record and/or the Office of Institutional Equity and Equal Opportunity. Students may also contact a faculty member within the department, program director, the director of undergraduate or graduate studies, the department chair, any college administrator, or the dean. All of these individuals are mandatory reporters under University policies.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Linguistics (Spring 2024)",
    "section": "",
    "text": "Welcome to the Lin511 Computational Linguistics website. This course is being taught in the University of Kentucky’s Linguistics Department in Spring 2024\nIf you are visiting on a mobile device, site navigation can be found under the “hamburger” at the top.\n\n\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {Computational {Linguistics} {(Spring} 2024)},\n  url = {https://lin511-2024.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “Computational Linguistics (Spring\n2024).” https://lin511-2024.github.io/.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "Week 1\n\n\n\nDates:\n\nJan 8-12\n\n\n\n\nTopics:\n\nSetup, What is Computational Linguistics?\n\n\n\n\n\nReadings\n\nFollow posted tutorials for setting up our course technology.\nTP Chapter 1\n\n\n\n\n\nWeek 2-3\n\n\n\nDates:\n\nJan 15-26\n\n\n\n\nTopics:\n\nFiles, Text, Strings, Regular Expressions, Finite State Automata\n\n\n\n\nNo Office Hours for MLK Jr Day.\n\nReadings:\n\nCourse Notes: What is Python\nSLP Chapter 2, section 2.1 (pdf)\nTP, Chapter 2, Variables, expressions and statements\nTP Chapter 3, Functionss\n\n\n\n\n\nWeek 4-5\n\n\n\nDates:\n\nJan 29-Feb 09\n\n\n\n\nTopics:\n\nText Normalization, Tokenization, Python Variables, Python Functions\n\n\n\n\n\nReadings:\n\nSLP Chapter 2, sections 2.2, 2.3, 2.4\nTP, Chapter 10, Lists\nTP, Chapter 7, Iteration\nTP, Chapter 5, Conditionals\nNLTK Book, Chapter 3\n\n\n\n\n\nWeek 6-7\n\n\n\nDates:\n\nFeb 12 - 23\n\n\n\n\nTopics:\n\nn-grams, Corpora, Document Classification, Python Loops and Conditionals\n\n\n\n\n\nReadings\n\nSLP Chapters 3, 4, 5\n\n\n\n\n\nWeek 8-9\n\n\n\nDates:\n\nFeb 26 - Mar 08\n\n\n\n\nTopics:\n\nHidden Markov Models\n\n\n\n\n\nReadings\n\nSLP Appendix A: Hidden Markov Models\nTP Chapter 11, Dictionaries\nTP Chapter 15, Classes and Objects\n\n\n\nSpring Break\n\n\n\n\n\nWeek 10-11\n\n\n\nDates:\n\nMar 18-29\n\n\n\n\nTopics:\n\nParsing, Dependency trees\n\n\n\n\n\n\n\nWeek 12-13\n\n\n\nDates:\n\nApr 01-12\n\n\n\n\nTopics:\n\nDistributional semantics, word “embeddings”\n\n\n\n\n\n\n\nWeek 14-15\n\n\n\nDates:\n\nApr 15-19\n\n\n\n\nTopics:\n\nNeural Network Basics, Review\n\n\n\n\n\n\n\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {Course {Schedule}},\n  url = {https://lin511-2024.github.io/syllabus/schedule.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “Course Schedule.” https://lin511-2024.github.io/syllabus/schedule.html.",
    "crumbs": [
      "Home",
      "Schedule"
    ]
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Meeting Notes",
    "section": "",
    "text": "These are the notes for our regular meetings. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup Notes\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nRegular Languages and Finite State Machines\n\n\n\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nFSA in-class notes\n\n\n\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nPushdown Automata and Context Free Languages\n\n\n\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nRegular Expressions, Quickly\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nMore Python: Poetry, Lists, Loops\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization\n\n\n\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Tokenizers\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nngrams\n\n\n-or- What if we could parse natural language with a finite state automaton?\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nMaking and Counting ngrams in python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nSampling from a probability distribution\n\n\n\n\n\n\ncompling\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJosef Fruehwald\n\n\n\n\n\n\n\n\n\n\n\n\nPython basics\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {Meeting {Notes}},\n  url = {https://lin511-2024.github.io/notes},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “Meeting Notes.” https://lin511-2024.github.io/notes.",
    "crumbs": [
      "Notes",
      "Notes Home"
    ]
  },
  {
    "objectID": "notes/programming/01_Python_basics.html",
    "href": "notes/programming/01_Python_basics.html",
    "title": "Python basics",
    "section": "",
    "text": "print(1)\n\n1\n\n\n\nprint(\"hello\")\n\nhello\n\n\n\nprint(2 + 2)\n\n4\n\n\n\nx = 1\ny = 10\n\nprint(x + y)\n\n11\n\n\n\nword1 = \"Hello\"\nword2 = \" World!\"\n\nprint(word1 + word2)\n\nHello World!\n\n\n\nprint(True)\n\nTrue\n\n\n\nprint(False)\n\nFalse\n\n\n\nprint(True and False)\n\nFalse\n\n\n\n\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {Python Basics},\n  url = {https://lin511-2024.github.io/notes/programming/01_Python_basics.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “Python Basics.” https://lin511-2024.github.io/notes/programming/01_Python_basics.html.",
    "crumbs": [
      "Notes",
      "Programming",
      "Python basics"
    ]
  },
  {
    "objectID": "notes/programming/05_ngrams.html",
    "href": "notes/programming/05_ngrams.html",
    "title": "Making and Counting ngrams in python",
    "section": "",
    "text": "Here, we’ll work through some of the practicalities of creating and counting ngrams from text. Let’s grab the book first.\nbook-getting function\nfrom gutenbergpy.textget \\\n    import get_text_by_id,\\\n           strip_headers\n\ndef get_clean_book(book_id):\n    \"\"\"Get the cleaned book\n\n    Args:\n        book_id (str|int): The book id\n\n    Returns:\n        (str): The full book\n    \"\"\"\n    raw_book = get_text_by_id(book_id)\n    book_byte = strip_headers(raw_book)\n    book_clean = book_byte.decode(\"utf-8\")\n\n    return book_clean\nmoby_dick = get_clean_book(2701)",
    "crumbs": [
      "Notes",
      "Programming",
      "Making and Counting ngrams in python"
    ]
  },
  {
    "objectID": "notes/programming/05_ngrams.html#first-unigrams",
    "href": "notes/programming/05_ngrams.html#first-unigrams",
    "title": "Making and Counting ngrams in python",
    "section": "First, unigrams",
    "text": "First, unigrams\nFirst step will be getting the “unigram” frequencies.\n\n\n\nwords in context\nwords to predict\ntotal\nname\n\n\n\n\n0\n1\n1\nunigram\n\n\n1\n1\n2\nbigram\n\n\n2\n2\n3\ntrigram\n\n\n\nTo get any counts, we need to tokenize.\n\nfrom nltk.tokenize import word_tokenize\n\nmoby_words = word_tokenize(moby_dick)\n\nNext, we can count with collections.Counter()\n\n\nfrom collections import Counter\n\nmoby_count = Counter(moby_words)\nmoby_count.most_common(10)\n\n[(',', 19211),\n ('the', 13717),\n ('.', 7164),\n ('of', 6563),\n ('and', 6009),\n ('to', 4515),\n ('a', 4507),\n (';', 4178),\n ('in', 3915),\n ('that', 2918)]\n\n\n\nThe “unigram” probability of a word: \\[\nP(w_i) = \\frac{C(w_i)}{\\sum_{i=1}^n C(w_i)}\n\\]\nWe can get \\(C(w_i)\\) from the counter dictionary\n\n\nmoby_count[\"whale\"]\n\n771\n\n\n\nThe trickier thing to get is \\(\\sum_{i=1}^n C(w_i)\\). One way to do it is with a for loop.\n\n\ntotal_freq = 0\nfor word in moby_count:\n    total_freq += moby_count[word]\n\ntotal_freq\n\n255958\n\n\n\nWith total_freq, we can calculate the probability of each token, which we can store in another dictionary.\n\n\nmoby_prob = {}\nfor word in moby_count:\n    moby_prob[word] = moby_count[word] / total_freq\n\nmoby_prob[\"whale\"]\n\n0.0030122129411856635\n\n\n\n\nIntroducing numpy\nnumpy is a python package that allows you to do numeric computation more easilly.\n\n## if you need to install it:\n# ! pip install numpy\n\nimport numpy as np\n\nIf we just had a python list of numbers, we couldn’t quickly divide each number by the sum.\n\n\nsample_list = [0, 1, 3]\nsample_list / sum(sample_list)\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n\nWe can do this with a numpy array.\n\n\nsample_array = np.array([0, 1, 3])\nsample_array / sample_array.sum()\n\narray([0.  , 0.25, 0.75])\n\n\n\nThere’s lots of numpy methods to make life easier when working with numbers.\n\n\n[\n    sample_array.min(), \n    sample_array.max()\n]\n\n[0, 3]\n\n\n\n\n\nRelating tokens, counts and probabilities\nWhile the dictionary moby_count is convenient for quickly getting the count of a token, we’ll need separate lists and arrays for:\n\nthe text of each token\nthe count of each token\nthe probability of each token\n\n\n# a list of the text of each token\nword_list = [w for w in moby_count]\n\n# an array of the count of each token\ncount_array = np.array(\n    [\n        moby_count[w] \n        for w in word_list\n    ]\n)\n\n# a array of the probability of each token\nprob_array = count_array / count_array.sum()\n\nA thing to think about is how the mathematical formula on the left is being implemented in the code on the right.\n\n\n\n\n\n\n\\[\n\\frac{C(w_i)}{\\sum_{i=1}^n w_i}\n\\]\n\n\ncount_array / count_array.sum()\n\n\n\nWe can get a specific word’s probability like so:\n\n\nprob_array[\n    word_list.index(\"whale\")\n]\n\n0.0030122129411856635\n\n\n\n\n\n“Sampling” random words\nWe can sample a random word from the unigram distribution like so:\n\n\nnp.random.choice(\n    word_list, \n    size = 10, \n    p = prob_array\n)\n\narray(['it', 'letter', 'compasses', 'and', 'cut', 'lot', 'a', 'thus',\n       'flew', 'With'], dtype='&lt;U28')",
    "crumbs": [
      "Notes",
      "Programming",
      "Making and Counting ngrams in python"
    ]
  },
  {
    "objectID": "notes/programming/05_ngrams.html#making-bigrams",
    "href": "notes/programming/05_ngrams.html#making-bigrams",
    "title": "Making and Counting ngrams in python",
    "section": "Making Bigrams",
    "text": "Making Bigrams\nMaking bigrams is a bit more complex. We need to get counts of each sequence of two tokens. Fortunately, nltk has a nice and convenient function for this.\n\n\nfrom nltk import ngrams\nsent1 = [\"Call\", \"me\", \"Ishmael\", \".\"]\n\nlist(\n    ngrams(sent1, n = 2)\n)\n\n[('Call', 'me'), ('me', 'Ishmael'), ('Ishmael', '.')]\n\n\n\nThis is a list of “tuples”. Tuples are kind of like lists, but you’re not able to edit them after you create them. We can use Counter() on a list of tuples just like we did a list of tokens.\n\n\nmoby_bigram_count = Counter(\n    ngrams(moby_words, 2)\n)\n\nmoby_bigram_count.most_common(10)\n\n[((',', 'and'), 2630),\n (('of', 'the'), 1869),\n (('’', 's'), 1784),\n (('in', 'the'), 1122),\n ((',', 'the'), 916),\n ((';', 'and'), 857),\n (('to', 'the'), 715),\n (('.', 'But'), 596),\n (('.', '“'), 594),\n ((',', 'that'), 583)]\n\n\n\n\nbigram_list = [bigram for bigram in moby_bigram_count]\nbigram_count = np.array(\n    [moby_bigram_count[bigram] for bigram in bigram_list]\n)\n\n\nGetting conditional probabilities\nNow, getting the probability of a single token isn’t as straightforward, since we’re looking at conditional probabilities.\n\\[\nP(w_i | w_{i-1}) = \\frac{C(w_{i-1}w_i)}{\\sum C(w_{i-1}w)}\n\\]\nTo use concrete words for a second:\n\\[\nP(\\text{whale} | \\text{the}) = \\frac{C(\\text{the whale})}{\\sum C(\\text{the }w)}\n\\]\nSo, to calculate the conditional probability, we need to get\n\nThe count of the bigram “the whale”\nA list (or array) of the count of every bigram that begins with “the”.\nThe sum of this second list.\n\n\n\n# the first word in the\n# 2 word sequence\nw_0 = \"the\"\n\nw_0w = [\n    bigram \n    for bigram in moby_bigram_count\n    if bigram[0] == w_0\n]\n\nw_0w[0:10]\n\n[('the', 'Whale'),\n ('the', 'Monstrous'),\n ('the', 'Less'),\n ('the', 'True'),\n ('the', 'Hand'),\n ('the', 'Arsacides'),\n ('the', 'Carpenter'),\n ('the', 'Cabin'),\n ('the', 'End'),\n ('the', 'First')]\n\n\n\n\n\nC_w_0w = np.array(\n    [\n        moby_bigram_count[bigram]\n        for bigram in w_0w\n    ]\n)\n\ntotal_w_0 = C_w_0w.sum()\n\ntotal_w_0\n\n13717\n\n\n\nWe can now get the specific conditional probability for the word “whale”\n\n\nmoby_bigram_count[\n    (\"the\", \"whale\")\n]\n\n325\n\n\n\n\n\nmoby_bigram_count[\n    (\"the\", \"whale\")\n] / total_w_0\n\n0.02369322738208063\n\n\n\nTo randomly generate any word following “the”, we need to get the probability distribution across bigrams.\n\n\nP_w_0w = C_w_0w / C_w_0w.sum()\n\nw_1 = [bigram[1] for bigram in w_0w]\n\nnp.random.choice(\n    w_1,\n    size = 4,\n    p = P_w_0w\n)\n\narray(['hammers', 'steak', 'chances', 'house'], dtype='&lt;U21')\n\n\n\nTo generate a sequence, starting with a specific word, we need to encapsulate our logic above into a single function.\n\ndef generate_sequence(\n        bigram_count:dict,\n        w_0:str = \"The\", \n        n:int = 10\n        )-&gt;list[str]:\n    \"\"\"Generate a sequence of words from \n    a bigram model\n\n    Args:\n        w_0 (str): The initial token\n        n (int): The size of the sequence\n            to generate\n\n    Returns:\n        (list[str]): The generated sequence\n    \"\"\"\n    ## start out with the seed token\n    sequence = [w_0]\n\n    for i in range(n):\n        ## The new seed token should be\n        ## the last one added\n        w_0 = sequence[-1]\n\n        ## get all bigrams beginning \n        ## with the seed token\n        w_0w = [\n            bigram \n            for bigram in bigram_count\n            if bigram[0] == w_0\n        ]\n\n        ## get the counts of all bigrams\n        C_w_0w = np.array([\n            bigram_count[bigram]\n            for bigram in w_0w\n        ])\n\n        ## get the probabilities of all bigrams\n        P_w_0w = C_w_0w / C_w_0w.sum()\n\n        ## get the second token \n        ## from every bigram\n        w_1 = [\n            bigram[1]\n            for bigram in w_0w\n        ]\n\n        ## sample a new token\n        chosen = np.random.choice(\n            w_1,\n            size = 1,\n            p = P_w_0w\n        )\n\n        ## add the sampled token to the \n        ## sequence\n        sequence.append(chosen[0])\n\n    return sequence\n\n\n\ngenerate_sequence(moby_bigram_count)\n\n['The',\n 'heavers',\n 'singing',\n 'in',\n 'his',\n 'suit',\n ',',\n 'filled',\n 'for',\n 'their',\n 'spirits']",
    "crumbs": [
      "Notes",
      "Programming",
      "Making and Counting ngrams in python"
    ]
  },
  {
    "objectID": "notes/programming/05_ngrams.html#padding",
    "href": "notes/programming/05_ngrams.html#padding",
    "title": "Making and Counting ngrams in python",
    "section": "Padding",
    "text": "Padding\nThe bigram sequence generator above has to start out on a specific word, and it will keep going for as many loops as we tell it.\nIf we wanted a generator that doesn’t need a specific word to start on, and will auto-stop when it gets to the end of a sentence, we’ll need to pre-process our data differently, so that there are special “start” and “stop” symbols, or “padding”.\n\n\nfrom nltk.tokenize import sent_tokenize\nmoby_sentences = sent_tokenize(moby_dick)\n\nprint(moby_sentences[500])\n\nDeep into distant woodlands\nwinds a mazy way, reaching to overlapping spurs of mountains bathed in\ntheir hill-side blue.\n\n\n\n\n\nmoby_sent_words = [\n    word_tokenize(sentence) \n    for sentence in moby_sentences\n]\n\nmoby_sent_words[500]\n\n['Deep',\n 'into',\n 'distant',\n 'woodlands',\n 'winds',\n 'a',\n 'mazy',\n 'way',\n ',',\n 'reaching',\n 'to',\n 'overlapping',\n 'spurs',\n 'of',\n 'mountains',\n 'bathed',\n 'in',\n 'their',\n 'hill-side',\n 'blue',\n '.']\n\n\n\n\nmoby_sent_words_padded = [\n    [\"&lt;s&gt;\"] + sent + [\"&lt;/s&gt;\"]\n    for sent in moby_sent_words\n]\n\nmoby_sent_words_padded[500]\n\n['&lt;s&gt;',\n 'Deep',\n 'into',\n 'distant',\n 'woodlands',\n 'winds',\n 'a',\n 'mazy',\n 'way',\n ',',\n 'reaching',\n 'to',\n 'overlapping',\n 'spurs',\n 'of',\n 'mountains',\n 'bathed',\n 'in',\n 'their',\n 'hill-side',\n 'blue',\n '.',\n '&lt;/s&gt;']\n\n\n\nmoby_words2 = [\n    token \n    for sent in moby_sent_words_padded\n        for token in sent\n]\n\nmoby_bigrams2 = ngrams(moby_words2, n = 2)\nmoby_bigram_count2 = Counter(moby_bigrams2)\n\n\n\ngenerate_sequence(\n    moby_bigram_count2, \n    w_0 = \"&lt;s&gt;\", \n    n = 20\n)\n\n['&lt;s&gt;',\n 'With',\n 'what',\n 'business',\n ',',\n 'Starbuck',\n 'caught',\n 'one',\n 'foot',\n 'part—what',\n 'a',\n 'chess-man',\n 'beside',\n 'the',\n 'rolling',\n 'on',\n 'the',\n 'boats',\n 'and',\n 'sunk',\n '!']",
    "crumbs": [
      "Notes",
      "Programming",
      "Making and Counting ngrams in python"
    ]
  },
  {
    "objectID": "notes/programming/05_ngrams.html#doing-it-faster-with-nltk",
    "href": "notes/programming/05_ngrams.html#doing-it-faster-with-nltk",
    "title": "Making and Counting ngrams in python",
    "section": "Doing it faster with nltk",
    "text": "Doing it faster with nltk\n\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.lm import MLE\n\nngram_size = 3\n\ntrain, vocab = padded_everygram_pipeline(ngram_size, moby_sent_words)\nlm = MLE(ngram_size)\nlm.fit(train, vocab)\n\n\n\nsequence = [\"&lt;s&gt;\", \"&lt;s&gt;\"]\nfor i in range(100):\n    w_0 = sequence[-2:]\n    new = lm.generate(num_words=1, text_seed=w_0)\n    sequence.append(new)\n    if new == \"&lt;/s&gt;\":\n        break\n\nsequence\n\n['&lt;s&gt;',\n '&lt;s&gt;',\n 'had',\n 'turned',\n ',',\n 'and',\n 'continually',\n 'set',\n 'in',\n 'a',\n 'calm—give',\n 'us',\n 'a',\n 'blue',\n 'hanging',\n 'tester',\n 'of',\n 'smoke',\n ',',\n 'illuminated',\n 'by',\n 'the',\n 'terms',\n 'of',\n 'my',\n 'own',\n 'voice',\n '.',\n '&lt;/s&gt;']",
    "crumbs": [
      "Notes",
      "Programming",
      "Making and Counting ngrams in python"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html",
    "href": "notes/programming/03_python_plus.html",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "",
    "text": "This lesson has two goals:",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#finding-and-reading-in-files",
    "href": "notes/programming/03_python_plus.html#finding-and-reading-in-files",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "Finding and reading in files",
    "text": "Finding and reading in files\nIn order to read in a file, first we need to tell python where a file is. We’ll do this with pathlib.Path(). For a recap of what paths are, see Understanding Directories.\n\n\npython\n\nfrom pathlib import Path\n\nLet’s just call Path() and see what it gives us.\n\n\npython\n\nhere = Path()\nhere\n\nPosixPath('.')\n\n\nSome detail of what gets printed out for here will depend on your operating system, but one thing that should be the same across the board is that '.' is a placeholder meaning “the dirtory you’re currently in.\nTo see the full path to your current directory, we’ll use the .resolve() method.\n\n\npython\n\nhere.resolve()\n\nPosixPath('/workspaces/Lin511-2024.github.io/notes/programming')\n\n\nWhat this full path looks like really depends on where you are running your code.\n\nGetting directory contents\nTo get the full contents of a directory, we’ll use the the .glob() method. To get readable output, we need to wrap list() around everything.\n\n\npython\n\nlist(\n    here.glob(pattern = \"*\")\n)\n\n[PosixPath('03_python_plus.ipynb'),\n PosixPath('assets'),\n PosixPath('_02_Paths.ipynb'),\n PosixPath('01_Python_basics.ipynb'),\n PosixPath('00_setup.qmd'),\n PosixPath('poems'),\n PosixPath('02_regex.ipynb')]",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#lists-a-new-data-structure",
    "href": "notes/programming/03_python_plus.html#lists-a-new-data-structure",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "*Lists (a new data structure)",
    "text": "*Lists (a new data structure)\nThe code above created a “list”, which is, like the name suggests, a list of values.\n\n\npython\n\nhere_list = list(\n    here.glob(pattern = \"*\")\n)\n\nWe can create our own lists by hand, like so:\n\n\npython\n\nmy_list = [\"a\", \"b\", \"c\", \"d\", \"e\"] \n\n\nDoing things with lists\n\nGetting their length\nTo get the length of anything, including lists, len().\n\n\npython\n\nlen(here_list)\n\n7\n\n\n\n\npython\n\nlen(my_list)\n\n5\n\n\n\n\nGetting a value from a list\nTo get a value out of a list in python, we need to “index” it. To get just the first value from a list, we’ll use the index 0.\n\n\npython\n\nhere_list[0]\n\nPosixPath('03_python_plus.ipynb')\n\n\n\n\npython\n\nmy_list[0]\n\n'a'\n\n\nTo get a range of values, we’ll use a “slice” operator :.\n\n\npython\n\nmy_list[0:3]\n\n['a', 'b', 'c']\n\n\n\n\npython\n\nmy_list[1:4]\n\n['b', 'c', 'd']\n\n\n\n\nHow to think about indices\nThe index for 'a' is 0, and the index for 'c' is 2.\n\n\npython\n\nmy_list.index(\"a\")\n\n0\n\n\n\n\npython\n\nmy_list.index(\"c\")\n\n2\n\n\nBut to get out every value between 'a' and 'c', we need to use 0:3.\n\n\npython\n\nmy_list[0:3]\n\n['a', 'b', 'c']\n\n\nThe way to conceptualize these numeric indices is that they come between the values in a list.\n\n\n\nIndex locations\n\n\nWhen we give just a single number as an index, we get back the value immediately to the right of the index.\n\n\n\nSingle value index\n\n\nWhen we give an index slice, we get back every value between the start of the slice and the end.\n\n\n\nSlice indexing\n\n\n\nNegative indexing\nSometimes, you know you’ll want the last, or second to last value from a list, so there are also negative indicies defined.\n\n\n\nNegative indexing\n\n\nThese work in the same way. A single index returns the value immediately to the left, a slice returns the values between the first and the second.\n\n\npython\n\nmy_list[-1]\n\n'e'\n\n\n\n\npython\n\nmy_list[-3:-1]\n\n['c', 'd']\n\n\n\n\npython\n\nmy_list[-2:]\n\n['d', 'e']",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#reading-in-a-text-file",
    "href": "notes/programming/03_python_plus.html#reading-in-a-text-file",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "Reading in a text file",
    "text": "Reading in a text file\nI’ve put two poems in the poems directory right next to the current notebook. We can get a list of them by\n\nadding poems/ to the here path\nGlobbing the new path.\n\n\n\npython\n\npoems_dir = here.joinpath(\"poems\")\npoems_files = list(poems_dir.glob(\"*\"))\npoems_files\n\n[PosixPath('poems/the_tiger.txt'), PosixPath('poems/this_is_just_to_say.txt')]\n\n\nLet’s read in poems/this_is_just_to_say.txt.\n\n\npython\n\nplums_path = poems_files[1]\nplums_path\n\nPosixPath('poems/this_is_just_to_say.txt')\n\n\nRight now plums_path just contains information about where the file is. In order to read its contents into python, we need to\n\nOpen it.\nRead it.\nClose it.\n\nWe’ll manage all three steps with the following code.\n\n\npython\n\nplums_poem = plums_path.read_text()\nprint(plums_poem)\n\nThis Is Just To Say\nBy William Carlos Williams\n\nI have eaten\nthe plums\nthat were in\nthe icebox\n\nand which\nyou were probably\nsaving\nfor breakfast\n\nForgive me\nthey were delicious\nso sweet\nand so cold",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#doing-things-to-strings",
    "href": "notes/programming/03_python_plus.html#doing-things-to-strings",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "Doing things to strings",
    "text": "Doing things to strings\nRight now, plums_poem is just one long string. When we print it out, we get some nice formatting, but if we look at the unformatted output, you’ll see it’s one long string with newline characters: \\n.\n\n\npython\n\nplums_poem\n\n'This Is Just To Say\\nBy William Carlos Williams\\n\\nI have eaten\\nthe plums\\nthat were in\\nthe icebox\\n\\nand which\\nyou were probably\\nsaving\\nfor breakfast\\n\\nForgive me\\nthey were delicious\\nso sweet\\nand so cold'\n\n\nTo start doing computational linguistics things to the poem, we’ll need to start splitting it up into pieces, like separate lines, or separate words.\n\nUpper and Lower case\nOne thing that can make life a little easier is to “case fold” text data, which we can do with the .lower() and .upper() methods.\n\n\npython\n\nplums_lower = plums_poem.lower()\nprint(plums_lower[0:46])\n\nthis is just to say\nby william carlos williams\n\n\n\n\nSplitting into lines\nWe can also split the poem into separate lines with the .split() method, which will return a list with the poem split by whatever value we give it.\n\n\npython\n\n# split according to newlines\nplums_lines = plums_poem.split(\"\\n\")\nplums_lines\n\n['This Is Just To Say',\n 'By William Carlos Williams',\n '',\n 'I have eaten',\n 'the plums',\n 'that were in',\n 'the icebox',\n '',\n 'and which',\n 'you were probably',\n 'saving',\n 'for breakfast',\n '',\n 'Forgive me',\n 'they were delicious',\n 'so sweet',\n 'and so cold']\n\n\nNow, we can get each individual line by indexing.\n\n\npython\n\nplums_lines[0]\n\n'This Is Just To Say'",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#looping-through-the-poem",
    "href": "notes/programming/03_python_plus.html#looping-through-the-poem",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "Looping through the poem",
    "text": "Looping through the poem\nAbove, I ran .split() on the non-case folded poem. What if I wanted to lowercase each line of the poem now, after the fact. We can use a for loop to do this. Let’s build up to it.\n\nfor-loop skeleton\n\n\npython\n\nfor line in plums_lines:\n    myspace_line = \"xXx \" + line + \" xXx\"\n    print(myspace_line)\n\nxXx This Is Just To Say xXx\nxXx By William Carlos Williams xXx\nxXx  xXx\nxXx I have eaten xXx\nxXx the plums xXx\nxXx that were in xXx\nxXx the icebox xXx\nxXx  xXx\nxXx and which xXx\nxXx you were probably xXx\nxXx saving xXx\nxXx for breakfast xXx\nxXx  xXx\nxXx Forgive me xXx\nxXx they were delicious xXx\nxXx so sweet xXx\nxXx and so cold xXx\n\n\nLet’s unpack the first line of the for loop:\nfor line in plums_line:\nThis tells python to take each value in plums_line, assign it to a new variable called line, then do something. After it has done something, it goes and grabs the next value from plums_line, assigns it to a variable called line, and starts over.\n\n\n\n\n\n\nSome common misconceptions\n\n\n\n\nWhat we tell python to name the variable it creates does not affect the way the for loop works. We called it line to make the code readable. If we really wanted to pull out every noun from the poem, and said for noun in plums_lines:, it is not going to pull out every noun.\n\n\n\npython\n\nfor noun in plums_lines[0:2]:\n    print(noun)\n\nThis Is Just To Say\nBy William Carlos Williams\n\n\n\nWhat we do inside of the loop doen’t have to involve the line variable at all.\n\n\n\npython\n\nfor line in plums_lines:\n    print(\"plum\")\n\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\nplum\n\n\n\nWhat the for-loop did does not get saved anywhere. The only “state” that gets preserved at all is the final one.\n\n\n\npython\n\nmyspace_line\n\n'xXx and so cold xXx'\n\n\n\n\nThis last point is super important. To save, or remember, what happened inside of a for-loop, you need to do two things.\n\nCreate a variable outside of the loop.\nUpdate the variable inside of the loop.\n\nFor example, if we wanted to get the total number of characters in the peom, we’d need to\n\nCreate a collector variable outside of the loop.\nAdd update its total value inside of the loop.\n\n\n\npython\n\ntotal_chars = 0\n\nprint(f\"The total number of chars is now {total_chars}\")\n\nfor line in plums_lines:\n    line_len = len(line)\n    total_chars = total_chars + line_len\n    print(f\"The total number of chars is now {total_chars}\")\n\nprint(f\"The final number of chars is {total_chars}\")\n\nThe total number of chars is now 0\nThe total number of chars is now 19\nThe total number of chars is now 45\nThe total number of chars is now 45\nThe total number of chars is now 57\nThe total number of chars is now 66\nThe total number of chars is now 78\nThe total number of chars is now 88\nThe total number of chars is now 88\nThe total number of chars is now 97\nThe total number of chars is now 114\nThe total number of chars is now 120\nThe total number of chars is now 133\nThe total number of chars is now 133\nThe total number of chars is now 143\nThe total number of chars is now 162\nThe total number of chars is now 170\nThe total number of chars is now 181\nThe final number of chars is 181\n\n\nThis use of a for-loop makes the “loop” part clearer.\n\n\n\n\n\nstateDiagram\n    state \"for line in lines:\" as for\n    state if_line &lt;&lt;choice&gt;&gt;\n    state fork &lt;&lt;fork&gt;&gt;\n    state fork1 &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork1\n    fork1 --&gt; total_chars: set to 0\n    fork1 --&gt; for\n    for --&gt; if_line\n    if_line --&gt; fork: If another line\n    if_line --&gt; [*]: If no more lines\n    fork --&gt; total_chars: +len(line)\n    fork --&gt; for\n\n\n\n\n\n\nTo actually lowercase the poem now, we need to\n\nCreate an empty list outside of the loop.\nAdd the lowercased line to this list inside of the loop.\n\n\n\npython\n\nlower_lines = []\n\nfor line in plums_lines:\n    lower_lines.append(\n        line.lower()\n    )\n\nlower_lines\n\n['this is just to say',\n 'by william carlos williams',\n '',\n 'i have eaten',\n 'the plums',\n 'that were in',\n 'the icebox',\n '',\n 'and which',\n 'you were probably',\n 'saving',\n 'for breakfast',\n '',\n 'forgive me',\n 'they were delicious',\n 'so sweet',\n 'and so cold']\n\n\n\n\nFiltering the poem\nThere are a few lines of the poem that are blank and have a length of 0.\n\n\npython\n\nprint(lower_lines[2])\n\n\n\n\n\n\npython\n\nlen(lower_lines[2])\n\n0\n\n\nWe can create a new list without these lines by\n\nCreating an empty list outside of the loop.\nAdding lines to that list inside the loop, only if they have a length greater than 0.\n\nThis introduces an if control block.\n\n\npython\n\nfull_lines = []\n\nfor line in lower_lines:\n    if len(line) &gt; 0:\n        full_lines.append(line)\n    else:\n        print(\"dumped an empty line\")\n\ndumped an empty line\ndumped an empty line\ndumped an empty line\n\n\n\n\npython\n\nfull_lines\n\n['this is just to say',\n 'by william carlos williams',\n 'i have eaten',\n 'the plums',\n 'that were in',\n 'the icebox',\n 'and which',\n 'you were probably',\n 'saving',\n 'for breakfast',\n 'forgive me',\n 'they were delicious',\n 'so sweet',\n 'and so cold']\n\n\nThe if block:\n\nChecks to see if the logical statement is True or False.\nIf True, it executes the code inside.\n\nIn this case, I’ve also included an else block. This executes its code if the if block above was False.",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#nesting-loops",
    "href": "notes/programming/03_python_plus.html#nesting-loops",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "Nesting loops",
    "text": "Nesting loops\nWe can nest for loops and if statements to an arbitrary level, but good programming practice tries to keep it to a minumum. For example, to get every word in the poem, based on where the spaces are, we can\n\nCreate an empty words list outside of a loop.\nSplit each line using spaces.\nStart a loop over that list.\nUpdate the words list inside this loop.\n\n\n\npython\n\nall_words = []\n\nfor line in full_lines:\n    words = line.split(\" \")\n    for word in words:\n        all_words.append(word)\n\nprint(all_words[0:19])\n\n['this', 'is', 'just', 'to', 'say', 'by', 'william', 'carlos', 'williams', 'i', 'have', 'eaten', 'the', 'plums', 'that', 'were', 'in', 'the', 'icebox']",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/programming/03_python_plus.html#a-slightly-nicer-way",
    "href": "notes/programming/03_python_plus.html#a-slightly-nicer-way",
    "title": "More Python: Poetry, Lists, Loops",
    "section": "A slightly nicer way",
    "text": "A slightly nicer way\nA slightly nicer way to do some of the operations we worked on here is with “list comprehensions”. These are special instructions to build a list that wind up saving us a bit of typing.\nHere’s the code we used to make all of the lines lowercase again.\n\n\npython\n\nlower_lines = []\n\nfor line in plums_lines:\n    lower_lines.append(line.lower())\n\nWith a list comprehension, we could rewrite this for-loop like so:\n\n\npython\n\nlower_lines2 = [\n    line.lower() \n    for line in plums_lines\n]\nlower_lines2\n\n['this is just to say',\n 'by william carlos williams',\n '',\n 'i have eaten',\n 'the plums',\n 'that were in',\n 'the icebox',\n '',\n 'and which',\n 'you were probably',\n 'saving',\n 'for breakfast',\n '',\n 'forgive me',\n 'they were delicious',\n 'so sweet',\n 'and so cold']\n\n\nWe can even include the empty line filtering into the list comprehension.\n\n\npython\n\nfull_lines2 = [\n    line.lower() \n    for line in plums_lines \n    if len(line)&gt;0\n]\n\nfull_lines2\n\n['this is just to say',\n 'by william carlos williams',\n 'i have eaten',\n 'the plums',\n 'that were in',\n 'the icebox',\n 'and which',\n 'you were probably',\n 'saving',\n 'for breakfast',\n 'forgive me',\n 'they were delicious',\n 'so sweet',\n 'and so cold']\n\n\nNested for statements also work inside of list comprehensions.\n\n\npython\n\nall_words2 = [\n    word\n    for line in full_lines2\n    for word in line.split(\" \")\n]\n\nall_words[0:19]\n\n['this',\n 'is',\n 'just',\n 'to',\n 'say',\n 'by',\n 'william',\n 'carlos',\n 'williams',\n 'i',\n 'have',\n 'eaten',\n 'the',\n 'plums',\n 'that',\n 'were',\n 'in',\n 'the',\n 'icebox']",
    "crumbs": [
      "Notes",
      "Programming",
      "More Python: Poetry, Lists, Loops"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html",
    "href": "notes/concepts/02_undersdanding-paths.html",
    "title": "Understanding Directories",
    "section": "",
    "text": "There has recently been an uptick in discussion among college professors that students are less familiar with the notions of files, folders, directories, and paths than they used to be in the past. This is in part a natural consequence of shifting paradigms in computer use. Many people are exclusively using online services like Google Docs, and don’t save files to their computers ever. And even when we do save files on out computer, many operating systems have search based approaches to finding files.\nThe problem for scientific computing purposes, or even just making some simple HTML pages on your local computer, is that you need to be able to navigate the hierarchical file system of your computer, in order to tell your programs how to navigate it.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#directory-structure",
    "href": "notes/concepts/02_undersdanding-paths.html#directory-structure",
    "title": "Understanding Directories",
    "section": "",
    "text": "There has recently been an uptick in discussion among college professors that students are less familiar with the notions of files, folders, directories, and paths than they used to be in the past. This is in part a natural consequence of shifting paradigms in computer use. Many people are exclusively using online services like Google Docs, and don’t save files to their computers ever. And even when we do save files on out computer, many operating systems have search based approaches to finding files.\nThe problem for scientific computing purposes, or even just making some simple HTML pages on your local computer, is that you need to be able to navigate the hierarchical file system of your computer, in order to tell your programs how to navigate it.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#files-in-folders",
    "href": "notes/concepts/02_undersdanding-paths.html#files-in-folders",
    "title": "Understanding Directories",
    "section": "Files in Folders",
    "text": "Files in Folders\nThe first thing to be clear about is that all files you’ve saved on your computer are in a folder (a.k.a. a directory). “Documents” or “My Documents” is a folder. Even your Desktop is a folder. This is true for both macOS and Windows. For example, I have an account on my laptop called “Display” for when I want to show how my computer works without showing everyone all of my personal files. In the Display account, I saved a little text file called Sampletext.txt to the desktop. In this screenshot, you can see Sampletext.txt shown both on my desktop, and as a file in a folder.\n\n\n\nDesktop is a folder\n\n\nWhat’s nice about this image is that you can see where Sampletext.txt exists in the context of my whole hard drive. My “Desktop” is just a folder that my computer shows the content of on my main screen. The Desktop folder is inside of a folder called “Display”. There are other folders in there, including my main user account, joseffruehwald. All of these folders are grouped together in the “Users” folder, and the “Users” folder is one of the first folders immediately underneath my hard drive.\nThe important thing to keep in mind here is that all files, programs, photos, etc are stored somewhere in a folder, inside of another folder, probably inside of another folder. You might not always interact with the files this way, and they might not always look like they’re in a folder, but they are.\n\nViewing the folder hierarchy on a Mac\nTo be able to better view the full hierarchy of folders on a Mac, I would recommend the following:\n\nOpen Finder, and in the View menu, select “as Columns”\nUnder the View menu again, select “Show Path Bar”\n\n\n\nViewing the folder hierarchy in Windows\nAt the top of the file browser on Windows, you’ll see the sequence of files in which the current folder is embedded.\n\n\n\nWindows directory structure",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#giving-directions-to-programs",
    "href": "notes/concepts/02_undersdanding-paths.html#giving-directions-to-programs",
    "title": "Understanding Directories",
    "section": "Giving Directions to Programs",
    "text": "Giving Directions to Programs\nPrograms are going to need directions for where to find things, and we can do this with “paths”. The concept of a path is so important, I’m going to really emphasize it\n“paths” \nThe “path” to a file is a written instruction for where to find a file on your specific computer.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#relative-paths",
    "href": "notes/concepts/02_undersdanding-paths.html#relative-paths",
    "title": "Understanding Directories",
    "section": "Relative Paths",
    "text": "Relative Paths\nSometimes, we only need to tell a file where another file is relative to itself. For example, here is an HTML document and a CSS stylesheet for that document.\n\n\n\nhtml and css in a dicectory\n\n\nFrom site.html to style.css: style.css\nBecause style.css is in the same folder as site.html, we only need to tell site.html to look for style.css, like so\n\nhtml\n\n&lt;link \n  rel=\"stylesheet\" \n  type=\"text/css\" \n  href=\"style.css\"&gt;\nJust telling the html file to look for style.css, it’s going to look inside of, and only inside of the same folder as itself.\nHowever, sometimes people like to put all of the stylesheets and images for a website inside of a folder called “assets,” like so:\n\n\n\nstylesheet in assets\n\n\nFrom site.html to style.css: assets/style.css\nNow, if we tell site.html just to look for style.css, it’s not going to find it anymore! That’s because style.css is no longer in the same folder as site.html. We’ll need to update the link to the stylesheet to reflect the fact that it’s now in a different folder, like so:\n\nhtml\n\n&lt;link \n  rel=\"stylesheet\" \n  type=\"text/css\" \n  href=\"assets/style.css\"&gt;\nHere’s how site.html interprets the string assets/style.css:\n\nLook inside the folder you’re currently in for a folder called assets\nThen, look inside of that folder for a file called style.css\n\nWe could take it a step further, and create a new folder called “css” inside of “assets”, and put our stylesheet there instead.\n\n\n\n`assets/css/style.css`\n\n\nFrom site.html to style.css: assets/css/style.css\nNow, we’d need to update the link to the stylesheet to look like this\n\nhtml\n\n&lt;link \n  rel=\"stylesheet\" \n  type=\"text/css\" \n  href=\"assets/css/style.css\"&gt;\nAgain, breaking this down into how site.html interprets the string assets/css/style.css goes like this:\n\nLook inside of the same folder as yourself for a folder called assets\nLook inside of that folder for a folder called css\nLook inside of that folder for a file called style.css",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#absolute-paths",
    "href": "notes/concepts/02_undersdanding-paths.html#absolute-paths",
    "title": "Understanding Directories",
    "section": "Absolute Paths",
    "text": "Absolute Paths\nRelative paths define the location of a file relative to another file, usually the one you’re writing. However, we can also define the location of a file on our computer that will work no matter where we’re looking from. These are called “absolute paths.” Put simply, absolute paths just list all of the nested folders a file is in, all the way up to the hard drive. You can see the absolute path for style.css at the bottom of the very last screen shot.\nDifferent operating systems have different short hands for describing the top level, or hard drive level, of the path. On macOS, it’s simply a / at the beginning of the path. On Windows, it usually begins with C:\\. For the final location of style.css in the screenshot above, the absolute path would be.\n/Users/Display/Documents/html_practice/assets/css/style.css\nIf for some reason I didn’t want to move or copy this stylesheet from this location, and I wanted to link to it from an HTML file saved in some completely different location on my laptop, I could give it this absolute path, and it would find it.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#important-note-on-naming",
    "href": "notes/concepts/02_undersdanding-paths.html#important-note-on-naming",
    "title": "Understanding Directories",
    "section": "Important Note on Naming",
    "text": "Important Note on Naming\nFrom here on out in the course, you should avoid using spaces in any of the files or folders you name. While you can take spaces into account when giving a path, it’s annoying. Instead, I’d recommend using an underscore _, a dash -, or naming files with “camel case”.\n\nmy_file.txt\nmy-file.txt\nmyFile.txt",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/02_undersdanding-paths.html#important-note-on-using-paths",
    "href": "notes/concepts/02_undersdanding-paths.html#important-note-on-using-paths",
    "title": "Understanding Directories",
    "section": "Important Note on Using Paths",
    "text": "Important Note on Using Paths\nThere are two crucial things to remember about using paths that can be conceptually difficult when first getting started.\n\nRelative paths always depend on where you’re coming from and where you’re going to. If either my HTML file was in a location I was confused about, or my CSS file was in a location I was confused about, my relative path might not work.\nBoth relative paths and absolute paths depend on how files are organized inside of your own computer. A path copied from an example online, or a professor’s template might not work  if your file structure is different.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Understanding Directories"
    ]
  },
  {
    "objectID": "notes/concepts/00_glossary.html",
    "href": "notes/concepts/00_glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Case Folding\n\nTo convert text data either to all uppercase or lowercase characters.\n\nClone\n\nMaking a copy of a git repository on another computer, whether locally or remotely.\n\nCopy-Paste\n\nCopying text to your computer’s ‘clipboard’ and pasting it into another location.\nCopying can be done many ways, including selecting text with your mouse, right-clicking then selecting ‘copy’, by selecting text and using your computer’s ‘hotkey’ for copying (), or, in the course documents, by clicking on the ‘copy’ icon in most code chunks.\nTo paste, you can either right-click and select ‘Paste’, or use your system’s ‘hotkey’ ().\n\nFind, or \n\nYou can find text in most pages or documents by either typing , then typing in the text you want to find in the dialogue box that pops up.\n\nFormal\n\nIn computational linguistics, ‘formal’ isn’t the opposite of ‘casual’. It’s about any system or language which has a strict set of acceptable symbols, and how they can be combined.\n\nGitHub Codespaces\n\nGitHub Codespaces are a ‘development environment’, running on a Virtual Machine. They’re like mini computers you can access through your web browser that get created just for the repository you launch one in. The actual memory and computation are being run on GitHub’s physical computers.\n\nIndex (noun)\n\nA value (usually an integer) that denotes the location of a valuable in an indexable object.\n\nIndex (verb)\n\nTo use a numeric index on an object to return the value at that index.\n\nLocal (adj)\n\nDescribes anything that exists or happens on the actual device you’re interacting with. For example, if you have a Microsoft Word document on your computer that ends in .docx, it is probably ‘local’ (even if theres a ‘remote’ copy somewhere). These days, it’s not always obvious if things happen locally or remotely. If you have a phone that does facial recognition on pictures you take, if the facial recognition program runs on your phone, it’s ‘local’. But if the picture needs to get uploaded to Apple or Google first, then the facial recognition is ‘remote’.\n\nRemote (adj)\n\nDescribes anything that exists or happens on a different device than the one you’re interacting with. For example, if you edit a Google Doc, the actual document is stored on a Google computer, not on the device you’re editing.\n\nSlice (verb)\n\nTo pull out a range of values from an object, defined by a starting and stopping position.\n\nTab Autocomplete\n\nMany programming applications have “Tab Autocomplete” suggestions that allow you to type in just a few letters of the command, function, or variable you want to use, and then hit TabTab.\n\nTerminal\n\nA text based interface to your computer. Instead of clicking on visual icons to, say, open a folder and look at its contents, you type in text based commands like cd and ls.\n\nVirtual Machine/VM\n\nVirtual Machines are simulations of computers. It’s like having an encapsulated computer, running its own operating system and programs, that borrows memory and computing resources from computers it’s running on.\n\ndictionary\n\nIn programming languages, a dictionary is a data structure that stores key, value pairs, and lets you retrieve a value with a given key.\n\n\n\n Back to topReuseCC-BY-SA 4.0CitationBibTeX citation:@online{fruehwald,\n  author = {Fruehwald, Josef},\n  title = {Glossary},\n  url = {https://lin511-2024.github.io/notes/concepts/00_glossary.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFruehwald, Josef. n.d. “Glossary.” https://lin511-2024.github.io/notes/concepts/00_glossary.html.",
    "crumbs": [
      "Notes",
      "Concepts",
      "Glossary"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html",
    "href": "notes/concepts/01_what-is-python.html",
    "title": "What is Python?",
    "section": "",
    "text": "When talking about “Python” or other programming langauges, we often engage in a form of polysemy1 that can be confusing at first. I’ll try to unpack the different meanings here.",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#python-is-set-of-rules-for-a-formal-language",
    "href": "notes/concepts/01_what-is-python.html#python-is-set-of-rules-for-a-formal-language",
    "title": "What is Python?",
    "section": "📕 Python is set of rules for a formal language",
    "text": "📕 Python is set of rules for a formal language\nWhen we talk about “Python”, one thing we sometimes refer to is an abstract set of rules for a formal language.\n\n\n\n\n\n\n📕 The abstract language\n\n\n\nWhen you say:\n\nI’m learning Python this semester.\n\nYou’re talking about the abstract set of rules.\n\n\nWe can describe some of these rules in natural language (this isn’t how they’re actually written).\n\nIf a list of values has been assigned to a variable, when the variable’s name is followed [0], return the first value in the list.\nIf print( is followed by a value or variable, which is then followed by ), print the value, or the value assigned to the variable.\nIf a # symbol is encountered on a line, ignore everything following it.\n\nThese rules define very precisely how code should be written, and also define what should be done when certain patterns of code are encountered, but these rules are abstract, and don’t actually do anything.",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#python-is-a-program-that-interprets-formal-language-input",
    "href": "notes/concepts/01_what-is-python.html#python-is-a-program-that-interprets-formal-language-input",
    "title": "What is Python?",
    "section": "🤖 Python is a program that interprets formal language input",
    "text": "🤖 Python is a program that interprets formal language input\nSimply having a set of abstract rules can be fun, but it’s not so useful if you have no way to actually do things. To make code we write actually do anything, we need to use a program called an Interpreter.\n\n\n\n\n\n\n🤖 The Python Interpreter\n\n\n\nWhen you say\n\nI’m having a hard time installing Python.\n\nyou’re talking about a Python Interpreter.\n\n\nThe Python interpreter processes code, determines whether or not it is valid, then does the things the code told it to do.\nCode can get passed to the interpreter many different ways, including\n\nIn a single text file, called a python “script”, (usually named to end in .py)\nIn an interactive python “shell”.\nInside a Jupyter notebook.\n\nFor example, the text in the shaded block below was passed to a Python interpreter, and the interpreter decided that it was invalid (because there is an opening quote \", but not a closing quote).\n\n\n\npython\n\n\nPython Code\n\n\"x\n\n\nSyntaxError: unterminated string literal (detected at line 1) (20073044.py, line 1)\n\n\nThe text in this next shaded block was passed to a Python interpreter. This time, it was deemed valid, and the interpreter did what the code instructed.\n\n\npython\n\nprint(\"hello! \" * 3)\n\nhello! hello! hello!",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#python-is-an-ecosystem",
    "href": "notes/concepts/01_what-is-python.html#python-is-an-ecosystem",
    "title": "What is Python?",
    "section": "⛰️ Python is an Ecosystem",
    "text": "⛰️ Python is an Ecosystem\nPeople often say that Python comes “with batteries included”, meaning it can do a lot of different things out without needing to install any extensions. But you can install extensions in the form of additional libraries that extend Python’s capabilities.\n\n\n\n\n\n\n⛰️ The Ecosystem\n\n\n\nWhen someone says\n\nPython is the most widely used language in natural language processing.\n\nThey are talking about the ecosystem of libraries for Python hat have been built to do natural language processing.\n\n\nSome examples of additional libraries for Python are\n\nnumpy: Allows for effective computation with numbers\nmatplotlib: Makes graphs\n\n\n\npython\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 2 * np.pi, 200)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\nplt.show()",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#python-is-a-social-project",
    "href": "notes/concepts/01_what-is-python.html#python-is-a-social-project",
    "title": "What is Python?",
    "section": "🧑‍💻 Python is a social project",
    "text": "🧑‍💻 Python is a social project\nThe meanings of “Python” we’ve already talked about are\n\n📕 The abstract language\n🤖 The Python Interpreter\n⛰️ The Python Ecosystem\n\nUnderstanding how to put all of these pieces together effectively is a social project.\n\n\n\n\n\n\n🧑‍💻 The social project\n\n\n\nWhen you say\n\nSo-n-so is really good at Python.\n\nYou are talking about their involvement in the Python social project.\n\n\n\n❌ There is no part of the 📕 abstract language that will inform you that there is a very good Python library for doing a certain task.\n❌ The 🤖 Python Interpreter won’t tell you that there’s a better way to do what you’re trying to do.\n❌ There are nearly half a million libraries on ⛰️ the Python Package Index, so you can’t just peruse it to find what’s relevant.\n\nIn order to know newest or most effective way to do things, you need to be involved in 🧑‍💻 the social project. That begins within a classroom, then extends to keeping up with blogs and other discourses about changes and developments.\nOne way people demonstrate that they are involved in the Python social project is by writing their code idiomatically. For example, in the code block above, I included the line import numpy as np. But there’s no rule that we import numpy as np, it’s simply convention.\nThis code below will run just as well, but would cause problems for people trying to read your code, because it’s not idiomatic.\n\n\npython\n\nimport matplotlib.pyplot as jokes\nimport numpy as farts\n\nx = farts.linspace(0, 2 * farts.pi, 200)\ny = farts.sin(x)\n\nfig, ax = jokes.subplots()\nax.plot(x, y)\njokes.show()",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#the-polysemy-is-ok",
    "href": "notes/concepts/01_what-is-python.html#the-polysemy-is-ok",
    "title": "What is Python?",
    "section": "📕, 🤖, ⛰️, 🧑‍💻 The Polysemy is Ok",
    "text": "📕, 🤖, ⛰️, 🧑‍💻 The Polysemy is Ok\nIt’s ok just say “Python” when you’re talking about one specific meaning.",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/concepts/01_what-is-python.html#footnotes",
    "href": "notes/concepts/01_what-is-python.html#footnotes",
    "title": "What is Python?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA word is polysemous when it has more than one meaning.↩︎",
    "crumbs": [
      "Notes",
      "Concepts",
      "What *is* Python?"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html",
    "href": "notes/meetings/04_ngrams.html",
    "title": "ngrams",
    "section": "",
    "text": "So, in our notes on finite state automata and push-down automata we concluded that since natural language has bracket matching patterns, and maybe even crossing dependencies, that it’s more complex than a “regular” language, and can’t really be parsed with a finite state automaton.\nngram language modelling asks the question: But what if we tried really hard?\nTrue\n\n[nltk_data] Downloading package punkt to /Users/runner/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#states-and-words",
    "href": "notes/meetings/04_ngrams.html#states-and-words",
    "title": "ngrams",
    "section": "States and Words",
    "text": "States and Words\n\n\n\n\n\n\n\n\n\n\n\n\nThe first sentence of Moby Dick is, famously,\n\nCall me Ishmael.\n\nWe could try representing this as a finite state automaton like so:\n\n\n\n\n\nstateDiagram\n    direction LR\n\n    [*] --&gt; call: call\n    call --&gt; me: me\n    me --&gt; ishmael: ishmael\n    ishmael --&gt; [*]\n\n\n\n\n\n\nBut, this is far from a complete model of the whole book Moby Dick. It would only work if the entire book was just “Call me Ishmael” over and over again.\nTo enrich the FSA, we could add all of the other words that could follow “call”.\n\n\n\n\n\nstateDiagram\n    direction LR\n\n    [*] --&gt; call: call\n    call --&gt; call: call\n    call --&gt; it: it\n    call --&gt; a: a\n    call --&gt; me: me\n    call --&gt; him: him\n    call --&gt; ...\n    call --&gt; [*]\n\n    me --&gt; ishmael: ishmael\n    me --&gt; call: call\n\n    ishmael --&gt; [*]\n\n\n\n\n\n\nBut, lots of other words could also follow “me”. And more words could also follow “Ishmael”\n\n\n\n\n\nstateDiagram\n    direction LR\n\n    [*] --&gt; call: call\n    call --&gt; call: call\n    call --&gt; it: it\n    call --&gt; a: a\n    call --&gt; me: me\n    call --&gt; him: him\n    call --&gt; ...\n    call --&gt; [*]\n\n    me --&gt; me: me\n    me --&gt; call: call\n    me --&gt; that: that\n    me --&gt; and: and\n    me --&gt; ishmael: ishmael\n    me --&gt; to: to\n    me --&gt; ...\n    me --&gt; [*]\n\n    ishmael --&gt; ishmael: ishmael\n    ishmael --&gt; me: me\n    ishmael --&gt; can: can\n    ishmael --&gt; said: said\n    ishmael --&gt; ...\n    ishmael --&gt; [*]\n\n\n\n\n\n\nIf we fully fleshed out this diagram with all1 of words in Moby Dick, it would look like this\n\n\n\n\n\n\n\n\nFigure 1: Bigram network for Moby Dick, where \\(C(w_1, w_2)\\ge 5\\)\n\n\n\n\n\n\n*grams\nThe “model” of word sequences is called an “ngram” model or more specifically a “bigram” model.\n\nHow we name *gram models\n\n\nWords in the current state\nWords in input\nTotal Words\nName\n\n\n\n\n1\n1\n2\nbigram\n\n\n2\n1\n3\ntrigram\n\n\n3\n1\n4\n4-gram\n\n\n4\n1\n5\n5-gram\n\n\n\n\n\netc\n\n\n\nWe can expand the context of the bigram model to a trigram model, which would look something like this\n\n\n\n\n\nstateDiagram\n  direction LR\n  \n  state \"_ call\" as _c\n  state \"call me\" as cm\n  state \"me Ishmael\" as mi\n  \n  [*] --&gt; _\n  _ --&gt; _c: call\n  _c --&gt; cm: me\n  cm --&gt; mi: Ishmael\n  mi --&gt; [*]\n  \n\n\n\n\n\n\nBut, again, even for this small vocabulary, this total number of states is incomplete. If we wired up all of the logical transitions they’d look like this ilithid monstrosity.\n\n\n\n\n\nstateDiagram\n    direction LR\n\n    state \"_ call\" as _c\n    state \"_ me\" as _m\n    state \"_ Ishmael\" as _i\n    \n    state \"call me\" as cm\n    state \"call call\" as cc\n    state \"call Ishmael\" as ci\n\n    state \"me Ishmael\" as mi\n    state \"me call\" as mc\n    state \"me me\" as mm\n\n    state \"Ishmael call\" as ic\n    state \"Ishmael Ishmael\" as ii\n    state \"Ishmael me\" as im\n\n    [*] --&gt; _: _\n    _ --&gt; _m: me\n    _ --&gt; _c: call\n    _ --&gt; _i: Ishmael\n    \n\n    _c --&gt; cm: me\n    _c --&gt; cc: call\n    _c --&gt; ci: Ishmael\n\n    cc --&gt; cc: call\n    cc --&gt; ci: Ishmael\n    cc --&gt; cm: me\n\n    ci --&gt; ii: Ishmael\n    ci --&gt; im: me\n    ci --&gt; ic: call\n\n    cm --&gt; mi: Ishmael\n    cm --&gt; mm: me\n    cm --&gt; mc: call\n\n    _m --&gt; mc: call\n    _m --&gt; mm: me\n    _m --&gt; mi: Ishmael\n    \n    mm --&gt; mm: me\n    mm --&gt; mc: call\n    mm --&gt; mi: Ishmael\n\n    mi --&gt; ii: Ishmael\n    mi --&gt; ic: call\n    mi --&gt; im: me\n\n    mc --&gt; cm: me\n    mc --&gt; ci: Ishmael\n    mc --&gt; cc: call\n\n\n\n    _i --&gt; ic: call\n    _i --&gt; im: me\n    _i --&gt; ii: Ishmael\n\n    ic --&gt; cc: call\n    ic --&gt; cm: me\n    ic --&gt; ci: Ishmael\n\n    im --&gt; mi: Ishmael\n    im --&gt; mc: call\n    im --&gt; mm: me\n\n    ii --&gt; im: me\n    ii --&gt; ic: call\n    ii --&gt; ii: Ishmael\n    \n\n\n\n\n\n\n\n\nProbabilistic ngrams\nBut, if we look at the actual entire book Moby Dick, not all of these connections are equally likely.\n\n\n\n\n\n\n\n\nnext word\nword\n\n\ncall\nme\nishmael\n\n\n\n\ncall\n0\n1\n0\n\n\nishmael\n0\n1\n0\n\n\nme\n3\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n%% fig-align: center\nflowchart LR\n    c[\"call\"] ==&gt; m[\"me\"]\n    m --&gt; c\n    m --&gt; i[\"Ishmael\"]",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#terminology-and-notation-moment",
    "href": "notes/meetings/04_ngrams.html#terminology-and-notation-moment",
    "title": "ngrams",
    "section": "Terminology and Notation Moment",
    "text": "Terminology and Notation Moment\n\nTypes vs Tokens\n\n\nA function to get all of the words in Moby Dick\n\n\n\npython\n\nimport re\nimport gutenbergpy.textget\nfrom nltk.tokenize import word_tokenize\n\ndef get_moby_dick_words():\n  raw_book = gutenbergpy.textget.get_text_by_id(2701) # with headers\n  moby_dick_byte = gutenbergpy.textget.strip_headers(raw_book) # without headers\n  moby_dick = moby_dick_byte.decode(\"utf-8\")\n  moby_dick_tokens = word_tokenize(moby_dick)\n  moby_dick_words = [tok for tok in moby_dick_tokens]\n  \n  return moby_dick_words\n\n\n\n\n\npython\n\nmoby_dick_words = get_moby_dick_words()\nfor idx, word in enumerate(moby_dick_words[0:20]):\n  print(word, end = \", \")\n  if (idx+1) % 5 == 0:\n     print(\"\")\n\nMOBY-DICK, ;, or, ,, THE, \nWHALE, ., By, Herman, Melville, \nCONTENTS, ETYMOLOGY, ., EXTRACTS, (, \nSupplied, by, a, Sub-Sub-Librarian, ), \n\n\n\nWe can get counts of how often each word appeared in the book with collections.Counter() .\n\n\n\npython\n\nfrom collections import Counter\n\nword_count = Counter(moby_dick_words)\n\n\nLet’s compare the length of the full list of words to the length of the word count dictionary.\n\n\n\npython\n\nprint(f\"There are {len(moby_dick_words):,} total  words.\")\n\nThere are 255,958 total  words.\n\n\npython\n\nprint(f\"There are  {len(word_count):,} unique words.\")\n\nThere are  21,897 unique words.\n\n\n\n\n\n\n\n\n\nTerminology\n\n\n\nIn more common corpus/compling terminology, we would say\n\nThere are 215,300 tokens in Moby Dick.\nThere are 19,989 types in Moby Dick.\n\n\n\nWe can get the frequency of the words “whale” and “ogre” in Moby Dick like so:\n\n\n\n\n\n\nIndexing with a string?\n\n\n\n\n\nWe can index word_count with the string \"whale\" because it is a “dictionary” We could create our own dictionary like this:\n\n\npython\n\nfood_type = {\n  \"banana\": \"fruit\",\n  \"strawberry\": \"fruit\",\n  \"carrot\": \"vegetable\",\n  \"onion\": \"vegetable\"\n}\n\nfood_type[\"banana\"]\n\n'fruit'\n\n\n\n\n\n\n\n\npython\n\nprint(f\"The word 'whale' appeared {word_count['whale']:,} times.\")\n\nThe word 'whale' appeared 771 times.\n\n\npython\n\nprint(f\"The word  'ogre' appeared    {word_count['ogre']} times.\")\n\nThe word  'ogre' appeared    0 times.\n\n\n\n\n\n\n\n\n\nTerminology\n\n\n\nThe way we’d describe this in a more corpus/comp-ling way is\n\nThe word type “whale” appears in Moby Dick. There are 1,070 tokens of “whale” in the book.\nThe word type “ogre” does not appear in Moby Dick.\n\n\n\n\n\nNotation\n\nWords and variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nCall\nme\nIshmael\n.\nSome\nyears\n…\n\n\n\nmath standin\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\n\\(w_5\\)\n\\(w_6\\)\n\n\\(w_i\\)\n\n\n\n\n\n\n\n\n\nMath Notation\n\n\n\n\n\\(w_2\\)\n\nLiterally the second word in a sequence.\n\n\\(w_i\\)\n\nThe \\(i\\)th word in the sequence (that is, any arbitrary word).\n\n\n\n\n\n\nCounting Words\n\n\n\n\n\n\nCounts of each type in Moby Dick\n\n\n\n\n\n\n\ntoken\nn\nmath\n\n\n\n\nCall\n3\n\\(C(w_1) = 3\\)\n\n\nme\n607\n\\(C(w_2) = 607\\)\n\n\nIshmael\n18\n\\(C(w_3) = 18\\)\n\n\n.\n7164\n\\(C(w_4) = 7164\\)\n\n\nSome\n38\n\\(C(w_5) = 38\\)\n\n\nyears\n91\n\\(C(w_6) = 91\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath Notation\n\n\n\n\n\\(C()\\)\n\nA function for the “C”ount of a value.\n\n\\(C(w_1)\\)\n\nThe frequency of the type of the first token\n\n\\(C(w_i)\\)\n\nThe frequency of an arbitrary type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntoken\nnext token\nn\nmath\n\n\n\n\nCall\nme\n1\n\\(C(w_1w_2) = 1\\)\n\n\nme\nIshmael\n1\n\\(C(w_2w_3) = 1\\)\n\n\nIshmael\n.\n3\n\\(C(w_3w_4) = 3\\)\n\n\n.\nSome\n27\n\\(C(w_4w_5) = 27\\)\n\n\nSome\nyears\n1\n\\(C(w_5w_6) = 1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath Notation\n\n\n\n\n\\(C(w_1w_2)\\)\n\nThe count of times the sequence \\(w_1w_2\\) occured.\n\n\\(C(w_iw_{i+1})\\)\n\nThe count of times an arbitrary 2 word sequence appeared\n\n\\(C(w_{i-1}w_i)\\)\n\nSame as before, but with emphasis on the second word.\n\n\n\n\n\n\nProbabilities\n\n\n\n\n\n\nMath Notation\n\n\n\n\n\\(P(w_1)\\)\n\nThe probability of the first word\n\n\\(P(w_i)\\)\n\nThe probability of an arbitrary word\n\n\\(P(w_2|w_1)\\)\n\nThe probability that we’ll get word 2 coming after word 1\n\n\\(P(w_i|w_{i-1})\\)\n\nThe probability we’ll get any arbitrary word coming after the word before.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#language-prediction",
    "href": "notes/meetings/04_ngrams.html#language-prediction",
    "title": "ngrams",
    "section": "Language Prediction",
    "text": "Language Prediction\nWhen we are perceiving language, we are constantly and in real-time making predictions about what we are about to hear next. While we’re going to be talking about this in terms of predicting the next word, It’s been shown that we do this even partway through a word (Allopenna, Magnuson, and Tanenhaus 1998).\nSo, let’s say I spoke this much of a sentence to you:\n\nI could tell he was angry from the tone of his___\n\nAnd then a sudden noise obscured the final word, and you only caught part of it. Which of the following three words was I probably trying to say?\n\nboys\nchoice\nvoice\n\nYour ability to guess which word it was is based on your i) experience with English turns of phrase and ii) the information in the context.\nOne goal of Language Models is to assign probabilities across the vocabulary for what the next word will be, and hopefully assign higher probabilities to the “correct” answer than the “incorrect” answer. Applications for this kind of prediction range from speech-to-text (which could suffer from a very similar circumstance as the fictional one above) to autocomplete or spellcheck.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#using-context-ngrams",
    "href": "notes/meetings/04_ngrams.html#using-context-ngrams",
    "title": "ngrams",
    "section": "Using context (ngrams)",
    "text": "Using context (ngrams)\nIn the example sentence above, one way we could go about trying to predict which word is most likely is to count up how many times the phrase “I could tell he was angry from the tone of his___” is finished by the candidate words. Here’s a table of google hits for the three possible phrases, as well as all hits for just the context phrase.\n\n\n\n“I could tell he was angry from the tone of his”\ncount\n\n\n\n\nboys\n0\n\n\nchoice\n0\n\n\nvoice\n3\n\n\n“I could tell he was angry from the tone of his”\n3\n\n\n\nWe’re going to start diving into mathematical formulas now (fortunately the numbers are easy right now).\nTo represent the count of a word or string of words in a corpus. We’ll use \\(C(\\text{word})\\). So given the table above we have\n\n\n\n\n\n\n\n\n\\(C(\\text{I could tell he was angry from the tone of his})\\)\n=\n3\n\n\n\\(C(\\text{I could tell he was angry from the tone of his boys})\\)\n=\n0\n\n\n\\(C(\\text{I could tell he was angry from the tone of his choice})\\)\n=\n0\n\n\n\\(C(\\text{I could tell he was angry from the tone of his voice})\\)\n=\n3\n\n\n\nTo describe the probability that the next word is “choice” given that we’ve already heard “I could tell he was angry from the tone of his”, we’ll use the notation \\(P(\\text{choice} | \\text{I could tell he was angry from the tone of his})\\). To calculate that probability, we’ll divide the total count of the whole phrase by the count of the preceding context.\n\\[\nP(\\text{choice} | \\text{I could tell he was angry from the tone of his}) = \\frac{C(\\text{I could tell he was angry by the tone of his choice})}{C(\\text{I could tell he was angry by the tone of his})} = \\frac{0}{3} = 0\n\\]\nOr, more generally:\n\\[\nP(w_i|w_{i-n}\\dots w_{i-1}) = \\frac{w_{i-n}\\dots w_i}{w_{i-n}\\dots w_{i-1}}\n\\]\nIn fact, we can estimate the probability of an entire sentence with the Probability Chain Rule. The probability of a sequence of events like \\(P(X_1X_2X_3)\\) can be estimated by multiplying out their conditional probabilities like so:\n\\[\nP(X_1X_2X_3) = P(X_1)P(X_2|X_1)P(X_3|X_1X_2)\n\\]\nOr, to use a phrase as an example:2\n\\[\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n\\]",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#data-sparsity",
    "href": "notes/meetings/04_ngrams.html#data-sparsity",
    "title": "ngrams",
    "section": "Data Sparsity",
    "text": "Data Sparsity\nThe problem we face is that, even with the whole internet to search, very long phrases like “I could tell he was angry by the tone of his” are relatively rare!\nIf we look at Moby Dick, using a standard tokenizer (more on that later) we wind up with 255,958 words in total. But not every word is equally likely.\n\n\n\n\n\n\n\n\nRank and Frequency of single words in Moby Dick\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd as the size of the ngrams increases, the sparsity gets worse.\n\n\n\n\n\nRank and Frequency of 2-grams through 5-grams in Moby Dick\n\n\n\n\nA “Hapax Legomenon” is a word or phrase that occurs just once in a corpus. If we look at the 2-grams through 5-grams in Moby Dick and make a plot of what proportion of tokens are hapax legomena, we can see that almost all 5grams appear just once.\n\n\n\n\n\nThe proportion of all tokens which are hapax legomena\n\n\n\n\n\nThe problem with data sparsity\nLet’s say we got the following sequence of 4 words, and I wanted to predict the 5th\n\n\n\na\nman\nis\nelevated\n?\n\n\n\\(w_1\\)\n\\(w_2\\)\n\\(w_3\\)\n\\(w_4\\)\n\\(w_5\\)\n\n\n\nSo, for each word type, I want to know\n\\[\nP(w_i | \\text{a man is elevated})\n\\]\nWe know this is going to be calculated with this formula:\n\\[\n\\frac{C(\\text{a man is elevated }w_i)}{C(\\text{a man is elevated)}}\n\\]\nFrom the 4gram counts, I’ll grab a table of the frequency of “a man is elevated”.\n\n\n\n\n\n\n\n\n4gram\nfreq\n\n\n\n\na man is elevated\n1\n\n\n\n\n\n\n\nIt looks like “a man is elevated” appeared just once, so it follows that the 5gram that starts with “a man is elevated” also appears just once.\n\n\n\n\n\n\n\n\n5gram\nfreq\n\n\n\n\na man is elevated in\n1\n\n\n\n\n\n\n\nIf we wanted to compare the probabilities of the words in and to in this context. we’d wind up with the following results.\n\\[\nP(\\text{in} | \\text{a man is elevated}) = \\frac{C(\\text{a man is elevated in})}{C(\\text{a man is elevated)}} = \\frac{1}{1} = 1\n\\]\n\\[\nP(\\text{to} | \\text{a man is elevated}) = \\frac{C(\\text{a man is elevated to})}{C(\\text{a man is elevated)}} = \\frac{0}{1} = 0\n\\]\nAccording to this 5gram model trained on Moby Dick, there’s a 0% chance that the next word could be “to”.\nIs that really reasonable?\n\n\nApproximating with ngrams\nInstead of using long ngrams, we can try approximating with shorter ngrams (known as the Markov Assumption).\n\\[\nP(\\text{in} | \\text{a man is elevated}) \\approx P(\\text{in} | \\text{elevated})\n\\]\n\n\n\n\n\n\n\n\nbigrams\nfreq\n\\(P(w_i)\\)\n\n\n\n\nelevated above\n2\n0.18\n\n\nelevated ,\n2\n0.18\n\n\nelevated out\n2\n0.18\n\n\nelevated in\n1\n0.09\n\n\nelevated quarter-deck\n1\n0.09\n\n\nelevated hump\n1\n0.09\n\n\nelevated part\n1\n0.09\n\n\nelevated open-work\n1\n0.09",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#calculating-probabilities",
    "href": "notes/meetings/04_ngrams.html#calculating-probabilities",
    "title": "ngrams",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\nRemember this chain rule?\n\\[\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n\\]\nWe’d simplify this, like so:\n\\[\nP(\\text{du hast mich gefragt}) = P({\\text{du} | \\text{\\#}})P(\\text{hast} | \\text{du})P(\\text{mich} | \\text{hast})P(\\text{gefragt}|\\text{mich})\n\\]\n\nLog Probabilities\nThere’s an additional complication about how we represent probabilities. Let’s build a very probable 10 word string starting with “The”. I’ll just grab the most frequent \\(w_i\\) that comes after \\(w_{i-1}\\).3\n\n\n\n\n\n\n\n\n\n\n\nbigram\nfreq\n\\(P(w_i | w_{i-1})\\)\n\n\n\n\nThe Pequod\n15\n0.02\n\n\nPequod ’\n49\n0.28\n\n\n’ s\n1,784\n0.64\n\n\ns a\n73\n0.04\n\n\na little\n105\n0.02\n\n\nlittle ,\n11\n0.04\n\n\n, and\n2,630\n0.14\n\n\nand the\n361\n0.06\n\n\nthe whale\n325\n0.02\n\n\nwhale ,\n173\n0.22\n\n\n\n\n\n\n\n\n\n[1] \"&gt; The Pequod’s a little, and the whale,\"\n\n\nWe can calculate the cumulative probability of each next substring of the sentence.\n\n\n\n\n\n\n\n\nbigram\nfreq\n\\(P(w_i | w_{i-1})\\)\n\\(P(w_{i-n}\\dots w_i)\\)\n\n\n\n\nThe Pequod\n15\n0.02\n0.0216763006\n\n\nPequod ’\n49\n0.28\n0.0060693642\n\n\n’ s\n1,784\n0.64\n0.0038809124\n\n\ns a\n73\n0.04\n0.0001588042\n\n\na little\n105\n0.02\n0.0000036997\n\n\nlittle ,\n11\n0.04\n0.0000001648\n\n\n, and\n2,630\n0.14\n0.0000000226\n\n\nand the\n361\n0.06\n0.0000000014\n\n\nthe whale\n325\n0.02\n0.0000000000\n\n\nwhale ,\n173\n0.22\n0.0000000000\n\n\n\n\n\n\n\nI artificially clamped the number of decimal points that would show in the final column to 10, but because of the way computers represent decimal points, they also have a lower limit they can get to.\nTo avoid things getting weird with decimals that are too small, these probabilities will often be represented as log probabilities.\nIf you don’t remember how logarithms work, that’s ok. There’s just a few useful properties to remember.\n\\[\n\\log(x)  \\left\\{ \\begin{array}{c} &gt; 0; x&gt;1\\\\=0; x =1\\\\&lt;0; x&lt;1 \\end{array} \\right\\}\n\\]\n\\[\n\\log(0) = -\\infty\n\\]\n\\[\n\\log(x  y) = \\log(x) + \\log(y)\n\\]\n\\[\n\\log(\\frac{x}{y}) = \\log(x) - \\log(y)\n\\]\n\n\n\n\n\n\n\n\nbigram\nprobability\nlog(prob)\ntotal log(prob)\n\n\n\n\nThe Pequod\n0.02\n−3.83\n−3.83\n\n\nPequod ’\n0.28\n−1.27\n−5.10\n\n\n’ s\n0.64\n−0.45\n−5.55\n\n\ns a\n0.04\n−3.20\n−8.75\n\n\na little\n0.02\n−3.76\n−12.51\n\n\nlittle ,\n0.04\n−3.11\n−15.62\n\n\n, and\n0.14\n−1.99\n−17.61\n\n\nand the\n0.06\n−2.81\n−20.42\n\n\nthe whale\n0.02\n−3.74\n−24.16\n\n\nwhale ,\n0.22\n−1.49\n−25.66",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/04_ngrams.html#footnotes",
    "href": "notes/meetings/04_ngrams.html#footnotes",
    "title": "ngrams",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwell, almost all.↩︎\nCredit here to Kyle Gorman for introducing me to this example.↩︎\nIf we went longer than 10, we actually get \\(\\text{The Pequod's a little}\\overline{\\text{, and the whale}}\\)↩︎",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "ngrams"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html",
    "href": "notes/meetings/01_fsm.html",
    "title": "Regular Languages and Finite State Machines",
    "section": "",
    "text": "In another class, we might just jump straight into learning about “Regular Expressions”, which are very useful tools for searching for patterns in text.\nBut since this is Computational Linguistics, we should probably learn about what is “regular” about “regular” expressions, because it’s related to formal language theory!",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#where-were-going",
    "href": "notes/meetings/01_fsm.html#where-were-going",
    "title": "Regular Languages and Finite State Machines",
    "section": "",
    "text": "In another class, we might just jump straight into learning about “Regular Expressions”, which are very useful tools for searching for patterns in text.\nBut since this is Computational Linguistics, we should probably learn about what is “regular” about “regular” expressions, because it’s related to formal language theory!",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#formal-language-theory-and-abstract-machines",
    "href": "notes/meetings/01_fsm.html#formal-language-theory-and-abstract-machines",
    "title": "Regular Languages and Finite State Machines",
    "section": "“Formal Language Theory” and “Abstract Machines”",
    "text": "“Formal Language Theory” and “Abstract Machines”\n\nFormal Languages are systems of rules (i.e. a formal “grammar”) for combining a set of strictly defined symbols.\n\n\n\n\n\n\n\nA small formal language\n\n\n\n\n\\(\\mathcal{L}\\) is the language, which is a set that contains all, and only, the possible strings of \\(\\mathcal{L}\\).\n\\(\\Sigma\\) is the vocabulary of \\(\\mathcal{L}\\).\n\n\\(\\Sigma = \\{a,b\\}\\)\n\n\\(\\mathcal{N}\\) Are the “non-terminal nodes” of \\(\\mathcal{L}\\), of which we have 3.\n\n\\(\\mathcal{N} = \\{S, A, B\\}\\)\n\n\\(\\mathcal{G}\\) is the grammar of \\(\\mathcal{L}\\). It contains 4 rules\n\n\\(S \\rightarrow aA\\)\n\\(A \\rightarrow aB\\)\n\\(A \\rightarrow aA\\)\n\\(B \\rightarrow b\\)\n\n\nWith the sets \\(\\mathcal{N}\\), \\(\\mathcal{\\Sigma}\\), and \\(\\mathcal{G}\\), can we figure out which strings are, or are not in \\(\\mathcal{L}\\)?\n\n\\(ab\\)?\n\\(aab\\)?\n\\(b\\)?\n\\(aabb\\)?\n\\(abba\\)?\n\\(cab\\)?\n\n\n\n\n“Automata” and other abstract machines are ways of conceptualizing the possible or necessary computational operations and resources needed for different tasks, like identifying whether or not a string is in a language!\n\nWe’ll start with the least complex1 machines, “Finite State Automata”",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#finite-state-automata",
    "href": "notes/meetings/01_fsm.html#finite-state-automata",
    "title": "Regular Languages and Finite State Machines",
    "section": "Finite State Automata",
    "text": "Finite State Automata\n\nFinite\n\nHaving a fixed, or non infinite amount of things\n\nState\n\nThe current state, or status, of a system.\n\nAutomaton\n\nA machine\n\n\nFSAs have\n\na fixed number of states\na fixed number of inputs they can accept\nrules about how to change their state given an input.\n\n\nExamples\n\nA light switch.\n\n\n\n\n\nstateDiagram\n  direction LR\n  state \"Off\" as off\n  state \"On 💡\" as on\n  \n  [*] --&gt; off\n  on --&gt; off: switch flip\n  off --&gt; on: switch flip\n\n\n\n\n\n\n\nStates\n\nOn, Off\n\nInputs\n\nswitch flip\n\n\n\n\nA metro turnstile\n\n\n\n\n\nstateDiagram\n  direction LR\n  state \"Locked\" as l\n  state \"Unlocked\" as u\n  \n  [*] --&gt; l\n  l --&gt; l: push\n  l --&gt; u: coin\n  u --&gt; u: coin\n  u --&gt; l: push\n\n\n\n\n\n\n\nstates\n\nLocked, Unlocked\n\ninputs\n\ncoin, push\n\n\n\n\nA Github Repo!\n\n\n\n\n\nstateDiagram\n  state \"Local Synced with Remote\" as s\n  state \"Unstaged Changes\" as us\n  state \"All changes staged\" as st\n  state \"No uncommitted changes\" as com\n  [*] --&gt; s\n  s --&gt; us: Editing & Saving\n  us --&gt; us: Editing & Saving\n  us --&gt; st: Staging all changes\n  st --&gt; us: Editing & Saving\n  st --&gt; com: Committing\n  com --&gt; us: Editing & Saving\n  com --&gt; s: Push\n\n\n\n\n\n\n\nstates\n\nLocal Synced with Remote, Unstaged Changes, All Changes Staged, No Uncommitted Changes\n\ninputs\n\nEditing & Saving, Staging all Changes, Committing, Push\n\n\n\n\nThe Batman Theme Song\n\n\n\n\n\n\nBatman!\n\n\n\n\n\nNa na na na na na na na na na na na\nBatman!\nNa na na na na na na na na na na na\nBatman!\nNa na na na na na na na na na na na\nBatman!\nNa na na na na na na na na na na na\nBatman!\nNa na na na na na na na na na na na\nBatman! Batman! Batman!\nNa na na na na na na na na na na na\nBatman!\n\n\n\n\n\n\n\n\nstateDiagram\n  direction LR\n  [*] --&gt; a: Na\n  a --&gt; b: na\n  b --&gt; c: na\n  c --&gt; d: na\n  d --&gt; e: na\n  e --&gt; f: na\n  f --&gt; g: na\n  g --&gt; h: na\n  h --&gt; i: na\n  i --&gt; j: na\n  j --&gt; k: na\n  k --&gt; l: na\n  l --&gt; batman1: Batman!\n  batman1 --&gt; a: Na\n  batman1 --&gt; batman2: Batman!\n  batman2 --&gt; batman3: Batman!\n  batman3 --&gt; a: Na\n  batman1 --&gt; [*]: ε\n\n\n\n\n\n\n\nStates\n\na-l, batman1, batman2, batman3\n\nInputs\n\nNa, na, Batman!",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#automata-and-formal-grammars",
    "href": "notes/meetings/01_fsm.html#automata-and-formal-grammars",
    "title": "Regular Languages and Finite State Machines",
    "section": "Automata and Formal Grammars",
    "text": "Automata and Formal Grammars\nIf you have\n\nsome formal Grammar \\(\\mathcal{G}\\)\n\nthat you use to generate\n\nthe strings of language \\(\\mathcal{L}\\)\n\nThere is some Automaton (maybe not a finite state automaton!) that can decide whether nor not a string is in \\(\\mathcal{L}\\) or not.\nThis relationship between automata of various complexity and formal grammars/languages is called the “Chomsky Hierarchy.”",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#returning-to-our-small-formal-language",
    "href": "notes/meetings/01_fsm.html#returning-to-our-small-formal-language",
    "title": "Regular Languages and Finite State Machines",
    "section": "Returning to our small formal language",
    "text": "Returning to our small formal language\nHere’s the grammar rules we had for our grammar\n\n\\(\\mathcal{G}\\) is the grammar of \\(\\mathcal{L}\\). It contains 4 rules\n\n\\(S \\rightarrow aA\\)\n\\(A \\rightarrow aB\\)\n\\(A \\rightarrow aA\\)\n\\(B \\rightarrow b\\)\n\n\n\nUsing the grammar to generate a string\nWe can build up a string by choosing rules and applying them\n\n\nStart\n\n\n\n\n\n\nflowchart TD\n  S\n\n\n\n\n\n\n\n\n\n\n\n\n\\(S\\rightarrow aA\\)\n\n\n\n\n\n\n\nflowchart TD\n  S --&gt; a1[\"a\"]\n  S --&gt; A1[\"A\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\rightarrow aA\\)\n\n\n\n\n\n\n\nflowchart TD\n  S --&gt; a1[\"a\"]\n  S --&gt; A1[\"A\"]\n  A1 --&gt; a2[\"a\"]\n  A1 --&gt; A3[\"A\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\rightarrow aB\\)\n\n\n\n\n\n\n\nflowchart TD\n  S --&gt; a1[\"a\"]\n  S --&gt; A1[\"A\"]\n  A1 --&gt; a2[\"a\"]\n  A1 --&gt; A2[\"A\"]\n  A2 --&gt; a3[\"a\"]\n  A2 --&gt; B\n\n\n\n\n\n\n\n\n\n\n\n\n\\(B \\rightarrow b\\)\n\n\n\n\n\n\n\nflowchart TD\n  S --&gt; a1[\"a\"]\n  S --&gt; A1[\"A\"]\n  A1 --&gt; a2[\"a\"]\n  A1 --&gt; A2[\"A\"]\n  A2 --&gt; a3[\"a\"]\n  A2 --&gt; B\n  B --&gt; b\n\n\n\n\n\n\n\n\n\nResult: \\(aaab\\)\n\n\nDefining a FSA to recognize the language\n\n\n\n\n\nstateDiagram\n  direction LR\n  \n  [*]--&gt;p\n  p --&gt; q: a\n  q --&gt; q: a\n  q --&gt; [*]: b\n  \n\n\n\n\n\n\n\nIf we take strings, generated by \\(\\mathcal{G}\\) and feed them to this FSA one-by-one, we should arrive at the “accepting” state without any symbols left over.\nIf we take a string, generated by some unknown grammar, pass it through this FSA, and arrive at the “accepting” state without any symbols left over, then it could have been generated by \\(\\mathcal{G}\\).\nIf we take a string, and pass it through this FSA, and either never get to the accepting state, or have symbols left over, then it couldn’t have been generated by \\(\\mathcal{G}\\), and isn’t in \\(\\mathcal{L}\\).",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#what-does-this-have-to-do-with-regular-expressions",
    "href": "notes/meetings/01_fsm.html#what-does-this-have-to-do-with-regular-expressions",
    "title": "Regular Languages and Finite State Machines",
    "section": "What does this have to do with “Regular Expressions?”",
    "text": "What does this have to do with “Regular Expressions?”\n“Regular Expressions” are a concise, computery way to define finite state automata that take text as input strings and return matches. That is, regular expressions can identify strings that belong to “regular” languages.\nWe’ll talk about details later, but if I wasn’t sure of someone was using British spelling or American spelling, I could write a RegEx to match either color or colour.\n\n\ncolou?r\n\n\n\n\n\n\nstateDiagram\n  direction LR\n  [*] --&gt; 1\n  1 --&gt; 2: c\n  2 --&gt; 3: o\n  3 --&gt; 4: l\n  4 --&gt; 5: o\n  5 --&gt; 6: u\n  5 --&gt; [*]: r\n  6 --&gt; [*]: r\n\n\n\n\n\n\n\n\n\nLimitations of RegEx\nBecause Regular Expressions define a finite state automaton, and FSAs can only successfully recognize regular languages, that means you can’t use Regular Expressions (reliably) to recognize any more complex language.\nFor example, HTML pages use “tags” to define where text elements begin and end. In this code snippet, the opening &lt;p&gt; tag says a paragraph is beginning, and the closing &lt;/p&gt; says the paragraph is ending. The opening &lt;strong&gt; tag says the text is turning bold, and the closing &lt;/strong&gt; tag says the text is turning regular again.\n\nhtml\n\n&lt;p&gt;\n  This is a paragraph with \n  &lt;strong&gt;\n    bold text\n  &lt;/strong&gt;\n  .\n&lt;/p&gt;\n\nFor an html page to be valid, every opening tag, &lt;tag&gt; needs to have a matching closing tag &lt;/tag&gt;.\nYou can’t have a closing tag &lt;/tag&gt; without a preceding opening &lt;tag&gt;.\nYou you can embed another opening and closing tag set within another opening and closing tag set.\nThis kind of long distance “bracket matching”, where you can have intervening brackets, requires at least a push-down automaton to recognize.\nThat means html is a context-sensitive language.\nYou cannot use RegEx to successfully validate html.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#some-further-reading",
    "href": "notes/meetings/01_fsm.html#some-further-reading",
    "title": "Regular Languages and Finite State Machines",
    "section": "Some further reading",
    "text": "Some further reading\nJäger and Rogers (2012)",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/01_fsm.html#footnotes",
    "href": "notes/meetings/01_fsm.html#footnotes",
    "title": "Regular Languages and Finite State Machines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComplex in the sense of what they can do, not necessarily how easy it is to understand them.↩︎",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Regular Languages and Finite State Machines"
    ]
  },
  {
    "objectID": "notes/meetings/02_pda.html",
    "href": "notes/meetings/02_pda.html",
    "title": "Pushdown Automata and Context Free Languages",
    "section": "",
    "text": "In the notes on Finite State Automata, we looked at this turnstile finite state automaton.\n\n\n\n\n\nstateDiagram\n  direction LR\n  state \"Locked\" as l\n  state \"Unlocked\" as u\n  \n  [*] --&gt; l\n  l --&gt; l: push\n  l --&gt; u: coin\n  u --&gt; u: coin\n  u --&gt; l: push\n\n\n\n\n\n\nAn annoying thing about this turnstile is that if you don’t know how it works, it will rip you off!\n\n\n\n\n\n\nA scenario\n\n\n\nRobin approaches the finite-state turnstile with two of their friends. They think\n\nThere’s three of us, and I have three tokens. I’ll speed things up and be a good friend by popping three tokens into the machine, and then all three of us can pop through.\n\n\n\nRobin is expecting a pattern like this to happen\n, , , , , \nLittle does Robin know that the way this turnstile works is that after you put a coin into the slot, the coin rolls past and triggers the unlocking mechanism and goes straight into the collection bin. If the turnstile is already unlocked, the coin just rolls into the collection bin. It doesn’t have any “memory” of how many coins it’s been fed, so after one person walks through, the turnstile relocks.\nSo here’s what happens to Robin and their friends\n\n\n\n\n\n\n\nInput\nNew State\n\n\n\n\n\nLocked\n\n\n\nUnlocked\n\n\n\nUnlocked\n\n\n\nUnlocked\n\n\n\nLocked\n\n\n\nWith the turnstile locked again, Robin’s two friends can’t get through unless they insert yet another token!",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Pushdown Automata and Context Free Languages"
    ]
  },
  {
    "objectID": "notes/meetings/02_pda.html#more-like-a-rip-off-machine",
    "href": "notes/meetings/02_pda.html#more-like-a-rip-off-machine",
    "title": "Pushdown Automata and Context Free Languages",
    "section": "",
    "text": "In the notes on Finite State Automata, we looked at this turnstile finite state automaton.\n\n\n\n\n\nstateDiagram\n  direction LR\n  state \"Locked\" as l\n  state \"Unlocked\" as u\n  \n  [*] --&gt; l\n  l --&gt; l: push\n  l --&gt; u: coin\n  u --&gt; u: coin\n  u --&gt; l: push\n\n\n\n\n\n\nAn annoying thing about this turnstile is that if you don’t know how it works, it will rip you off!\n\n\n\n\n\n\nA scenario\n\n\n\nRobin approaches the finite-state turnstile with two of their friends. They think\n\nThere’s three of us, and I have three tokens. I’ll speed things up and be a good friend by popping three tokens into the machine, and then all three of us can pop through.\n\n\n\nRobin is expecting a pattern like this to happen\n, , , , , \nLittle does Robin know that the way this turnstile works is that after you put a coin into the slot, the coin rolls past and triggers the unlocking mechanism and goes straight into the collection bin. If the turnstile is already unlocked, the coin just rolls into the collection bin. It doesn’t have any “memory” of how many coins it’s been fed, so after one person walks through, the turnstile relocks.\nSo here’s what happens to Robin and their friends\n\n\n\n\n\n\n\nInput\nNew State\n\n\n\n\n\nLocked\n\n\n\nUnlocked\n\n\n\nUnlocked\n\n\n\nUnlocked\n\n\n\nLocked\n\n\n\nWith the turnstile locked again, Robin’s two friends can’t get through unless they insert yet another token!",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Pushdown Automata and Context Free Languages"
    ]
  },
  {
    "objectID": "notes/meetings/02_pda.html#a-pushdown-automaton",
    "href": "notes/meetings/02_pda.html#a-pushdown-automaton",
    "title": "Pushdown Automata and Context Free Languages",
    "section": "A Pushdown Automaton",
    "text": "A Pushdown Automaton\nRobin was really upset and embarrassed at losing two whole tokens to the rip-off (finite state) machine in front of their friends. They vowed to invent a better turnstile so no one would ever have to face that kind of embarrassment again.\n\nIncorporating a memory\nThe problem with the finite-state turnstile is that it has no “memory” of how many coins it’s been fed. Robin’s new prototype works like so:\n\nEvery time someone inserts a coin into Robin’s turnstile, it lands in a little collection tray. If someone inserts multiple coins, they form a stack of coins.\nIf there is even one coin in the stack, the turnstile is unlocked.\nAny time someone pushes through the turnstile, the collection tray bounces one coin off of the stack.\n\nEven this simple system gets a little unwieldy to represent in the same kind of state diagram. So, here’s the last one of these we’ll see for a bit.\n\n\n\n\n\nstateDiagram\n  direction LR\n  state \"Locked\" as l\n  state \"Unlocked\" as u\n  state coin_fork1 &lt;&lt;fork&gt;&gt;\n  state coin_fork2 &lt;&lt;fork&gt;&gt;\n  state pop1 &lt;&lt;fork&gt;&gt;\n  state pop2 &lt;&lt;fork&gt;&gt;\n  state choice_state &lt;&lt;choice&gt;&gt;\n  state \"Stack\" as s\n  \n  [*] --&gt; l\n  l --&gt; l : push\n  l --&gt; coin_fork1: coin\n  coin_fork1 --&gt; u\n  coin_fork1 --&gt; s: +1\n  u --&gt; coin_fork2: coin\n  coin_fork2 --&gt; u\n  coin_fork2 --&gt; s: +1\n  \n  u --&gt; choice_state: push\n  choice_state --&gt; pop1: if Stack &gt; 1\n  pop1 --&gt; s: -1\n  pop1 --&gt; u\n  \n  choice_state --&gt; pop2: if Stack == 1\n  pop2 --&gt; s: -1\n  pop2 --&gt; l\n\n\n\n\n\n\n\n\nUsing the Pushdown Turnstile\nWith Robin’s new Pushdown Turnstile installed at metro stations everywhere, they bring their two friends back to the scene of the crime, and retry their three-coins, three-people strategy. Here’s what happens.\n\n\n\n\n\n\n\n\nInput\nNew State\nCoin Stack\n\n\n\n\n\nLocked\n0\n\n\n\nUnlocked\n1\n\n\n\nUnlocked\n2\n\n\n\nUnlocked\n3\n\n\n\nUnlocked\n2\n\n\n\nUnlocked\n1\n\n\n\nLocked\n0\n\n\n\n\n\nGeneralizing the pattern\nThe way this turnstile works, generally, is that if you put in \\(n\\) coins, \\(n\\) people will be able to push through. Another way of notating that sequence of events is \\(^n\\)\\(^n\\). In the more formal-language-theory world, these kinds of patterns are usually labeled \\(a^nb^n\\).\nAnother way to think about these \\(a^nb^n\\) systems is in terms of bracket matching. If we replace each  symbol with [ and each  symbol with ], then we get a pattern that looks like this:\n[\n[\n[\n]\n]\n]\nThe requirement for the language is that every opening bracket [ needs to get matched with a closing bracket ].",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Pushdown Automata and Context Free Languages"
    ]
  },
  {
    "objectID": "notes/meetings/02_pda.html#context-free-grammars",
    "href": "notes/meetings/02_pda.html#context-free-grammars",
    "title": "Pushdown Automata and Context Free Languages",
    "section": "Context Free Grammars",
    "text": "Context Free Grammars\nWe get nested, bracket matching patterns in natural language all the time. For example the person-number agreement in this sentence.\n\nThe person who I, the guy you are looking at, am talking to is not listening.\n\nIf the form of be in this sentence were generated by a Regular Grammar, to be parsed with a Finite State Automaton, once the “you” subject appears in the sentence, every following form of be would have to be “are” the rest of the way.\n\n*The person who I, the guy you are looking at, are talking to are not listening.\n\nSince the first sentence is how English and other languages work, we’d conclude that natural language is, at least, Context Free.\n\nContext Free Rules\n\n\nRegular rules can look like this:\n\\[\nA \\rightarrow aA\n\\]\n\nContext free rules can look like this: \\[\nA \\rightarrow aAb\n\\]\n\n\nReturning to this html snippit:\n&lt;p&gt;\n  This is a paragraph with \n  &lt;strong&gt;\n    bold text.\n  &lt;/strong&gt;\n&lt;/p&gt;\nRules of a context free grammar that could give rise to this well-formed html are:\n\\[\nD \\rightarrow &lt;p&gt;C&lt;/p&gt;\n\\]\n\\[\nC \\rightarrow words\n\\]\n\\[\nC \\rightarrow words S\n\\]\n\\[\nS \\rightarrow &lt;strong&gt;C&lt;/strong&gt;\n\\]\n\n\n\n\n\nflowchart TD\n\n  D --&gt; p1[\"p\"]\n  D --&gt; C1[\"C\"]\n  D --&gt; p2[\"/p\"]\n\n  C1 --&gt; w1[\"This is a paragraph with\"]\n  C1 --&gt; S\n\n  S --&gt; s1[\"strong\"]\n  S --&gt; C2[\"C\"]\n  S --&gt; s2[\"/strong\"]\n\n  C2 --&gt; w2[\"bold text.\"]\n\n\n\n\n\n\n\n\n\nA PDA for this grammar\nHere’s a way we’d describe a Pushdown Automaton that decides whether or not a document is generated by this grammar:\n\nEach time it encounters an opening &lt;tag&gt;, it adds it to the stack, and when it encounters a closing &lt;/tag&gt;, it pops it from the stack.\n\n\n\nWhen it encounters a closing &lt;/tag&gt;, it has to match the opening &lt;tag&gt; that’s at the top of the stack.\nWhen it gets to the end of the document, the stack needs to be empty.\n\nHere’s a table showing how that’d play out\n\n\n\n\n\n\n\n\nInput\nevent\nStack\n\n\n\n\n&lt;p&gt;\npush &lt;p&gt;\n&lt;p&gt;\n\n\nThis, is, a, paragraph, with\n\n&lt;p&gt;\n\n\n&lt;strong&gt;\npush &lt;strong&gt;\n&lt;strong&gt;, &lt;p&gt;\n\n\nbold, text.\n\n&lt;strong&gt;, &lt;p&gt;\n\n\n&lt;/strong&gt;\npop &lt;strong&gt;\n&lt;p&gt;\n\n\n&lt;/p&gt;\npop &lt;p&gt;\n\n\n\n\nOne consequence of the rule that tags need to match when you pop them is that the following is not valid html.\n&lt;p&gt;\n  This is &lt;strong&gt;invalid!\n&lt;/p&gt;\n&lt;/strong&gt;\nIf you were to reason through the state of the stack after the opening &lt;strong&gt; tag, it would look like\n\n&lt;strong&gt;, &lt;p&gt;\n\nThen, when you feed it &lt;/p&gt;, it doen’t match the tag at the top of the stack, so we’d get an error of some sort.",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Pushdown Automata and Context Free Languages"
    ]
  },
  {
    "objectID": "notes/meetings/02_pda.html#limits-of-pushdown-automata",
    "href": "notes/meetings/02_pda.html#limits-of-pushdown-automata",
    "title": "Pushdown Automata and Context Free Languages",
    "section": "Limits of Pushdown Automata",
    "text": "Limits of Pushdown Automata\nAn office building has installed a version of Robin’s turnstile. Each person who enters the building has to insert their id card, and the machine scans it and spits it out the other side when a person pushes through.\nRobin approaches the turnstile with their friends Skylar and Alex. Both Skylar and Alex have their hands full carrying packages into the building, so Robin tries to be helpful and insert all of their id cards first, so they can then pass through. They’re walking through in the order\n\nRobin\nSkylar\nAlex\n\nSo Robin puts their ID cards into the turnstile in that order. Here’s how it works out\n\n\n\n\n\n\n\n\nInput\nAction\nStack\n\n\n\n\n\npush \n\n\n\n\npush \n, \n\n\n\npush \n, , \n\n\n\npop \n, \n\n\n\n🚨\n\n\n\n\nOh no! The turnstile has handed Robin Alex’s id card! What a mess!\n\nBeyond Context Free\nRobin was expecting a sequence like this\n\n, , , , , \n\nThis involves so-called “crossing dependencies”, which can’t be recognized by a Pushdown Automaton, which means they involve a more complex grammar than context free rules.\nThere are some examples of crossing dependencies in human language as well, like this example in Swiss German from Shieber (1985) (cited in Jäger and Rogers (2012))\n\n“that we let the children help Hans paint the house”\n\n\n\n\n\n\n\n\n\n\n\n\ndass\nmer\nd’ chind\nem Hans\nes Huus\nlönd\nhälfe\naanstriiche\n\n\nthat\nwe\nthe children-ACC\nHans-DAT\nthe house-ACC\nlet\nhelp\npaint",
    "crumbs": [
      "Notes",
      "Class Meetings",
      "Pushdown Automata and Context Free Languages"
    ]
  }
]