{
  "hash": "ae410b5da13eb5a457e7a64ef43a6151",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ngram \\\"smoothing\\\"\"\ndate: 2024-02-27\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\n    warning: false\n    dev: ragg_png\nfilters: \n  - codeblocklabel\ncategories:\n  - compling\n---\n\n\n## Recap\n\nIn the notes on information theory, we talked about how we can evaluate texts with respect to language models, and vice versa, with metrics like perplexity.\n\n*But* we run into trouble with [really novel sentences](07_information-theory.qmd#really-novel-sentences), when the probability is estimated to be 0. This can happen for one of two reasons:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Where 0 probabilities come from\n\n### \"Out of Vocabulary Items\"\n\n\"Out of Vocabulary Items\" or OOVs are tokens that were not present in the training data. This won't be so common for closed-vocabulary tokenizers, like [Byte Pair Encoding](03_tokenization.qmd#closed-vocabulary-tokenization-byte-pair-encoding), but for tokenization like we've used on *Moby Dick*, this is guaranteed to happen.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\"ogre\" in moby_dick_tokens\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n:::\n\n\nOOVs aren't just the domain of ngram models, but also come up in, say, looking up word pronunciations in pronunciation dictionaries.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n\n[nltk_data] Downloading package cmudict to\n[nltk_data]     /Users/joseffruehwald/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.corpus import cmudict\ncmu_dictionary = cmudict.dict()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncmu_dictionary[\"philadelphia\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[['F', 'IH2', 'L', 'AH0', 'D', 'EH1', 'L', 'F', 'IY0', 'AH0']]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncmu_dictionary[\"passyunk\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyError: 'passyunk'\n```\n\n\n:::\n:::\n\n\n### Missing Sequences\n\nEven if every token is represented in the training data, specific sequences aren't.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\"screamed\" in moby_dick_tokens\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nscream_idx = moby_dick_tokens.index(\"screamed\")\nmoby_dick_tokens[\n  (scream_idx-3):(scream_idx+3)\n]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['Wa-hee', '!', '‚Äù', 'screamed', 'the', 'Gay-Header']\n```\n\n\n:::\n:::\n\n\n## A *predictable* problem\n\nAn interesting thing is that there is a pretty good way to estimate whether the next token you see in a corpus will be new to you. It's approximately equal to the probability that you'll see a token that's only appeared once so far.\n\n$$\nP(\\text{new}) \\approx P(\\text{once)}\n$$\n\nIt takes a lot of processing to actually calculate, but here's how the Good-Turing estimate works out for the book Frankenstein.\n\n![Good Turing Smoothing](assets/good_turing.png){fig-align=\"center\" width=\"90%\"}\n\nSo, for *Moby Dick*, we can calculate the probability that a new token will be completely new.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncounter_dict = Counter(moby_dick_words)\nones = 0\nfor k in counter_dict:\n  if counter_dict[k] == 1:\n    ones+=1\n    \nones / counter_dict.total()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.04445651239656506\n```\n\n\n:::\n:::\n\n\nMaybe that doesn't seem so bad, but remember, the problem only gets worse the larger our ngrams get.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The proportion of all tokens which are hapax legomena ](08_ngram_smoothing_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Dealing with it\n\n### `Unk`ifying\n\nOne thing you can do to deal with the OOV issue is convert all tokens below a certain frequency to the label `<UNK>` for \"unknown.\"\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmoby_count = Counter(moby_dick_tokens)\nmoby_unk_tokens = moby_dick_tokens\nfor idx, tok in enumerate(moby_unk_tokens):\n  if moby_count[tok] < 2:\n    moby_unk_tokens[idx] = \"<UNK>\"\n\nmoby_unk_count = Counter(moby_unk_tokens)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ncheck_word  = \"ogre\"\nif check_word in moby_unk_count:\n  print(moby_unk_count[check_word])\nelse:\n  print(moby_unk_count[\"<UNK>\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n11379\n```\n\n\n:::\n:::\n\n\nNLTK conveniently. builds this into is vocabulary module.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.lm import Vocabulary\nmoby_vocab = Vocabulary(moby_dick_tokens, unk_cutoff = 2)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmoby_vocab.lookup(\"ogre\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'<UNK>'\n```\n\n\n:::\n:::\n\n\n### Smoothing\n\nEven after unkifying, there are still going to be some ngrams where the individual *tokens* occur in the training data, but the specific sequence doesn't.\n\nSo, we estimated the probability that the next token we see is going to be new token:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nones / counter_dict.total()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.04445651239656506\n```\n\n\n:::\n:::\n\n\nThe problem is, in order to give *unseen* token a slice of the pie, we need to take *away* some space from the seen tokens. Where does that probability fit into this layer cake, where all the space is already accounted for?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](08_ngram_smoothing_files/figure-html/unnamed-chunk-20-1.png){width=768}\n:::\n:::\n",
    "supporting": [
      "08_ngram_smoothing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}