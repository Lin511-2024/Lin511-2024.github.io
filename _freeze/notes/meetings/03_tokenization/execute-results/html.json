{
  "hash": "1c10fbffd8868605d63a4c1fd2be8b54",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tokenization\ndate: 2024-02-06\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\n    warning: false\nfilters: \n  - codeblocklabel\ncategories:\n  - compling\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## A nice token printer\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef wrap_print(tokens, width = 5):\n  for idx, token in enumerate(tokens):\n    print(f\"'{token}'\", end = \"\\t\")\n    if (idx+1) % width == 0:\n      print(\"\")\n```\n:::\n\n:::\n\n## Lesson 1: Data is Messy\n\n### Headers, etc.\n\nIf you go take a look at the plain-text version of [Moby Dick on Project Gutenberg](https://www.gutenberg.org/cache/epub/2701/pg2701.txt), you'll see that it starts out with the following block of text:\n\n::: codebox\n``` {.txt style=\"font-size: 0.6em;\"}\nThe Project Gutenberg eBook of Moby Dick; Or, The Whale\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n\nTitle: Moby Dick; Or, The Whale\n\n\nAuthor: Herman Melville\n\nRelease date: July 1, 2001 [eBook #2701]\n                Most recently updated: August 18, 2021\n\nLanguage: English\n\n\n\n*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n```\n:::\n\nAnd if you scroll to the very bottom of the file, you'll also find a *very* long block of text describing the Project Gutenberg License. If we wanted to analyze the text of *Moby Dick*, we'd have to make sure to remove these headers and footers appropriately.\n\n::: {.callout-note collapse=\"true\"}\n## Header removal\n\nI've been using `gutenbergpy` to get text from project Gutenberg, which has a special function `gutenbergpy.textget.strip_headers()` to handily remove these headers.\n:::\n\nSimilarly, if you wanted to analyze the text of the course notes, and you downloaded the html page, on each page you'd be faced with a *long* header looking like this:\n\n::: codebox\n``` {.html style=\"font-size: 0.3em\"}\n!DOCTYPE html>\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"><head>\n\n<meta charset=\"utf-8\">\n<meta name=\"generator\" content=\"quarto-1.4.549\">\n\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n\n<meta name=\"author\" content=\"Josef Fruehwald\">\n<meta name=\"dcterms.date\" content=\"2024-01-16\">\n\n<title>Lin511-2024 - Regular Languages and Finite State Machines</title>\n<style>\ncode{white-space: pre-wrap;}\nspan.smallcaps{font-variant: small-caps;}\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\ndiv.column{flex: auto; overflow-x: auto;}\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\nul.task-list{list-style: none;}\nul.task-list li input[type=\"checkbox\"] {\n  width: 0.8em;\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \n  vertical-align: middle;\n}\n/* CSS for syntax highlighting */\npre > code.sourceCode { white-space: pre; position: relative; }\npre > code.sourceCode > span { line-height: 1.25; }\npre > code.sourceCode > span:empty { height: 1.2em; }\n.sourceCode { overflow: visible; }\ncode.sourceCode > span { color: inherit; text-decoration: inherit; }\ndiv.sourceCode { margin: 1em 0; }\npre.sourceCode { margin: 0; }\n@media screen {\ndiv.sourceCode { overflow: auto; }\n}\n@media print {\npre > code.sourceCode { white-space: pre-wrap; }\npre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }\n}\npre.numberSource code\n  { counter-reset: source-line 0; }\npre.numberSource code > span\n  { position: relative; left: -4em; counter-increment: source-line; }\npre.numberSource code > span > a:first-child::before\n  { content: counter(source-line);\n    position: relative; left: -1em; text-align: right; vertical-align: baseline;\n    border: none; display: inline-block;\n    -webkit-touch-callout: none; -webkit-user-select: none;\n    -khtml-user-select: none; -moz-user-select: none;\n    -ms-user-select: none; user-select: none;\n    padding: 0 4px; width: 4em;\n  }\npre.numberSource { margin-left: 3em;  padding-left: 4px; }\ndiv.sourceCode\n  {   }\n@media screen {\npre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }\n}\n/* CSS for citations */\ndiv.csl-bib-body { }\ndiv.csl-entry {\n  clear: both;\n  margin-bottom: 0em;\n}\n.hanging-indent div.csl-entry {\n  margin-left:2em;\n  text-indent:-2em;\n}\ndiv.csl-left-margin {\n  min-width:2em;\n  float:left;\n}\ndiv.csl-right-inline {\n  margin-left:2em;\n  padding-left:1em;\n}\ndiv.csl-indent {\n  margin-left: 2em;\n}</style>\n...\n```\n:::\n\nThat's not even the end of it.\n\n### Markup\n\nOnce we get to the *content* of the text, there's still \"markup\" to deal with. Here's a sentence from *Moby Dick*.\n\n::: codebox\n``` txt\nBut _being paid_,—what will compare with it?\n```\n:::\n\nThe underscores `_` are there to indicate italics in the original text. Here's how it looks when rendered:\n\n![Rendered text](assets/being_paid.png){fig-align=\"center\" width=\"100%\"}\n\nIf we just split this text up into words based on spaces, those underscores (and other punctiuation) are going to stuck around.\n\n::: codebox\n\n::: {.cell}\n\n```{.python .cell-code}\nsentence = \"But _being paid_,—what will compare with it?\"\nwrap_print(sentence.split(\" \"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'But'\t'_being'\t'paid_,—what'\t'will'\t'compare'\t\n'with'\t'it?'\t\n```\n\n\n:::\n:::\n\n:::\n\nI don't think \"paid\\_,-what\" is a word.\n\nThe same issue goes for trying to analyze text from the course notes. Here a paragraph from the [finite state automata notes](01_fsm.qmd).\n\n::: codebox\n``` html\n<p>\n  But since this is <em>Computational</em> Linguistics, \n  we should probably learn about what is \n  “regular” about “regular” expressions, \n  because it’s related to formal language \n  theory!\n</p>\n```\n:::\n\nAgain, if we want to analyze the text, we'd need to extract it from this markup.\n\n## Lesson 2: Language is complex.\n\nWe already started touching on how we may need to \"case-fold\" text before we analyze it.\n\n::: codebox\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\nfrom collections import Counter\n\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_count = Counter(words)\n\nfor key in cat_count:\n  if re.match(\"[Cc]at\", key):\n    print(f\"{key}\\t{cat_count[key]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCats\t2\ncats.\t1\ncat\t1\n```\n\n\n:::\n:::\n\n:::\n\nConverting the whole phrase to lowercase will help, but there's still the issue of punctuation.\n\n**Tokenization** is the non-trivial task of splitting text up into meaningful units.\n\n### Tokenization\n\nSetting aside *semantic* issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and **tokenizing** it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\n#### Places to leave in punctuation\n\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\n-   You don't want to eliminate punctuation from inside `Ph.D`, or `m.p.h.`. You also don't want to eliminate it from some proper names, like ampersands in `Procter & Gamble`, `Texas A&M`, `A&W`, `m&m's`.\n\n-   You'll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n    -   `1,000.55`\n\n    -   `1.000,55`\n\n    -   `1 000,55`\n\n-   Currency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n    -   `$0.99`\n\n    -   `99¢`\n\n    -   `€0,99`\n\n-   Dates: There are so many different permutations on how dates can be formatted that I shouldn't list them all here, but here are some.[^1]\n\n    -   4 digit year, 2 digit month, 2 digit day\n\n        -   `2022-09-12`\n        -   `2022/09/12`\n\n    -   4 digit year, 1 or 2 digit month, 2 digit day\n\n        -   `2022-9-12`\n        -   `2022/9/12`\n\n    -   2 digit day, 2 digit month, 4 digit year\n\n        -   `12-09-2022`\n        -   `12/09/2022`\n\n    -   2 digit day, 1 or 2 digit month, 4 digit year\n\n        -   `12-9-2022`\n        -   `12/9/2022`\n\n    -   2 digit day, 2 digit month, 2 digit year\n\n        -   `12-09-22`\n        -   `12/09/22`\n\n    -   2 digit month, 2 digit day, 4 digit year\n\n        -   `09-12-2022`\n        -   `09/12/2022`\n\n    -   1 digit month, 2 digit day, 2 digit year\n\n        -   `9-12-22`\n        -   `9/12/22`\n\n-   Emoticons,[^2] where the token is entirely punctuation `:)`, `>.<`.\n\n[^1]: I'm being tedious here on purpose, because you have to keep in mind that if you wrote a function to handle just one of these possible date formats, it would not immediately translate over to the others! There are also *entire libraries* in multiple programming languages for parsing and reformatting date times. Python: [datetime](https://docs.python.org/3/library/datetime.html), R: [lubridate](https://lubridate.tidyverse.org/).\n\n[^2]: This example *isn't* from Jurafsky & Martin.\n\n#### Places to split up words\n\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like `hard-won`.\n\n-   `hard-won` ➔ `hard`, `won` or `hard`, `-`, `won`.\n\nAnother example involves clitics, like `n't` or `'re` in English.\n\n-   `isn't` ➔ `is`, `n't`\n\n-   `can't` ➔ `ca`, `n't`\n\n-   `what're` ➔ `what`, `'re`\n\n#### Places to glue words together\n\nYou might want to also glue together tokens from whitespace tokenization.\n\n-   `New`, `York`, `City` ➔ `New York City`\n\n-   `Super`, `Smash`, `Brothers` ➔ `Super Smash Brothers`\n\n#### Challenges with speech and text\n\n-   {{< fa keyboard >}}: \\$1,500\n    -   {{< fa bullhorn >}}: \"one thousand five hundred dollars\"\n\n    -   {{< fa bullhorn >}}: \"fifteen hundred dollars\"\n\n    -   {{< fa bullhorn >}}: \"one and a half thousand dollars\"\n\n    -   {{< fa bullhorn >}}: \"one point five thousand dollars\"\n\n## Tokenizers -part 1-\n\nThe Natural Language Toolkit library [@bird2009] has a few tokenizers available.\n\n::: codebox\n\n::: {.cell}\n\n```{.python .cell-code}\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n## The first time you run it, \n## you need to download some data\nnltk.download('punkt')\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntokens_01 = word_tokenize(sentence)\nwrap_print(tokens_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'But'\t'_being'\t'paid_'\t','\t'—what'\t\n'will'\t'compare'\t'with'\t'it'\t'?'\t\n```\n\n\n:::\n:::\n\n:::\n\nThe spacy package also has tokenizers available,\n\n::: codebox\n``` bash\npip install spacy\npip install $(spacy info en_core_web_sm --url)\n```\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(sentence)\nwrap_print(doc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'But'\t'_'\t'being'\t'paid_,—what'\t'will'\t\n'compare'\t'with'\t'it'\t'?'\t\n```\n\n\n:::\n:::\n\n:::\n\n## Data Sparsity\n\nWith *any* tokenization of text, you're going to wind up with a lot of tokens that appear just once or twice.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nHere's a plot of the top 10 most frequent tokens in *Moby Dick*.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Top 10 tokens in Moby Dick](03_tokenization_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThe trend of tokens getting less and less frequent continues\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![All tokens in Moby Dick](03_tokenization_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThere is also a general phenomenon that the larger your corpus of tokens gets, the larger the vocabulary will get.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![As the number of tokens increases, the size of the vocabulary increases](03_tokenization_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThis means that if you have an \"open vocabulary\" tokenization method, where you can always create a new token based on some rules, you'll *never* be able to analyze enough text such that you'll never encounter a new token you've never seen before.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The probability of a new type as token size increases](03_tokenization_files/figure-html/unnamed-chunk-13-1.png){width=90%}\n:::\n:::\n\n\n## Closed Vocabulary Tokenization (Byte Pair Encoding)\n\nA different approach to tokenization is to have a pre-specified closed vocabulary that you use to pull tokens out of text.\n\nLet's start out with a fake training of a byte pair encoder with the simple vocabulary \"cats can't canter\". We kick things off treating every character as a token, plus a specialized start-of-word symbol, which I'm representing with `_`.\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\nTokens\n\n```         \n_ c a t s\n_ c a n ' t\n_ c a n t e r\n```\n\nTypes\n\n```         \n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", '_'}\n```\n:::\n\nThis is, in principle, the smallest and *simplest* tokenization we could do for any input text. While the total number of *words* is infinite, the total number of characters or symbols we use to create those words is finite.\n\nThe next step is to count up all of the pairs (or bigrams) of tokens in the training data. In this case, both (`_`, `c`) and (`c`, `a`) appear equally commonly, so I make a decision and say (`_`, `c`) is the one we'll process first. We'll paste them together, call them a new type, and replace all (`_`, `c`) sequences with `_c`.\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\ntokens\n\n```         \n_c a t s\n_c a n ' t\n_c a n t e r\n```\n\ntypes\n\n```         \n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ',\n`_c`}\n```\n:::\n\nRepeating the process, the most frequently occurring bigram is now (`_c`, `a`), so we'll add `_ca` as a new type, and replace all (`_c`, `a`) sequences with `_ca`.\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\ntokens\n\n```         \n_ca t s \n_ca n ' t _\n_ca n t e r _\n```\n\ntypes\n\n```         \n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca'}\n```\n:::\n\nFinally, the last most frequent sequence is (`_ca`, `n`), so we'll add `_can` to the vocabulary, and collapse (`_ca`, `n`) sequences.\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\ntokens\n\n```         \n_ca t s\n_can ' t\n_can t e r\n```\n\ntypes\n\n```         \n{'a', 'e', 't', 'r', \n's', 'n', 'c', \"'\", ' ', \n'_c', '_ca', '_can'}\n```\n:::\n\nWe'll stop at that point, but we could either continue for a fixed number of iterations, or until our type, or vocabulary size reaches a fixed number.\n\n### The use of Byte Pair Encoding\n\nThis kind of tokenization approach is necessary when you want to be able to tokenizer *anything*, and also have a pre-specified vocabulary size. We can see how OpenAI's byte pair encoder handles the first few sentences of *Moby Dick*\n\n::: codebox\n\n::: {.cell}\n\n```{.python .cell-code}\nimport tiktoken\nenc = tiktoken.encoding_for_model(\"gpt-4\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmb_string = \"\"\"\nCall me Ishmael. Some years ago—never mind how long precisely—having\nlittle or no money in my purse, and nothing particular to interest me\non shore, I thought I would sail about a little and see the watery part\nof the world. It is a way I have of driving off the spleen and\nregulating the circulation.\n\"\"\"\n\ntokens = [\n  enc.decode([token]) \n    for token in enc.encode(mb_string.replace(\"\\n\", \" \"))\n]\n\nwrap_print(tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n' Call'\t' me'\t' Ish'\t'ma'\t'el'\t\n'.'\t' Some'\t' years'\t' ago'\t'—'\t\n'never'\t' mind'\t' how'\t' long'\t' precisely'\t\n'—'\t'having'\t' little'\t' or'\t' no'\t\n' money'\t' in'\t' my'\t' purse'\t','\t\n' and'\t' nothing'\t' particular'\t' to'\t' interest'\t\n' me'\t' on'\t' shore'\t','\t' I'\t\n' thought'\t' I'\t' would'\t' sail'\t' about'\t\n' a'\t' little'\t' and'\t' see'\t' the'\t\n' wat'\t'ery'\t' part'\t' of'\t' the'\t\n' world'\t'.'\t' It'\t' is'\t' a'\t\n' way'\t' I'\t' have'\t' of'\t' driving'\t\n' off'\t' the'\t' sple'\t'en'\t' and'\t\n' regulating'\t' the'\t' circulation'\t'.'\t' '\t\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Rank vs Frequency per tokenizer](03_tokenization_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=576}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Corpus size vs Vocabulary size per tokenizer](03_tokenization_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "03_tokenization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}