{
  "hash": "22df898bfdeaa86ab9a039704e9d5bf9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ngrams\"\nsubtitle: \"-or- What if we *could* parse natural language with a finite state automaton?\"\ndate: 2024-02-05\nfilters: \n  - codeblocklabel\nknitr:\n  opts_chunk: \n    warning: false\n    message: false\n    echo: false\nbibliography: references.bib\ncategories:\n  - compling\n---\n\n\nSo, in our notes on [finite state automata](01_fsm.qmd) and [push-down automata](02_pda.qmd) we concluded that since natural language has bracket matching patterns, and maybe even crossing dependencies, that it's more complex than a \"regular\" language, and can't really be parsed with a finite state automaton.\n\nngram language modelling asks the question: But what if we tried really hard?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## States and Words\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nThe first sentence of Moby Dick is, famously,\n\n> Call me Ishmael.\n\nWe could try representing this as a finite state automaton like so:\n\n\n```{mermaid}\nstateDiagram\n\tdirection LR\n\n\t[*] --> call: call\n\tcall --> me: me\n\tme --> ishmael: ishmael\n\tishmael --> [*]\n```\n\n\nBut, this is *far* from a complete model of the whole book *Moby Dick*. It would only work if the entire book was just \"Call me Ishmael\" over and over again.\n\nTo enrich the FSA, we could add all of the other words that could follow *\"call\".*\n\n\n```{mermaid}\nstateDiagram\n\tdirection LR\n\n\t[*] --> call: call\n\tcall --> call: call\n\tcall --> it: it\n\tcall --> a: a\n\tcall --> me: me\n\tcall --> him: him\n\tcall --> ...\n\tcall --> [*]\n\n\tme --> ishmael: ishmael\n\tme --> call: call\n\n\tishmael --> [*]\n```\n\n\nBut, lots of other words could *also* follow *\"me\"*. And more words could also follow *\"Ishmael\"*\n\n\n```{mermaid}\nstateDiagram\n\tdirection LR\n\n\t[*] --> call: call\n\tcall --> call: call\n\tcall --> it: it\n\tcall --> a: a\n\tcall --> me: me\n\tcall --> him: him\n\tcall --> ...\n\tcall --> [*]\n\n\tme --> me: me\n\tme --> call: call\n\tme --> that: that\n\tme --> and: and\n\tme --> ishmael: ishmael\n\tme --> to: to\n\tme --> ...\n\tme --> [*]\n\n\tishmael --> ishmael: ishmael\n\tishmael --> me: me\n\tishmael --> can: can\n\tishmael --> said: said\n\tishmael --> ...\n\tishmael --> [*]\n```\n\n\nIf we fully fleshed out this diagram with *all*[^1] of words in *Moby Dick*, it would look like this\n\n[^1]: well, *almost* all.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Bigram network for Moby Dick, where $C(w_1, w_2)\\ge 5$](04_ngrams_files/figure-html/fig-moby_dick_network-1.png){#fig-moby_dick_network fig-align='center' width=80%}\n:::\n:::\n\n\n### \\*grams\n\nThe \"model\" of word sequences is called an \"ngram\" model or more specifically a \"bigram\" model.\n\n| Words in the current state | Words in input | Total Words | Name    |\n|----------------------------|----------------|-------------|---------|\n| 1                          | 1              | 2           | bigram  |\n| 2                          | 1              | 3           | trigram |\n| 3                          | 1              | 4           | 4-gram  |\n| 4                          | 1              | 5           | 5-gram  |\n|                            |                |             | etc     |\n\n: How we name \\*gram models\n\nWe can expand the context of the bigram model to a trigram model, which would look something like this\n\n\n```{mermaid}\nstateDiagram\n  direction LR\n  \n  state \"_ call\" as _c\n  state \"call me\" as cm\n  state \"me Ishmael\" as mi\n  \n  [*] --> _\n  _ --> _c: call\n  _c --> cm: me\n  cm --> mi: Ishmael\n  mi --> [*]\n  \n```\n\n\nBut, again, even for this small vocabulary, this *total* number of states is incomplete. If we wired up *all* of the logical transitions they'd look like this [ilithid](https://en.wikipedia.org/wiki/Illithid) monstrosity.\n\n\n```{mermaid}\nstateDiagram\n\tdirection LR\n\n\tstate \"_ call\" as _c\n\tstate \"_ me\" as _m\n\tstate \"_ Ishmael\" as _i\n\t\n\tstate \"call me\" as cm\n\tstate \"call call\" as cc\n\tstate \"call Ishmael\" as ci\n\n\tstate \"me Ishmael\" as mi\n\tstate \"me call\" as mc\n\tstate \"me me\" as mm\n\n\tstate \"Ishmael call\" as ic\n\tstate \"Ishmael Ishmael\" as ii\n\tstate \"Ishmael me\" as im\n\n\t[*] --> _: _\n\t_ --> _m: me\n\t_ --> _c: call\n\t_ --> _i: Ishmael\n\t\n\n\t_c --> cm: me\n\t_c --> cc: call\n\t_c --> ci: Ishmael\n\n\tcc --> cc: call\n\tcc --> ci: Ishmael\n\tcc --> cm: me\n\n\tci --> ii: Ishmael\n\tci --> im: me\n\tci --> ic: call\n\n\tcm --> mi: Ishmael\n\tcm --> mm: me\n\tcm --> mc: call\n\n\t_m --> mc: call\n\t_m --> mm: me\n\t_m --> mi: Ishmael\n\t\n\tmm --> mm: me\n\tmm --> mc: call\n\tmm --> mi: Ishmael\n\n\tmi --> ii: Ishmael\n\tmi --> ic: call\n\tmi --> im: me\n\n\tmc --> cm: me\n\tmc --> ci: Ishmael\n\tmc --> cc: call\n\n\n\n\t_i --> ic: call\n\t_i --> im: me\n\t_i --> ii: Ishmael\n\n\tic --> cc: call\n\tic --> cm: me\n\tic --> ci: Ishmael\n\n\tim --> mi: Ishmael\n\tim --> mc: call\n\tim --> mm: me\n\n\tii --> im: me\n\tii --> ic: call\n\tii --> ii: Ishmael\n\t\n```\n\n\n### Probabilistic ngrams\n\nBut, if we look at the actual entire book *Moby Dick,* not all of these connections are equally likely.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"owszwwhmyf\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#owszwwhmyf table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#owszwwhmyf thead, #owszwwhmyf tbody, #owszwwhmyf tfoot, #owszwwhmyf tr, #owszwwhmyf td, #owszwwhmyf th {\n  border-style: none;\n}\n\n#owszwwhmyf p {\n  margin: 0;\n  padding: 0;\n}\n\n#owszwwhmyf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#owszwwhmyf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#owszwwhmyf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#owszwwhmyf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#owszwwhmyf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#owszwwhmyf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#owszwwhmyf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#owszwwhmyf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#owszwwhmyf .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#owszwwhmyf .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#owszwwhmyf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#owszwwhmyf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#owszwwhmyf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#owszwwhmyf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#owszwwhmyf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#owszwwhmyf .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#owszwwhmyf .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#owszwwhmyf .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#owszwwhmyf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#owszwwhmyf .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#owszwwhmyf .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#owszwwhmyf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#owszwwhmyf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#owszwwhmyf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#owszwwhmyf .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#owszwwhmyf .gt_left {\n  text-align: left;\n}\n\n#owszwwhmyf .gt_center {\n  text-align: center;\n}\n\n#owszwwhmyf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#owszwwhmyf .gt_font_normal {\n  font-weight: normal;\n}\n\n#owszwwhmyf .gt_font_bold {\n  font-weight: bold;\n}\n\n#owszwwhmyf .gt_font_italic {\n  font-style: italic;\n}\n\n#owszwwhmyf .gt_super {\n  font-size: 65%;\n}\n\n#owszwwhmyf .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#owszwwhmyf .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#owszwwhmyf .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#owszwwhmyf .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#owszwwhmyf .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#owszwwhmyf .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#owszwwhmyf .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    \n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"next word\">next word</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"3\" scope=\"colgroup\" id=\"word\">\n        <span class=\"gt_column_spanner\">word</span>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"call\">call</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"me\">me</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"ishmael\">ishmael</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><th id=\"stub_1_1\" scope=\"row\" class=\"gt_row gt_left gt_stub\">call</th>\n<td headers=\"stub_1_1 call\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td>\n<td headers=\"stub_1_1 me\" class=\"gt_row gt_right\" style=\"background-color: #FFEBEE; color: #000000;\">1</td>\n<td headers=\"stub_1_1 ishmael\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td></tr>\n    <tr><th id=\"stub_1_2\" scope=\"row\" class=\"gt_row gt_left gt_stub\">ishmael</th>\n<td headers=\"stub_1_2 call\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td>\n<td headers=\"stub_1_2 me\" class=\"gt_row gt_right\" style=\"background-color: #FFEBEE; color: #000000;\">1</td>\n<td headers=\"stub_1_2 ishmael\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td></tr>\n    <tr><th id=\"stub_1_3\" scope=\"row\" class=\"gt_row gt_left gt_stub\">me</th>\n<td headers=\"stub_1_3 call\" class=\"gt_row gt_right\" style=\"background-color: #B71C1C; color: #FFFFFF;\">3</td>\n<td headers=\"stub_1_3 me\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td>\n<td headers=\"stub_1_3 ishmael\" class=\"gt_row gt_right\" style=\"background-color: #808080; color: #FFFFFF;\">0</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n```{mermaid}\n%% fig-align: center\nflowchart LR\n\tc[\"call\"] ==> m[\"me\"]\n\tm --> c\n\tm --> i[\"Ishmael\"]\n```\n\n\n## Terminology and Notation Moment\n\n### Types vs Tokens\n\n<details>\n\n<summary>A function to get all of the words in Moby Dick</summary>\n\n\n::: {.cell}\n\n```{.python .cell-code  code-summary=\"A function to get all Moby Dick words\"}\nimport re\nimport gutenbergpy.textget\nfrom nltk.tokenize import word_tokenize\n\ndef get_moby_dick_words():\n  raw_book = gutenbergpy.textget.get_text_by_id(2701) # with headers\n  moby_dick_byte = gutenbergpy.textget.strip_headers(raw_book) # without headers\n  moby_dick = moby_dick_byte.decode(\"utf-8\").lower() \n  moby_dick_tokens = word_tokenize(moby_dick)\n  moby_dick_words = [tok for tok in moby_dick_tokens if re.match(r\"\\w\", tok)]\n  \n  return moby_dick_words\n```\n:::\n\n\n</details>\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmoby_dick_words = get_moby_dick_words()\nfor idx, word in enumerate(moby_dick_words[0:20]):\n  print(word, end = \", \")\n  if (idx+1) % 5 == 0:\n     print(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmoby-dick, or, the, whale, by, \nherman, melville, contents, etymology, extracts, \nsupplied, by, a, sub-sub-librarian, chapter, \n1., loomings, chapter, 2., the, \n```\n\n\n:::\n:::\n\n\nWe can get counts of how often each word appeared in the book with `collections.Counter()` .\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom collections import Counter\n\nword_count = Counter(moby_dick_words)\n```\n:::\n\n\nLet's compare the length of the full list of words to the length of the word count dictionary.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nThere are 215,300 total  words.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThere are  19,989 unique words.\n```\n\n\n:::\n:::\n\n\n::: callout-important\n## Terminology\n\nIn more common corpus/compling terminology, we would say\n\n-   There are 215,300 **tokens** in *Moby Dick*.\n\n-   There are 19,989 **types** in *Moby Dick*.\n:::\n\nWe can get the frequency of the words \"whale\" and \"ogre\" in *Moby Dick* like so:\n\n::: {.callout-note collapse=\"true\"}\n## Indexing with a string?\n\nWe can index `word_count` with the string `\"whale\"` because it is a [\"dictionary\"](../concepts/glossary.qmd#dictionary) We could create our own dictionary like this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfood_type = {\n  \"banana\": \"fruit\",\n  \"strawberry\": \"fruit\",\n  \"carrot\": \"vegetable\",\n  \"onion\": \"vegetable\"\n}\n\nfood_type[\"banana\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'fruit'\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(f\"The word 'whale' appeared {word_count['whale']:,} times.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe word 'whale' appeared 1,070 times.\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"The word  'ogre' appeared    {word_count['ogre']} times.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe word  'ogre' appeared    0 times.\n```\n\n\n:::\n:::\n\n\n::: callout-important\n## Terminology\n\nThe way we'd describe this in a more corpus/comp-ling way is\n\n> The word **type** \"whale\" appears in *Moby Dick*. There are 1,070 **tokens** of \"whale\" in the book.\n>\n> The word **type** \"ogre\" does not appear in *Moby Dick*.\n:::\n\n## Language Prediction\n\nWhen we are perceiving language, we are constantly and in real-time making predictions about what we are about to hear next. While we're going to be talking about this in terms of predicting the next word, It's been shown that we do this even partway through a word [@allopenna1998].\n\nSo, let's say I spoke this much of a sentence to you:\n\n> I could tell he was angry from the tone of his\\_\\_\\_\n\nAnd then a sudden noise obscured the final word, and you only caught part of it. Which of the following three words was I *probably* trying to say?\n\na.  boys\nb.  choice\nc.  voice\n\nYour ability to guess which word it was is based on your i) experience with English turns of phrase and ii) the information in the context.\n\nOne goal of Language Models is to assign probabilities across the vocabulary for what the next word will be, and hopefully assign higher probabilities to the \"correct\" answer than the \"incorrect\" answer. Applications for this kind of prediction range from speech-to-text (which could suffer from a very similar circumstance as the fictional one above) to autocomplete or spellcheck.\n\n## Using context (ngrams)\n\nIn the example sentence above, one way we could go about trying to predict which word is most likely is to count up how many times the phrase \"I could tell he was angry from the tone of his\\_\\_\\_\" is finished by the candidate words. Here's a table of google hits for the three possible phrases, as well as all hits for just the context phrase.\n\n|   \"I could tell he was angry from the tone of his\" | count |\n|---------------------------------------------------:|------:|\n|                                               boys |     0 |\n|                                             choice |     0 |\n|                                              voice |     3 |\n| *\"I could tell he was angry from the tone of his\"* |     3 |\n\nWe're going to start diving into mathematical formulas now (fortunately the numbers are easy right now).\n\nTo represent the count of a word or string of words in a corpus. We'll use $C(\\text{word})$. So given the table above we have\n\n$$\n\\displaylines{C(\\text{I could tell he was angry from the tone of his})=3\\\\\nC(\\text{I could tell he was angry from the tone of his boys})=0\\\\\nC(\\text{I could tell he was angry from the tone of his choice})=0\\\\\nC(\\text{I could tell he was angry from the tone of his voice})=3}\n$$\n\nTo describe the probability that the next word is \"choice\" given that we've already heard \"I could tell he was angry from the tone of his\", we'll use the notation $P(\\text{choice} | \\text{I could tell he was angry from the tone of his})$. To *calculate* that probability, we'll divide the total count of the whole phrase by the count of the preceding context.\n\n$$\nP(\\text{choice} | \\text{I could tell he was angry from the tone of his}) = \\frac{C(\\text{I could tell he was angry by the tone of his choice})}{C(\\text{I could tell he was angry by the tone of his})} = \\frac{0}{3} = 0\n$$\n\nIn fact, we can estimate the probability of an entire sentence with the *Probability Chain Rule*. The probability of a sequence of events like $P(X_1X_2X_3)$ can be estimated by multiplying out their conditional probabilities like so:\n\n$$\nP(X_1X_2X_3) = P(X_1)P(X_2|X_1)P(X_3|X_1X_2)\n$$\n\nOr, to use a phrase as an example:[^2]\n\n[^2]: Credit here to Kyle Gorman for introducing me to this example.\n\n$$\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n$$\n\n## Data Sparsity\n\nThe problem we face is that, even with the whole internet to search, very long phrases like *\"I could tell he was angry by the tone of his\"* are relatively rare!\n\nIf we look at *Moby Dick*, using a standard tokenizer (more on that later) we wind up with 215,300 words in total. But not every word is equally likely.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\" crop='true'}\n::: {.cell-output-display}\n![Rank and Frequency of single words in Moby Dick](04_ngrams_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\" crop='true'}\n::: {.cell-output-display}\n![Rank and Frequency of 2-grams through 5-grams in Moby Dick](04_ngrams_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=80%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The proportion of all tokens which are hapax legomena ](04_ngrams_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=80%}\n:::\n:::\n",
    "supporting": [
      "04_ngrams_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}