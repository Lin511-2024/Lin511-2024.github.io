{
  "hash": "88e1a5f0a062da851b5fd98e11556380",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Logistic Regression\ndate: 2024-03-25\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\n    warning: false\n    dev: ragg_png\nformat:\n  html:\n    mermaid:\n      theme: neutral\nfilters: \n  - codeblocklabel\ncategories:\n  - compling\n---\n\n::: {.cell}\n\n:::\n\n\n## Comparison\n\n### Naive Bayes\n\nA Naive Bayes classifier tries to build a model *for each class*. Recall Bayes' Theorem:\n\n$$\nP(\\text{data} | \\text{class)}P(\\text{class})\n$$\n\nThe term $P(\\text{data} | \\text{class})$ is a model of the kind of data that appears in each class. When trying to do a classification task, we get back how probable the data distribution for each class.\n\n### Logistic Regression\n\nLogistic Regression instead tries to directly estimate\n\n$$\nP(\\text{class} | \\text{data})\n$$\n\n## Some Terminology and Math\n\n### Probabilities\n\nProbabilities range from 0 (impossible) to 1 (guaranteed).\n\n$$\n0 \\le p \\le1\n$$\n\nIf something happens 2 out of every 3 times, we can calculate its probability with a fraction.\n\n$$\np = \\frac{2}{3} = 0.6\\bar{6}\n$$\n\n### Odds\n\nInstead of representing how often something happened out of the total possible number of times, we'd get it's **odds**.\n\nIf something happens 2 out of every 3 times, that means for every 2 times it happens, 1 time it doesn't.\n\n$$\no = 2:1 = \\frac{2}{1} = 2\n$$\n\nIf we expressed the odds of the opposite outcome, it would be\n\n$$\no = 1:2 = \\frac{1}{2} = 0.5\n$$\n\nThe smallest odds you can get are $0:1$ (something *never* happens). The largest odds can get are $1:0$ (something *always* happens). That means odds are bounded by 0 and infinity.\n\n$$\n0 \\le 0 \\le \\infty\n$$\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![The relationship between probabilities and odds](11_logistic_regression_files/figure-html/fig-prob-odds-1.png){#fig-prob-odds width=80%}\n:::\n:::\n\n\n### Log-Odds or Logits\n\nThe log of 0 is $-\\infty$, and the log of $\\infty$ is $\\infty$. So, if we take the log of the odds, we get a value that is symmetrically bounded.\n\n$$\n-\\infty \\le \\log{o} \\le \\infty\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The relationship between probabilities and logits](11_logistic_regression_files/figure-html/fig-prob-logit-1.png){#fig-prob-logit width=80%}\n:::\n:::\n\n\nWhat's also useful about this is that probabilities on opposite sides of 0.5 differ in logits only in their sign.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nfrom scipy.special import logit\n\nprint(logit(1/3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-0.6931471805599454\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(logit(2/3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6931471805599452\n```\n\n\n:::\n:::\n\n\n### Why use logits?\n\nBecause we can add and subtract an *arbitrary* number of values together (the outcome can be any negative or positive number) and then translate that back into a probability by reversing the function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.special import expit\nprint(expit(-2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.11920292202211755\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(expit(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.8807970779778823\n```\n\n\n:::\n:::\n\n\n#### The \"inverse logit\" function.\n\nThe \"inverse logit\" function, in NLP tasks, is usually called the \"sigmoid\" function, because it looks like an \"S\" and is represented with $\\sigma()$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The Ïƒ function](11_logistic_regression_files/figure-html/fig-sigma-1.png){#fig-sigma width=80%}\n:::\n:::\n\n\n## Training, or Fitting, a Logistic Regression\n\n### Step 1: Deciding on your binary outcome\n\ne.g: Positive (1) vs Negative (0) movie reviews.\n\n### Step 2: Feature engineering\n\nYou need to settle on features to encode into each training token. For example:\n\n-   How many words in the review.\n\n-   How many positive or negative words from specific sentiment lexicons.\n\n-   etc.\n\n### Step 3: \"Train\" the model\n\nThe model will return \"weights\" for each feature, and a \"bias\".\n\n-   As the absolute value of weights move away from 0, you can think of that feature as being more or less important.\n\n-   As the \"bias\" moves up and down from 0, you can think of the overall probability moving up and down.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The effect of different 'biases' on the same set of weights.](11_logistic_regression_files/figure-html/fig-biases-1.png){#fig-biases width=80%}\n:::\n:::\n\n\n#### The weights\n\nThe weights we get for each feature get multiplied by the feature value, and added together to get the logit value.\n\n$$\nz = 0.5~\\text{N positive words} + -0.2 ~\\text{N negative words} + 0.1~\\text{total length} + b\n$$\n\nWe could generalize the multiplication and addition:\n\n$$\nz = \\left(\\sum w_ix_i \\right) + b\n$$\n\nAnd, we could also even further simplify things with [matrix multiplication](09_markov_models.qmd#matrix-multiplication), using the \"dot product\".\n\n$$\nz = (w\\cdot x) + b\n$$\n\n### Step 4: Use the model to classify\n\nNow, with the features for a new set of data, multiply them by the weights, and add the bias. The categorization rule says we'll say it's 1 if $\\sigma(z)>0.5$, and otherwise.\n\n$$\nz' = w\\cdot x' +b\n$$\n\n$$\nc = \\left\\{\\begin{array}{l}1~\\text{if}~\\sigma(z')\\gt0.5\\\\0~\\text{otherwise} \\end{array} \\right\\}\n$$\n\n## Overfitting vs Underfitting\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily high temps for the Patterson Office Tower area.](11_logistic_regression_files/figure-html/fig-temp-1.png){#fig-temp width=768}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily high temps for the Patterson Office Tower area.](11_logistic_regression_files/figure-html/fig-temp-under-1.png){#fig-temp-under width=768}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Daily high temps for the Patterson Office Tower area.](11_logistic_regression_files/figure-html/fig-temp-over-1.png){#fig-temp-over width=768}\n:::\n:::\n\n\n### Overfitting with a classifier\n\nIf you fit a logistic regression with a *ton* of features, and each and every feature can get its own weight, you might \"overfit\" on the training data.\n\nOne way to deal with this is to try to \"regularize\" the estimation of weights, so that they get biased towards 0. The usual regularizing methods are (frustratingly) called \"L1\" and \"L2\" regularization.\n\nL1 Regularization\n\n:   Shrinks weights down towards 0, and may even 0 out some weights entirely.\n\nL2 Regularization\n\n:   Shrinks weights down towards 0, but probably won't set any to exactly 0.\n\nFor \\~reasons\\~, L2 regularization is mathematically and computationally easier to use.\n",
    "supporting": [
      "11_logistic_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}