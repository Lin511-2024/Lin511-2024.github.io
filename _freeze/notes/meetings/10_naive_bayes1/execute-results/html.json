{
  "hash": "85217c8a311e9cc73df35092b7598013",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bayes Theorem\ndate: 2024-03-19\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\n    warning: false\n    dev: ragg_png\nformat:\n  html:\n    mermaid:\n      theme: neutral\nfilters: \n  - codeblocklabel\ncategories:\n  - compling\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\nBefore we can get started with \"Naive Bayes Classification\", we should maybe begin with \"What is Bayes Theorem\".\n\n## Bayes Theorem\n\n![Thomas Bayes. [Public domain, via Wikimedia](https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif)](assets/Thomas_Bayes.gif){fig-align=\"center\"}\n\nBayes Theorem describes how we should use evidence or data as an indicator of some kind of outcome, or hypothesis. Specifically:\n\n-   We need to consider how likely the data would be given the outcome (a conditional probability, $P(d|h)$)\n\n-   The probability of the outcome, generally $P(h)$.\n\n-   The probability of the data, generally $P(d)$.\n\nTo use a linguistic example, let's say we were listening to a recording with a lot of static, and we couldn't make out even which language we were listening to. But then, we hear a very distinct \\[ð\\]. What is the probability that we're listening to English?\n\n-   The probability that we would hear \\[ð\\] if it *were* English is pretty high. It appears at the start of a lot of frequent function words.\n\n-   The probability we're listening to a recording of English, generally, depends on the context. Are we pulling random recordings from the internet? Are we listening to collection of specifically cross-linguistic data?\n\n-   The probability of hearing \\[ð\\], regardless of the language, would require some more data exploration. It's not that common a phoneme, cross-linguistically, but it is an allophone in many languages (e.g. Spanish).\n\nThe formula for calculating the probability that we're listening to English would be:\n\n$$\nP(\\text{English} | ð) = \\frac{P(ð|\\text{English}) P(\\text{English})}{P(ð)}\n$$\n\nThe really important thing to remember is that inverting a conditional probability is *hard*.\n\n### A Bayes Failure\n\n@wieling2016 found that many different languages were undergoing a change from preferring \"UH\" type filled pauses to \"UM\" type filled pauses. And, like many language changes where women lead the change:\n\n$$\nP(\\text{UM} | \\text{woman}) \\gt P(\\text{UM} | \\text{man})\n$$\n\nThe Daily Mail ran a headline that said something like \"Did you say 'Uh'? You probably support [UKIP](https://en.wikipedia.org/wiki/UK_Independence_Party).\"\n\nThis involved the inversion of a *few* different conditional probabilities.\n\n$$\nP(\\text{UKIP support} | \\text{man}) = \\frac{P{(\\text{man} | \\text{UKIP support})P(\\text{UKIP support)}}}{P(\\text{man})}\n$$\n\nand\n\n$$\nP(\\text{man}|\\text{Uh}) = \\frac{P(\\text{Uh}|\\text{man})P(\\text{man})}{P(\\text{UH})}\n$$\n\nThey really only had access to the two *conditional* probabilities on the right hand side of the equations! And, additionally, the $P(\\text{UKIP support})$ probability was pretty low!\n\n## An example for document classification\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport nltk\nimport numpy as np\nfrom collections import Counter\nnltk.download(\"movie_reviews\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.corpus import movie_reviews\n```\n:::\n\n\nThe `movie_reviews` object is kind of idiosyncractic. Here's how we access its contents.\n\n### Movie Reviews setup\n\n#### Getting all File IDs\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# To get all review ids:\nall_ids = list(\n  movie_reviews.fileids()\n  )\n\nall_ids[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n```\n\n\n:::\n:::\n\n\n#### Getting sentiments from file ids\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom collections import Counter\nCounter([x.split(\"/\")[0] for x in all_ids])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCounter({'neg': 1000, 'pos': 1000})\n```\n\n\n:::\n:::\n\n\nSo, we know $P(\\text{positive}) = P(\\text{negative}) = 0.5$.\n\n#### Getting the words from a single review\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmovie_reviews.words(\n  all_ids[1]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...]\n```\n\n\n:::\n:::\n\n\n#### Getting words from negative or positive reviews\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmovie_reviews.words(categories = [\"neg\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmovie_reviews.words(categories = [\"pos\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n```\n\n\n:::\n:::\n\n\n### Getting ready to classify\n\nLet's say we wanted to know whether or not a movie review was negative given the fact it has the word \"*barely* \" in it. That is, we want to know:\n\n$$\nP(\\text{negative}|barely) \\gt P(\\text{positive} | barely)\n$$\n\nor\n\n$$\nP(\\text{negative} | barely) \\lt P(\\text{positive}|barely)\n$$\n\nUsing Bayes theorem, that means we need to find out:\n\n$$\nP(\\text{sentiment}|barely) = \\frac{P(barely | \\text{sentiment})P(\\text{sentiment})}{P(barely)}\n$$\n\n#### Getting Counts\n\nOur probabilities will be derived from counts:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# from collections import Counter\nC_neg = Counter(\n  movie_reviews.words(\n    categories = [\"neg\"]\n    )\n  )\n\nC_pos = Counter(\n  movie_reviews.words(\n    categories = [\"pos\"]\n    )\n  )  \n\nC_all = Counter(\n  movie_reviews.words()\n)\n```\n:::\n\n\n#### Calculating probabilities\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Conditional probabilities\nP_barely_pos = C_pos[\"barely\"] / C_pos.total()\nP_barely_neg = C_neg[\"barely\"] / C_neg.total()\n\n# Base probability\nP_barely = C_all[\"barely\"] / C_all.total()\n\n# Sentiment probabilities\nP_pos = 0.5\nP_neg = 0.5\n```\n:::\n\n\n#### Calculating the posterior\n\n\n::: {.cell}\n\n```{.python .cell-code}\nP_pos_barely = (P_barely_pos * P_pos) / P_barely\n\nP_neg_barely = (P_barely_neg * P_neg) / P_barely\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nP_pos_barely\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.3891150491952352\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nP_neg_barely\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6228859645471293\n```\n\n\n:::\n:::\n\n\n## Naive Bayes\n\nNow, this is the measure of how a *single* word contributes the the classification of a document. But let's grab just one of the reviews with \"barely\" in it.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreview_words = [\n  list(movie_reviews.words(id)) \n  for id in all_ids\n  ]\n\nbarely_reviews = [\n  review \n  for review in review_words \n  if \"barely\" in review\n  ] \n\nlen(barely_reviews[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1025\n```\n\n\n:::\n:::\n\n\nThere were 1025 other words in the review. Surely they *each* contributed some to the total meaning.\n\n### \"Bag of Words\"\n\nThe most basic Naive Bayes model treats each review like a \"bag of words.\"\n\n\n::: {.cell}\n\n```{.python .cell-code}\nCounter(barely_reviews[0]).most_common(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[('.', 55), ('the', 54), (',', 51), (\"'\", 22), ('\"', 22), ('and', 18), ('a', 16), ('of', 15), ('-', 14), ('(', 14)]\n```\n\n\n:::\n:::\n\n\nThen, for each token, we calculate the $P(\\text{sentiment} | w)$ and multiply them together.\n\n$$\nP(\\text{sentiment} | W) = P(\\text{sentiment)}\\prod_{w\\in W}P(w|\\text{sentiment)}\n$$\n\nOr, to do it in log-space\n\n$$\n\\log P(\\text{sentiment}|W) = \\log P(\\text{sentiment)} + \\sum_{w\\in W}\\log P(w|\\text{sentiment})\n$$\n\n### Practicalities\n\nAll of the same issues that arose for n-gram models [which required \"smoothing\"](08_ngram_smoothing.qmd) also arise for these Naive Bayes models, including:\n\n-   Out of vocabulary items in the document you're trying to classify.\n\n-   A token if interest $w$ not appearing in one of the categories, leading to a 0.0 probability.\n\n### \"Feature Engineering\".\n\nAnother thing to take into account is that you might want to adjust or create new features based on the text to do your classification, which is commonly known as \"feature engineering.\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}