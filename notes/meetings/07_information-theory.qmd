---
title: "Information Theory"
date: 2024-02-20
categories:
  - compling
tbl-cap-location: margin  
fig-align: center
knitr: 
  opts_chunk: 
    echo: false
    warning: false
    message: false
    dev: ragg_png
filters:
  - codeblocklabel
---

## Bits

Let's say you live in a studio apartment. It would probably have two big-light switches. One for the main room, and one for the the bathroom. One switch has two states

1.  {{< fa toggle-off >}}: Off
2.  {{< fa toggle-on >}}: On

```{r}
library(tidyverse)
library(gt)

ragg_png <- function(..., res = 300) {
  ragg::agg_png(..., res = res, units = "in")
}
```

```{r}
toggle_on <-  "<i class='fa-regular fa-toggle-on'></i>"
toggle_off <- "<i class='fa-regular fa-toggle-off'></i>"
bulb_on <- "<i class='fa-solid fa-lightbulb'></i>"
bulb_off <- "<i class='fa-regular fa-lightbulb'></i>"
```

A single switch in a single room can give rise to two lighting conditions.

```{r}
#| label: tbl-basic-state
#| tbl-cap: A single switch.
tibble(
  state = c("on", "off"),
  switches = c(toggle_on, toggle_off),
  lighting = c(bulb_on, bulb_off)
) -> single_bit

single_bit |> 
  gt() |> 
  cols_align("center") |> 
  fmt_markdown() 
```

With two switches, you can have 4 unique lighting situations between the main room and the bathroom.

```{r}
#| label: tbl-two-room
#| tbl-cap: "Two room lighting configurations."
expand_grid(
  room = c("on",  "off"),
  bathroom = c("on", "off")
) |> 
  mutate(
    state_n = row_number()
  ) |> 
  pivot_longer(
    1:2,
    names_to = "room",
    values_to = "state"
  ) |> 
  left_join(
    single_bit
  ) |> 
  mutate(
    total_lighting = str_c(lighting, collapse = ", "),
    .by = state_n
  ) |> 
  select(state_n, room, switches, total_lighting) |> 
  pivot_wider(
    names_from = room,
    values_from = switches
  ) |> 
  relocate(
    total_lighting,
    .after = last_col()
  ) |>
  gt() |> 
  cols_align("center") |> 
  fmt_markdown() 
```

If you then moved into a bigger apartment, with a separate bedroom (still open-plan kitchen+living room) that had three switches, we could have 8 different unique lighting conditions.

```{r}
#| label: tbl-three-switch
#| tbl-cap: "Three room lighting configurations"
expand_grid(
  room = c("on",  "off"),
  bedroom = c("on", "off"),
  bathroom = c("on", "off")
) |> 
  mutate(
    state_n = row_number()
  ) |> 
  pivot_longer(
    1:3,
    names_to = "room",
    values_to = "state"
  ) |> 
  left_join(
    single_bit
  ) |> 
  mutate(
    total_lighting = str_c(lighting, collapse = ", "),
    .by = state_n
  ) |> 
  select(state_n, room, switches, total_lighting) |> 
  pivot_wider(
    names_from = room,
    values_from = switches
  ) |> 
  relocate(
    total_lighting,
    .after = last_col()
  ) |> 
  gt() |> 
    cols_align("center") |> 
  fmt_markdown() 
```

I've put the 4 switch (16 lighting configurations) and 5 switch (32 lighting configurations) tables under these collapsible components.

::: {.callout-note collapse="true"}
## 4 switches

```{r}
expand_grid(
  room = c("on",  "off"),
  bedroom = c("on", "off"),
  kitchen = c("on", "off"),
  bathroom = c("on", "off")
) |> 
  mutate(
    state_n = row_number()
  ) |> 
  pivot_longer(
    1:4,
    names_to = "room",
    values_to = "state"
  ) |> 
  left_join(
    single_bit
  ) |> 
  mutate(
    total_lighting = str_c(lighting, collapse = ", "),
    .by = state_n
  ) |> 
  select(state_n, room, switches, total_lighting) |> 
  pivot_wider(
    names_from = room,
    values_from = switches
  ) |> 
  relocate(
    total_lighting,
    .after = last_col()
  ) |> 
  gt() |> 
    cols_align("center") |> 
  fmt_markdown() 
```
:::

::: {.callout-note collapse="true"}
## 5 switches

```{r}
expand_grid(
  `living room` = c("on",  "off"),
  `dining room` = c("on", "off"),
  bedroom = c("on", "off"),
  kitchen = c("on", "off"),
  bathroom = c("on", "off")
) |> 
  mutate(
    state_n = row_number()
  ) |> 
  pivot_longer(
    1:5,
    names_to = "room",
    values_to = "state"
  ) |> 
  left_join(
    single_bit
  ) |> 
  mutate(
    total_lighting = str_c(lighting, collapse = ", "),
    .by = state_n
  ) |> 
  select(state_n, room, switches, total_lighting) |> 
  pivot_wider(
    names_from = room,
    values_from = switches
  ) |> 
  relocate(
    total_lighting,
    .after = last_col()
  ) |> 
  gt() |> 
   cols_align("center") |> 
  fmt_markdown() 
```
:::

To generalize, if you have $n$ switches, you can have $2^n$ states.

```{r}
tibble(
  switches = 1:10,
  states = 2^switches
) |> 
  gt() |> 
  fmt_number(decimals = 0)
```

### Bits → Probabilities

If someone let loose a gremlin into your $n=3$ switch house which then randomly flipped switches, and then asked you to wager for \$50 what the lighting situation was inside. You'd have to guess 1 lighting configuration out of $2^n = 8$ possible configuration, giving you a $\frac{1}{8} = 0.125 = 12.5\%$ chance of getting it right.

In order to represent smaller and smaller probabilities, you actually need more and more bits.

```{r}
tibble(
  switches = 1:10,
  states = 2^switches,
  probability = 1/states
) |> 
  gt() |> 
  fmt_number(
    columns = 1:2,
    decimals = 0
  ) |> 
  fmt_number(
    3,
    decimals = 5 
  )
```

### Probabilities → Bits

If we know how probable something is, can we calculate how many bits we'd need to represent it?

$$
P(x) = \frac{1}{2^n}
$$

$$
P(x) = 2^{-n}
$$

$$
\log_2 P(x) = -n
$$

$$
-\log_2 P(x) = n
$$

```{r}
library(reticulate)
```

```{python}
#| echo: true
import numpy as np

# 1/8 = 0.125
-np.log2(0.125)
```

#### Abstract Bits

We can get more abstract, and figure out how many "bits" it takes to represent probabilities that don't come out to whole numbers.

```{r}
tibble(
  prob = seq(0.1, 0.9, by = 0.1),
  bits = -log2(prob)
) |> 
  arrange(bits) |> 
  gt() |> 
  fmt_number(
    bits,
    decimals = 2
  )
```

The lower the probability, the more bits we need to represent it.

```{r}
#| fig-width: 5
#| fig-height: 3
#| out-width: 70%
#| fig-align: center
#| fig-cap: Relationship between probabilities and bits
tibble(
  prob = seq(0.01, 0.99, length = 100),
  bits = -log2(prob)
) |> 
  ggplot(aes(prob, bits))+
    geom_line(linewidth = 1)+
    scale_x_continuous(limits = c(0,1))+
    theme_minimal()+
    theme(aspect.ratio = 0.7)
```

## Surprisal

The term of art for "The number of bits it takes to represent a probability" is "surprisal". Imagine I came up to you and said:

> The sun rose this morning.

That’s not especially *informative* or *surprising*, since the sun rises every morning. The sun rising in the morning is a very high probability event,[^1] so it’s not surprising it happens, and in the information theory world, we don’t need very many bits for it.

[^1]: Depending on what latitude you live at and the time of year

On the other hand, if someone came up to me and said:

> The sun failed to rise this morning.

*That* is surprising! It’s also very informative. Thank you for telling me! I wasn’t expecting that! The smaller the probability of an event, the more surprising and informative it is if it happens, the larger the *surprisal* value is.

### Unigram surprisal

If we get the count of every token in Moby Dick, then calculate its unigram probability, we'll get the surprisal of every token in the book.

```{python}
import re
import gutenbergpy.textget
from nltk.tokenize import word_tokenize
from collections import Counter
```

```{python}
raw_book = gutenbergpy.textget.get_text_by_id(2701) # with headers
moby_dick_byte = gutenbergpy.textget.strip_headers(raw_book) # without headers
moby_dick = moby_dick_byte.decode("utf-8")
moby_dick_tokens = word_tokenize(moby_dick)
moby_dick_words = [tok for tok in moby_dick_tokens if re.match(r"\w", tok)]
```

```{r}
moby_df <- tibble(
  tokens = py$moby_dick_tokens
)

moby_df |> 
  count(tokens) ->
  moby_count
```

```{r}
#| fig-width: 5
#| fig-height: 3
#| out-width: 70%
#| fig-align: center
#| fig-cap: Relationship between probabilities and bits in unigram probabilities
moby_count |> 
  mutate(
    prob = n /sum(n),
    surprisal = -log2(prob)
  ) |> 
  ggplot(
    aes(prob, surprisal)
  ) +
    geom_line(color = "grey50")+
    geom_point()+
    expand_limits(y = 0, x = 1)+
    theme_minimal()+
    theme(aspect.ratio = 0.7)
```

We can do this for bigrams too, looking at every $P(w_i | w_{i-1})$.

```{r}
#| fig-width: 5
#| fig-height: 3
#| out-width: 70%
#| fig-align: center
moby_df |> 
  mutate(fol = lead(tokens)) |> 
  count(
    tokens, fol
  ) -> 
  moby_bigrams

moby_bigrams |> 
  mutate(
    prob = (n)/(sum(n)),
    surprisal = -log2(prob),
    .by = tokens
  ) |> 
  ggplot(
    aes(prob, surprisal)
  )+
    geom_line(color = "grey50")+
    geom_point()+
    expand_limits(y = 0, x = 1)+
    theme_minimal()+
    theme(aspect.ratio = 0.7)  
```

Since a lot of $P(w_i | w_{i-1})$ probabilities are very high, we have a lot more probabilities closer to 1, with lower surprisal, in this plot.

## Entropy (or average Surprisal)

So, typically, while reading Moby Dick, how surprised, on average, are you going to be? Well, that depends on

-   The surprisal of each word.

-   The *probability* of encountering each word

$$
s(w_i) = -\log_2 P(x_i)
$$

$$
H(W) = \sum_{i=1}^n P(w_i)s(w_i) = -\sum_{i=1}^n P(w_i)\log_2P(w_i)
$$

```{r}
moby_count |> 
  arrange(desc(n)) |> 
  head() |> 
  mutate(
    prob = n/sum(n),
    surprisal = -log2(prob),
    e_surp = prob * surprisal,
    g = "top 6"
  ) |> 
  group_by(g) |> 
  gt() |> 
  summary_rows(
    groups = everything(),
    columns = e_surp,
    fns = list(
      sum = "sum"
    ),
    fmt = ~fmt_number(., decimals =2)
  ) |> 
  fmt_number(
    prob:e_surp,
    decimals = 2
  ) |> 
  fmt_number(
    n ,
    decimals = 0
  ) |> 
  cols_label(
    e_surp = "prob ⋅ surprisal"
  )
```

```{r}
moby_count |> 
  arrange(desc(n)) |> 
  mutate(
    prob = n/sum(n),
    surprisal = -log2(prob),
    e_surp = prob * surprisal
  ) |> 
  summarise(
    entropy = sum(e_surp)
  ) |> 
  pull(entropy) -> mb_unigram_H
```

If we do this for all unigrams, we get `r round(mb_unigram_H, digits = 2)` bits.

## Perplexity (or Entropy → States)

In the tokens of Moby Dick, we have 21,897 unique types.

```{python}
#| echo: true
from collections import Counter
moby_count = Counter(moby_dick_tokens)
len(moby_count)
```

If each type was equally probable, our entropy would be equal to the surprisal of each token:

```{python}
#| echo: true
-np.log2(1/21897)
```

-   14.42 bits of entropy

-   21,897 states

But, with the math we did before, our *average* surprisal was 9.66. We could convert this actual *average* surprisal back states by raising it to the power of 2

```{python}
#| echo: true
np.exp2(9.66)
```

-   9.66 bits of entropy

-   809 states

### How to think about Perplexity

-   The actual number of states we would need in something like an FSA to capture every token in Moby Dick is 21,897

-   *But,* the edges between nodes in that FSA are weighted.

-   The degree of surprisal we experience transitioning from state to state in that FSA is *like* if we were using a smaller, 809 state FSA with unweighted edges.

## Evalulating an ngram model.

Let's say I had a pre-trained bigram model, and I wanted to figure out the the probability of the sentence

> He saw the ship.

```{python}
from nltk.tokenize import sent_tokenize
from nltk import bigrams
from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.lm.preprocessing import pad_both_ends
from nltk.lm import MLE

lm = MLE(2)

moby_sents = sent_tokenize(moby_dick)
moby_words = [word_tokenize(sent) for sent in moby_sents]
train, vocab = padded_everygram_pipeline(2, moby_words)

lm.fit(train, vocab)
```

```{python}
sent_bigram = list(bigrams(pad_both_ends(word_tokenize("He saw the ship."), 2)))
sent_bigram2 = list(bigrams(pad_both_ends(word_tokenize("He screamed the ship."), 2)))
```

Our method of calculating the probability of a sentence is going to be

$$
P(w_i\dots w_{i+n}) = \prod P(w_{i+1} | w_i)
$$

```{python}
lm_probs = [lm.score(bg[1], [bg[0]]) for bg in sent_bigram]
lm_probs2 = [lm.score(bg[1], [bg[0]]) for bg in sent_bigram2]
```

```{python}
equal_prob = 1/np.array([len(lm.counts[[bg[0]]]) for bg in sent_bigram])
```

```{r}
pos_glue <- function(x){
  a = x[1]
  b = x[2]
  return(
    str_glue("P({b}|{a})")
  )
}

py$sent_bigram |> 
  map(as_vector) |> 
  map(pos_glue) |> 
  flatten() |> 
  as_vector()->
  bigram_seq

tibble(
  bigrams = bigram_seq,
  prob = py$lm_probs,
  g = "bigrams"
) |> 
  group_by(g) |> 
  gt() |> 
  fmt_number(
    decimals = 3
  ) |> 
  summary_rows(
    groups = everything(),
    columns = prob,
    fns = list(
      Product = "prod"
    ),
    fmt = ~fmt_number(., decimals = 8)
  ) |> 
  cols_label(
    bigrams = ""
  )
```

Very small probability, but that's what you get when you multiply a bunch of probabilities. The total summed log-probabilities is:

```{python}
#| echo: true
np.array([
    lm.logscore(bg[1], [bg[0]]) 
    for bg in sent_bigram
]).sum()

```

If all transitions were equally probable, we'd have

```{python}
np.log(equal_prob).sum()
```

This can be converted into an entropy:

```{python}
#| echo: true
lm.entropy(sent_bigram)
```

If we'd treated every possible transition as equally probable, we would've had an entropy more like:

```{python}
#| echo: true
-np.log2(equal_prob).mean()
```

And we can get the perplexity:

```{python}
#| echo: true
lm.perplexity(sent_bigram)
```

Again, if every transition had been equally probable, we'd have a perplexity of:

```{python}
np.exp2(-np.log2(equal_prob).mean())
```

### The upshot

The higher the probability an ngram assigns to a sentence, the lower its entropy, and the lower the perplexity.

## *Really* novel sentences

Using the probabilities based on raw counts, we run into trouble with, say, a very novel sentence

> He screamed the ship.

```{r}
py$sent_bigram2 |> 
  map(as_vector) |> 
  map(pos_glue) |> 
  flatten() |> 
  as_vector()->
  bigram_seq2

tibble(
  bigrams = bigram_seq2,
  prob = py$lm_probs2,
  g = "bigrams"
) |> 
  group_by(g) |> 
  gt() |> 
  fmt_number(
    decimals = 3
  ) |> 
  summary_rows(
    groups = everything(),
    columns = prob,
    fns = list(
      Product = "prod"
    ),
    #fmt = ~fmt_number(., decimals = 3)
  ) |> 
  cols_label(
    bigrams = ""
  )
```

Since the sequence "He screamed" never occurred in the book, $P(\text{screamed}|text{He}) = 0$, which means the total product of these bigrams is 0.

```{python}
#| echo: true
lm.entropy(sent_bigram2)
```

```{python}
#| echo: true
-np.log2(0)
```

```{python}
#| echo: true
target = [
  sent
  for sent in moby_words
  if "screamed" in sent
][0]

print(target[0:9])
```
