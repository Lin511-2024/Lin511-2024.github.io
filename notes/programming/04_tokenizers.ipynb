{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Working with Tokenizers\n",
    "date: 2024-02-08\n",
    "categories:\n",
    "  - python\n",
    "filters:\n",
    "  - codeblocklabel\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a little bit practical with \n",
    "\n",
    "- getting text\n",
    "- getting a tokenizer\n",
    "- using the tokenizer\n",
    "\n",
    "\n",
    "For this lesson, we're going to use `gutenbergpy` and `nltk`, but if you try to import them right now, like they were in the course notes, you're going to get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: codebox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------------------------------------------------------\n",
    "ModuleNotFoundError      Traceback (most recent call last)\n",
    "Cell In[2], line 1\n",
    "----> 1 import nltk\n",
    "\n",
    "ModuleNotFoundError: No module named 'nltk'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing `gutenbergpy`\n",
    "\n",
    "We'll need to install these packages. We'll start with `gutenbergpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gutenbergpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import the functions to get Project Gutenberg books. The url for Moby Dick on Project Gutenberg is [https://www.gutenberg.org/ebooks/2701](https://www.gutenberg.org/ebooks/2701). That last part of the url is the ID of the book, which we can pass to `get_text_by_id()` to download the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenbergpy.textget import get_text_by_id, strip_headers\n",
    "\n",
    "book_id = 2701\n",
    "\n",
    "raw_book = get_text_by_id(book_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`raw_book` contains the book with all of its legal headers and footers. we can remove the headers and footers with `strip_headers()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_byte = strip_headers(raw_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last hitch here has to do with \"character encoding\". We need to \"decode\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_clean = book_byte.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap that up into one function we can re-run on new IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_book(book_id):\n",
    "    \"\"\"Get the cleaned book\n",
    "\n",
    "    Args:\n",
    "        book_id (str|int): The book id\n",
    "\n",
    "    Returns:\n",
    "        (str): The full book\n",
    "    \"\"\"\n",
    "    raw_book = get_text_by_id(book_id)\n",
    "    book_byte = strip_headers(raw_book)\n",
    "    book_clean = book_byte.decode(\"utf-8\")\n",
    "\n",
    "    return book_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and point `get_clean_book()` at another book id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK tokenization\n",
    "\n",
    "Let's tokenize one of our books with `nltk.tokenize.word_tokenize()`.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Install `nltk`.\n",
    "2. Try tokenizing your book.\n",
    "\n",
    "It might not go right at first. You can double check what to do here in [the course notes](https://lin511-2024.github.io/notes/meetings/03_tokenization.html#tokenizers--part-1-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try `spacy`\n",
    "\n",
    "To work with `spacy`, we need to:\n",
    "\n",
    "1. Install `spacy`\n",
    "2. Install one of the spacy models.\n",
    "\n",
    "### The steps\n",
    "\n",
    "1. Go to the [spacy website](https://spacy.io/)\n",
    "2. Can you find the code to successfully install it and its language model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "first_para = re.findall(\n",
    "    r\"Call me Ishmael.*?\\n\\n\", \n",
    "    book_clean, \n",
    "    re.DOTALL)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_doc = nlp(first_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `nlp` is actually a complex object enriched with a lot of information that we can access a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
       "little or no money in my purse, and nothing particular to interest me\n",
       "on shore, I thought I would sail about a little and see the watery part\n",
       "of the world. It is a way I have of driving off the spleen and\n",
       "regulating the circulation. Whenever I find myself growing grim about\n",
       "the mouth; whenever it is a damp, drizzly November in my soul; whenever\n",
       "I find myself involuntarily pausing before coffin warehouses, and\n",
       "bringing up the rear of every funeral I meet; and especially whenever\n",
       "my hypos get such an upper hand of me, that it requires a strong moral\n",
       "principle to prevent me from deliberately stepping into the street, and\n",
       "methodically knocking people’s hats off—then, I account it high time to\n",
       "get to sea as soon as I can. This is my substitute for pistol and ball.\n",
       "With a philosophical flourish Cato throws himself upon his sword; I\n",
       "quietly take to the ship. There is nothing surprising in this. If they\n",
       "but knew it, almost all men in their degree, some time or other,\n",
       "cherish very nearly the same feelings towards the ocean with me.\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get any particular token out, you can do ordinary indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ishmael"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_doc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual *text* of a token, we need to get its `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ishmael'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_doc[2].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's lots of great stuff we can get out, like each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call me Ishmael."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(para_doc.sents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the parts of speech of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VERB', 'PRON', 'PROPN', 'PUNCT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sent = list(para_doc.sents)[0]\n",
    "[x.pos_ for x in first_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VerbForm=Inf,\n",
       " Case=Acc|Number=Sing|Person=1|PronType=Prs,\n",
       " Number=Sing,\n",
       " PunctType=Peri]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.morph for x in first_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case': 'Acc', 'Number': 'Sing', 'Person': '1', 'PronType': 'Prs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sent[1].morph.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renv-python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
